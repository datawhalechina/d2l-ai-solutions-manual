# ç¬¬6ç«  å·ç§¯ç¥ç»ç½‘ç»œ

## 6.1 ä»å…¨è¿æ¥å±‚åˆ°å·ç§¯

### ç»ƒä¹  6.1.1

å‡è®¾å·ç§¯å±‚(6.3)è¦†ç›–çš„å±€éƒ¨åŒºåŸŸ$\Delta = 0$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯æ˜å·ç§¯å†…æ ¸ä¸ºæ¯ç»„é€šé“ç‹¬ç«‹åœ°å®ç°ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp;å±€éƒ¨åŒºåŸŸ$\Delta=0$ è¡¨ç¤ºå·ç§¯æ ¸çš„å¤§å°ç­‰äºè¾“å…¥çš„å¤§å°ã€‚å®é™…å°±æ˜¯é—®ï¼Œ1Ã—1çš„å·ç§¯æ ¸æ˜¯å¦ç­‰ä»·äºå…¨è¿æ¥ï¼ˆå‚è§æœ¬ä¹¦7.3èŠ‚ï¼šNiNç½‘ç»œç»“æ„ï¼‰ã€‚å› æ­¤ï¼Œæ¯ä¸ªå·ç§¯æ ¸åªèƒ½è¦†ç›–ä¸€ä¸ªåƒç´ ç‚¹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå·ç§¯å±‚çš„è®¡ç®—æ–¹å¼ä¸å…¨è¿æ¥å±‚éå¸¸ç›¸ä¼¼ã€‚å› ä¸ºæ¯ä¸ªå·ç§¯æ ¸åªèƒ½çœ‹åˆ°ä¸€ä¸ªé€šé“çš„ä¿¡æ¯ï¼Œç›¸å½“äºæ¯ä¸ªå·ç§¯æ ¸åªæ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æƒé‡çŸ©é˜µã€‚ æ‰€ä»¥ï¼Œå·ç§¯å†…æ ¸å¯ä»¥çœ‹ä½œæ˜¯æ¯ç»„é€šé“ç‹¬ç«‹åœ°å®ç°ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚æ¯ä¸ªå·ç§¯æ ¸éƒ½æœ‰è‡ªå·±çš„æƒé‡ï¼Œæ¯ä¸ªè¾“å…¥é€šé“éƒ½è¢«ç‹¬ç«‹å¤„ç†ï¼Œè¾“å‡ºé€šé“æ˜¯å„ä¸ªè¾“å…¥é€šé“çš„åŠ æƒå’Œã€‚è¿™ç§ç‹¬ç«‹å¤„ç†çš„æ–¹å¼æœ‰æ•ˆåœ°å‡å°‘äº†æƒé‡çš„æ•°é‡ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”èƒ½å¤Ÿæå–å‡ºè¾“å…¥æ•°æ®ä¸­çš„ç©ºé—´ç‰¹å¾ã€‚


```python
# ä»£ç éªŒè¯
import torch
import torch.nn as nn


class MyNet1(nn.Module):
    def __init__(self, linear1, linear2):
        super(MyNet1, self).__init__()
        self.linear1 = linear1
        self.linear2 = linear2

    def forward(self, X):
        return self.linear2(self.linear1(nn.Flatten()(X)))


class MyNet2(nn.Module):
    def __init__(self, linear, conv2d):
        super(MyNet2, self).__init__()
        self.linear = linear
        self.conv2d = conv2d

    def forward(self, X):
        X = self.linear(nn.Flatten()(X))
        X = X.reshape(X.shape[0], -1, 1, 1)
        X = nn.Flatten()(self.conv2d(X))
        return X


linear1 = nn.Linear(15, 10)
linear2 = nn.Linear(10, 5)
conv2d = nn.Conv2d(10, 5, 1)

linear2.weight = nn.Parameter(conv2d.weight.reshape(linear2.weight.shape))
linear2.bias = nn.Parameter(conv2d.bias)

net1 = MyNet1(linear1, linear2)
net2 = MyNet2(linear1, conv2d)

X = torch.randn(2, 3, 5)
# ä¸¤ä¸ªç»“æœå®é™…å­˜åœ¨ä¸€å®šçš„è¯¯å·®ï¼Œç›´æ¥print(net1(X) == net2(X))å¾—åˆ°çš„ç»“æœä¸å…¨æ˜¯True
print(net1(X))
print(net2(X))
```

    tensor([[0.1190, 0.2377, 0.1443, 0.1020, 0.0702],
            [0.1301, 0.2734, 0.1215, 0.0839, 0.1271]], grad_fn=<AddmmBackward0>)
    tensor([[0.1190, 0.2377, 0.1443, 0.1020, 0.0702],
            [0.1301, 0.2734, 0.1215, 0.0839, 0.1271]],
           grad_fn=<ReshapeAliasBackward0>)


### ç»ƒä¹  6.1.2

ä¸ºä»€ä¹ˆå¹³ç§»ä¸å˜æ€§å¯èƒ½ä¹Ÿä¸æ˜¯å¥½ä¸»æ„å‘¢ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¹³ç§»ä¸å˜æ€§å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºæŸäº›ä»»åŠ¡ï¼Œå¹³ç§»ä¸å˜æ€§å¹¶ä¸æ˜¯å¿…é¡»çš„ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«ç‰©ä½“çš„ä½ç½®å’Œå§¿æ€ï¼Œå¹¶æ ¹æ®è¿™äº›ä¿¡æ¯å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¹³ç§»ä¸å˜æ€§å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒå¿½ç•¥äº†ç‰©ä½“çš„ä½ç½®å’Œå§¿æ€ç­‰é‡è¦ä¿¡æ¯ã€‚ å…¶æ¬¡ï¼Œå¹³ç§»ä¸å˜æ€§å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

&emsp;&emsp;å‚è€ƒï¼š[https://arxiv.org/pdf/1805.12177.pdf](https://arxiv.org/pdf/1805.12177.pdf)

### ç»ƒä¹  6.1.3

å½“ä»å›¾åƒè¾¹ç•Œåƒç´ è·å–éšè—è¡¨ç¤ºæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ€è€ƒå“ªäº›é—®é¢˜ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;è€ƒè™‘æ˜¯å¦å¡«å……`padding`ï¼Œä»¥åŠå¡«å……å¤šå¤§çš„`padding`çš„é—®é¢˜ã€‚å¯ä»¥ä½¿ç”¨`torch.nn`æ¨¡å—ä¸­çš„`functional.pad`å‡½æ•°å¯¹å›¾åƒè¿›è¡Œå¡«å……æ“ä½œï¼Œä»¥ä¿è¯è¾¹ç•Œåƒç´ çš„ä¿¡æ¯å®Œæ•´ã€‚å¡«å……åè¿˜éœ€è¦è¿›è¡Œé¢å¤–çš„å¤„ç†ï¼Œä¾‹å¦‚ä½¿ç”¨å›¾åƒå¤åˆ¶ã€å¡«å……ã€å¹³æ»‘ç­‰æ–¹æ³•æ¥è·å–éšè—è¡¨ç¤ºã€‚

### ç»ƒä¹  6.1.4

æè¿°ä¸€ä¸ªç±»ä¼¼çš„éŸ³é¢‘å·ç§¯å±‚çš„æ¶æ„ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp;ä¸€ç§åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„éŸ³é¢‘ç‰¹å¾ç”Ÿæˆæ–¹æ³•ï¼Œé¦–å…ˆå¯¹å£°éŸ³ä¿¡å·è¿›è¡Œé¢„å¤„ç†å’Œç¦»æ•£å‚…é‡Œå¶å˜æ¢è®¡ç®—å£°éŸ³ä¿¡å·çš„å¹…åº¦è°±ï¼Œå½¢æˆäºŒç»´è°±å›¾ä¿¡å·ï¼›ç„¶åæ­å»ºä»¥ä¸Šè¿°äºŒç»´è°±å›¾ä¿¡å·ä¸ºè¾“å…¥çš„ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œå¹¶è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¾—åˆ°ç‰¹å¾ç”Ÿæˆå™¨æ¨¡å‹ï¼›æœ€åå¯¹å¾…æµ‹å£°éŸ³è¿›è¡Œé¢„å¤„ç†å’Œç¦»æ•£å‚…é‡Œå¶å˜æ¢å¾—åˆ°äºŒç»´è°±å›¾ä¿¡å·ï¼Œå¹¶å°†å…¶é€å…¥è®­ç»ƒå¥½çš„ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å·ç§¯ç½‘ç»œè®¡ç®—ï¼Œå¾—åˆ°è¾“å‡ºå³ä¸ºæ‰€è¦ç”Ÿæˆçš„éŸ³é¢‘ç‰¹å¾ï¼Œå®ç°å£°éŸ³ä¿¡å·çš„éŸ³é¢‘ç‰¹å¾ç”Ÿæˆã€‚

### ç»ƒä¹  6.1.5

å·ç§¯å±‚ä¹Ÿé€‚åˆäºæ–‡æœ¬æ•°æ®å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å·ç§¯å±‚ä¹Ÿé€‚åˆäºæ–‡æœ¬æ•°æ®ã€‚ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæ–‡æœ¬æ•°æ®é€šå¸¸è¡¨ç¤ºä¸ºè¯å‘é‡çŸ©é˜µï¼Œå…¶ä¸­æ¯è¡Œä»£è¡¨ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºã€‚å·ç§¯å±‚å¯ä»¥åœ¨è¿™ä¸ªçŸ©é˜µä¸Šè¿›è¡Œå·ç§¯æ“ä½œï¼Œç±»ä¼¼äºå›¾åƒå·ç§¯å±‚ä¸­å¯¹å›¾åƒè¿›è¡Œå·ç§¯æ“ä½œã€‚ åœ¨å·ç§¯å±‚ä¸­ï¼Œå·ç§¯æ ¸ä¼šåœ¨è¾“å…¥çŸ©é˜µä¸Šè¿›è¡Œæ»‘åŠ¨çª—å£è®¡ç®—ï¼Œè¾“å‡ºä¸€ä¸ªæ–°çš„ç‰¹å¾çŸ©é˜µã€‚åœ¨æ–‡æœ¬æ•°æ®ä¸­ï¼Œè¿™ä¸ªç‰¹å¾çŸ©é˜µå¯ä»¥çœ‹ä½œæ˜¯å¯¹è¾“å…¥æ–‡æœ¬çš„ä¸åŒn-gramç‰¹å¾çš„æå–ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå¤§å°ä¸º3çš„å·ç§¯æ ¸å¯ä»¥æå–å‡ºè¾“å…¥æ–‡æœ¬ä¸­æ¯ä¸ªé•¿åº¦ä¸º3çš„n-gramç‰¹å¾ã€‚è¿™äº›ç‰¹å¾å¯ä»¥ç”¨äºåç»­çš„åˆ†ç±»æˆ–è€…å›å½’ä»»åŠ¡ã€‚ æ­¤å¤–ï¼Œå·ç§¯å±‚è¿˜å¯ä»¥ä¸å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç»“åˆä½¿ç”¨ï¼Œå½¢æˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„æ··åˆæ¨¡å‹ã€‚è¿™ç§æ¨¡å‹å¯ä»¥åŒæ—¶æ•æ‰æ–‡æœ¬ä¸­çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ å› æ­¤ï¼Œå·ç§¯å±‚é€‚ç”¨äºæ–‡æœ¬æ•°æ®ï¼Œå¯ä»¥å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œå·ç§¯æ“ä½œï¼Œæå–å‡ºä¸åŒn-gramç‰¹å¾ï¼Œå¹¶ä¸”å¯ä»¥ä¸RNNç»“åˆä½¿ç”¨ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

### ç»ƒä¹  6.1.6

è¯æ˜åœ¨å¼(6.6)ä¸­ï¼Œ$f * g = g * f$ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp;é€šè¿‡å¼(6.6)çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$(f * g)(x) = \int_{-\infty}^{\infty}f(y)g(x-y)dy$$

$$(g * f)(x) = \int_{-\infty}^{\infty}g(y)f(x-y)dy$$

&emsp;&emsp;è¦è¯æ˜$f * g = g * f$ï¼Œå³è¯æ˜ï¼š

$$\int_{-\infty}^{\infty}f(y)g(x-y)dy = \int_{-\infty}^{\infty}g(y)f(x-y)dy$$

&emsp;&emsp;ä¸ºäº†è¯æ˜ä¸Šå¼æˆç«‹ï¼Œæˆ‘ä»¬å°†å…¶ä¸­ä¸€ä¸ªç§¯åˆ†çš„å˜é‡åæ”¹ä¸º$t=x-y$ï¼Œåˆ™æœ‰ï¼š

$$\int_{-\infty}^{\infty}f(y)g(x-y)dy = \int_{-\infty}^{\infty}f(x-t)g(t)dt$$

&emsp;&emsp;å†å°†è¿™ä¸ªå¼å­ä»£å›å¼(6.6)ä¸­ï¼š

$$(f * g)(x) = \int_{-\infty}^{\infty}f(x-t)g(t)dt$$

&emsp;&emsp;å¯¹æ¯”å¼(6.6)å’Œä¸Šé¢çš„å¼å­ï¼Œå¯ä»¥å‘ç°å®ƒä»¬çš„å½¢å¼æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œåªæ˜¯ç§¯åˆ†å˜é‡åä¸åŒè€Œå·²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$(f * g)(x) = \int_{-\infty}^{\infty}f(y)g(x-y)dy = \int_{-\infty}^{\infty}g(y)f(x-y)dy = (g * f)(x)$$

&emsp;&emsp;å› æ­¤ï¼Œ$f * g = g * f$ï¼Œè¯æ¯•ã€‚



## 6.2 å›¾åƒå·ç§¯

### ç»ƒä¹  6.2.1  

æ„å»ºä¸€ä¸ªå…·æœ‰å¯¹è§’çº¿è¾¹ç¼˜çš„å›¾åƒ`X`ã€‚
1. å¦‚æœå°†æœ¬èŠ‚ä¸­ä¸¾ä¾‹çš„å·ç§¯æ ¸`K`åº”ç”¨äº`X`ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿ
1. å¦‚æœè½¬ç½®`X`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
1. å¦‚æœè½¬ç½®`K`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;åœ¨å¯¹è§’çº¿å¤„æœ‰åˆ†åˆ«ä¸º1å’Œ-1çš„æ•°æ®ï¼Œå…¶ä»–åŒºåŸŸéƒ½ä¸º0ã€‚


```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """è®¡ç®—äºŒç»´äº’ç›¸å…³è¿ç®—"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y

# å¦‚æœå°†æœ¬èŠ‚ä¸­ä¸¾ä¾‹çš„å·ç§¯æ ¸Kåº”ç”¨äºXï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿ
X = torch.eye(8)
K = torch.tensor([[1.0, -1.0]])
Y = corr2d(X, K)
print(Y)
```

    tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
            [-1.,  1.,  0.,  0.,  0.,  0.,  0.],
            [ 0., -1.,  1.,  0.,  0.,  0.,  0.],
            [ 0.,  0., -1.,  1.,  0.,  0.,  0.],
            [ 0.,  0.,  0., -1.,  1.,  0.,  0.],
            [ 0.,  0.,  0.,  0., -1.,  1.,  0.],
            [ 0.,  0.,  0.,  0.,  0., -1.,  1.],
            [ 0.,  0.,  0.,  0.,  0.,  0., -1.]])


**ç¬¬2é—®ï¼š**

&emsp;&emsp;è½¬ç½®åç»“æœä¸å˜ã€‚


```python
Y = corr2d(X.T, K)
X, Y
```




    (tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
             [0., 1., 0., 0., 0., 0., 0., 0.],
             [0., 0., 1., 0., 0., 0., 0., 0.],
             [0., 0., 0., 1., 0., 0., 0., 0.],
             [0., 0., 0., 0., 1., 0., 0., 0.],
             [0., 0., 0., 0., 0., 1., 0., 0.],
             [0., 0., 0., 0., 0., 0., 1., 0.],
             [0., 0., 0., 0., 0., 0., 0., 1.]]),
     tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
             [-1.,  1.,  0.,  0.,  0.,  0.,  0.],
             [ 0., -1.,  1.,  0.,  0.,  0.,  0.],
             [ 0.,  0., -1.,  1.,  0.,  0.,  0.],
             [ 0.,  0.,  0., -1.,  1.,  0.,  0.],
             [ 0.,  0.,  0.,  0., -1.,  1.,  0.],
             [ 0.,  0.,  0.,  0.,  0., -1.,  1.],
             [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]))



**ç¬¬3é—®ï¼š**

&emsp;&emsp;Kè½¬ç½®åï¼Œç»“æœä¹Ÿè½¬ç½®äº† 


```python
Y = corr2d(X, K.T)
X, Y
```




    (tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
             [0., 1., 0., 0., 0., 0., 0., 0.],
             [0., 0., 1., 0., 0., 0., 0., 0.],
             [0., 0., 0., 1., 0., 0., 0., 0.],
             [0., 0., 0., 0., 1., 0., 0., 0.],
             [0., 0., 0., 0., 0., 1., 0., 0.],
             [0., 0., 0., 0., 0., 0., 1., 0.],
             [0., 0., 0., 0., 0., 0., 0., 1.]]),
     tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],
             [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.],
             [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.]]))



### ç»ƒä¹  6.2.2 

åœ¨æˆ‘ä»¬åˆ›å»ºçš„`Conv2D`è‡ªåŠ¨æ±‚å¯¼æ—¶ï¼Œæœ‰ä»€ä¹ˆé”™è¯¯æ¶ˆæ¯ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;ä¼šæç¤ºç»´åº¦ä¸å¯¹ç§°çš„é”™è¯¯ä¿¡æ¯ï¼Œå› ä¸ºtorchæä¾›çš„äºŒç»´å·ç§¯å±‚æ˜¯nn.Conv2d() é‡‡ç”¨çš„æ˜¯å››ç»´è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ï¼ˆæ‰¹é‡å¤§å°ã€é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼‰,è€Œæˆ‘ä»¬è‡ªå®šä¹‰çš„ä»…ä»…æ˜¯äºŒç»´çš„ã€‚

&emsp;&emsp;ä»£ç éªŒè¯å¦‚ä¸‹


```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """è®¡ç®—äºŒç»´äº’ç›¸å…³è¿ç®—"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y
```


```python
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias
```


```python
X = torch.ones((6, 8))
X[:, 2:6] = 0
X
```




    tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.]])




```python
K = torch.tensor([[1.0, -1.0]])
```


```python
Y = corr2d(X, K)
Y
```




    tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])




```python
corr2d(X.t(), K)
```




    tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])



&emsp;&emsp;ä½¿ç”¨`nn.Conv2d`æ—¶å¯ä»¥æ­£å¸¸è¿è¡Œ


```python
# æ„é€ ä¸€ä¸ªäºŒç»´å·ç§¯å±‚ï¼Œå®ƒå…·æœ‰1ä¸ªè¾“å‡ºé€šé“å’Œå½¢çŠ¶ä¸ºï¼ˆ1ï¼Œ2ï¼‰çš„å·ç§¯æ ¸
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# è¿™ä¸ªäºŒç»´å·ç§¯å±‚ä½¿ç”¨å››ç»´è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ï¼ˆæ‰¹é‡å¤§å°ã€é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼‰ï¼Œ
# å…¶ä¸­æ‰¹é‡å¤§å°å’Œé€šé“æ•°éƒ½ä¸º1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # å­¦ä¹ ç‡

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # è¿­ä»£å·ç§¯æ ¸
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
```

    epoch 2, loss 5.625
    epoch 4, loss 1.443
    epoch 6, loss 0.446
    epoch 8, loss 0.159
    epoch 10, loss 0.061


&emsp;&emsp;ä½¿ç”¨åˆ›å»ºçš„`Conv2D`æ—¶å¯ä»¥ä¼šæŠ¥å¦‚ä¸‹é”™è¯¯


```python
conv2d = Conv2D(kernel_size=(1, 2))

try:
  for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # è¿­ä»£å·ç§¯æ ¸
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
except Exception as e:
    print(e)
```

    The size of tensor a (0) must match the size of tensor b (7) at non-singleton dimension 3


&emsp;&emsp;å¯ä»¥é€šè¿‡ä¸‹è¿°æ–¹å¼è°ƒæ•´


```python
X = X.reshape((6, 8))
Y = Y.reshape((6, 7))
lr = 3e-2  # å­¦ä¹ ç‡

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # è¿­ä»£å·ç§¯æ ¸
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
```

    epoch 2, loss 17.816
    epoch 4, loss 5.911
    epoch 6, loss 2.188
    epoch 8, loss 0.857
    epoch 10, loss 0.345


### ç»ƒä¹  6.2.3

å¦‚ä½•é€šè¿‡æ”¹å˜è¾“å…¥å¼ é‡å’Œå·ç§¯æ ¸å¼ é‡ï¼Œå°†äº’ç›¸å…³è¿ç®—è¡¨ç¤ºä¸ºçŸ©é˜µä¹˜æ³•ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;é¢˜ç›®çš„æ„æ€åº”è¯¥æ˜¯å¦‚ä½•é€šè¿‡çŸ©é˜µä¹˜æ³•å¾—åˆ°äº’ç›¸å…³ï¼ˆå·ç§¯ï¼‰è¿ç®—


```python
import torch
from torch import nn
from d2l import torch as d2l

def conv2d_by_mul(X, K):
    # è·å–å·ç§¯æ ¸å¤§å°
    h, w = K.shape
    # è®¡ç®—è¾“å‡ºå›¾åƒå¤§å°
    outh = X.shape[0] - h + 1
    outw = X.shape[1] - w + 1
    # è°ƒæ•´å·ç§¯æ ¸å½¢çŠ¶ä»¥ä¾¿åšä¹˜æ³•
    K = K.reshape(-1, 1)
    # å°†è¾“å…¥å›¾åƒåˆ‡æˆå·ç§¯æ ¸å¤§å°çš„å—ï¼Œæ‰“å¹³æˆä¸€ç»´ï¼Œå­˜æ”¾åœ¨åˆ—è¡¨ Y ä¸­
    Y = []
    for i in range(outh):
        for j in range(outw):
            Y.append(X[i:i + h, j:j + w].reshape(-1))
    # å°†åˆ—è¡¨ Y è½¬ä¸ºå¼ é‡ï¼Œæ¯è¡Œä»£è¡¨ä¸€å—çš„æ‰“å¹³ç»“æœ
    Y = torch.stack(Y, 0)
    # ç”¨çŸ©é˜µä¹˜æ³•è¡¨ç¤ºäº’ç›¸å…³è¿ç®—
    res = (torch.matmul(Y, K)).reshape(outh, outw)
    # è¿”å›è¾“å‡ºç»“æœ
    return res
```

### ç»ƒä¹  6.2.4

æ‰‹å·¥è®¾è®¡ä¸€äº›å·ç§¯æ ¸ã€‚
1. äºŒé˜¶å¯¼æ•°çš„æ ¸çš„å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ
1. ç§¯åˆ†çš„æ ¸çš„å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ
1. å¾—åˆ°$d$æ¬¡å¯¼æ•°çš„æœ€å°æ ¸çš„å¤§å°æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;äºŒé˜¶å¯¼æ•°çš„æ ¸çš„å½¢å¼æ˜¯ï¼š

$$\begin{bmatrix}-1 & 2 & -1\end{bmatrix}$$

**ç¬¬2é—®ï¼š**

&emsp;&emsp;ç§¯åˆ†çš„æ ¸çš„å½¢å¼æ˜¯ï¼š

$$\begin{bmatrix}1 & 1 & 1 & \cdots & 1\end{bmatrix}$$


**ç¬¬3é—®ï¼š**

&emsp;&emsp;å¾—åˆ° ğ‘‘ æ¬¡å¯¼æ•°çš„æœ€å°æ ¸çš„å¤§å°æ˜¯ $d+1$ã€‚ä¾‹å¦‚ï¼Œä¸€é˜¶å¯¼æ•°çš„æœ€å°æ ¸å¤§å°ä¸º $2$ï¼ŒäºŒé˜¶å¯¼æ•°çš„æœ€å°æ ¸å¤§å°ä¸º $3$ï¼Œä¸‰é˜¶å¯¼æ•°çš„æœ€å°æ ¸å¤§å°ä¸º $4$ï¼Œä»¥æ­¤ç±»æ¨ã€‚

## 6.3 å¡«å……å’Œæ­¥å¹… 

### ç»ƒä¹  6.3.1  

å¯¹äºæœ¬èŠ‚ä¸­çš„æœ€åä¸€ä¸ªç¤ºä¾‹ï¼Œè®¡ç®—å…¶è¾“å‡ºå½¢çŠ¶ï¼Œä»¥æŸ¥çœ‹å®ƒæ˜¯å¦ä¸å®éªŒç»“æœä¸€è‡´ã€‚

**è§£ç­”ï¼š** 

$$out_{shape}=\lfloor(n_h-k_h+p_h*2+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w*2+s_w)/s_w\rfloor.$$

&emsp;&emsp;ç¤ºä¾‹ä¸­$X.shape = [8, 8]$ï¼Œè®¡ç®—å¾—å‡º$out_shape = [(8-3+0+3)/3, (8-5+2+4)/4] = [2.67, 2.25]$,å‘ä¸‹å–æ•´ï¼Œæ‰€ä»¥ä¸º$[2, 2]$



&emsp;&emsp; ä»£ç éªŒè¯å¦‚ä¸‹


```python
import torch
from torch import nn


# ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªè®¡ç®—å·ç§¯å±‚çš„å‡½æ•°ã€‚
# æ­¤å‡½æ•°åˆå§‹åŒ–å·ç§¯å±‚æƒé‡ï¼Œå¹¶å¯¹è¾“å…¥å’Œè¾“å‡ºæé«˜å’Œç¼©å‡ç›¸åº”çš„ç»´æ•°
def comp_conv2d(conv2d, X):
    # è¿™é‡Œçš„ï¼ˆ1ï¼Œ1ï¼‰è¡¨ç¤ºæ‰¹é‡å¤§å°å’Œé€šé“æ•°éƒ½æ˜¯1
    X = X.reshape((1, 1) + X.shape)
    Y = conv2d(X)
    # çœç•¥å‰ä¸¤ä¸ªç»´åº¦ï¼šæ‰¹é‡å¤§å°å’Œé€šé“
    return Y.reshape(Y.shape[2:])

# è¯·æ³¨æ„ï¼Œè¿™é‡Œæ¯è¾¹éƒ½å¡«å……äº†1è¡Œæˆ–1åˆ—ï¼Œå› æ­¤æ€»å…±æ·»åŠ äº†2è¡Œæˆ–2åˆ—
X = torch.rand(size=(8, 8))
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
comp_conv2d(conv2d, X).shape
```




    torch.Size([2, 2])



### ç»ƒä¹  6.3.2

åœ¨æœ¬èŠ‚ä¸­çš„å®éªŒä¸­ï¼Œè¯•ä¸€è¯•å…¶ä»–å¡«å……å’Œæ­¥å¹…ç»„åˆã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp; ä¸‹é¢å°†ä¸¾ä¸¤ä¸ªå…¶ä»–çš„å¡«å……å’Œæ­¥å¹…ç»„åˆ


```python
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(1, 2), stride=(2, 3))
comp_conv2d(conv2d, X).shape
```




    torch.Size([4, 3])



&emsp;&emsp;åœ¨`padding`å¤§å°ä¸º$[1,2]$ï¼Œ`stride`å¤§å°ä¸º$[2,3]$æ—¶ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º$[4,3]$

&emsp;&emsp;ç¤ºä¾‹ä¸­$X.shape = [8, 8]$ï¼Œè®¡ç®—å¾—å‡º$out_{shape} = [(8-3+2+2)/2, (8-5+4+3)/3] = [4.5, 3,33]$,å‘ä¸‹å–æ•´ï¼Œæ‰€ä»¥ä¸º$[4, 3]$


```python
conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(2, 3), stride=(1, 2))
comp_conv2d(conv2d, X).shape
```




    torch.Size([10, 5])



&emsp;&emsp;åœ¨`padding`å¤§å°ä¸º$[2,3]$ï¼Œ`stride`å¤§å°ä¸º$[1,2]$æ—¶ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º$[10,5]$

&emsp;&emsp;ç¤ºä¾‹ä¸­$X.shape = [8, 8]$ï¼Œè®¡ç®—å¾—å‡º$out_{shape} = [(8-3+4+1)/1, (8-5+6+2)/2] = [10, 5.5]$,å‘ä¸‹å–æ•´ï¼Œæ‰€ä»¥ä¸º$[10, 5]$

### ç»ƒä¹  6.3.3

å¯¹äºéŸ³é¢‘ä¿¡å·ï¼Œæ­¥å¹…$2$è¯´æ˜ä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¯¹äºéŸ³é¢‘ä¿¡å·è€Œè¨€ï¼Œæ­¥å¹…ä¸º2å°±æ˜¯ä»¥2ä¸ºå‘¨æœŸå¯¹ä¿¡å·è¿›è¡Œé‡‡æ ·è®¡ç®—ã€‚

### ç»ƒä¹  6.3.4

æ­¥å¹…å¤§äº$1$çš„è®¡ç®—ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å‡å°è®¡ç®—é‡ï¼Œ å‡å°å†…å­˜å ç”¨ï¼Œ æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## 6.4 å¤šè¾“å…¥å¤šè¾“å‡ºé€šé“ 

### ç»ƒä¹  6.4.1

å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªå·ç§¯æ ¸ï¼Œå¤§å°åˆ†åˆ«ä¸º$k_1$å’Œ$k_2$ï¼ˆä¸­é—´æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼‰ã€‚
1. è¯æ˜è¿ç®—å¯ä»¥ç”¨å•æ¬¡å·ç§¯æ¥è¡¨ç¤ºã€‚
1. è¿™ä¸ªç­‰æ•ˆçš„å•ä¸ªå·ç§¯æ ¸çš„ç»´æ•°æ˜¯å¤šå°‘å‘¢ï¼Ÿ
1. åä¹‹äº¦ç„¶å—ï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;å‡è®¾è¾“å…¥çš„å›¾åƒå¤§å°ä¸º$WÃ—H$ï¼Œè®¾å·ç§¯æ ¸1çš„å¤§å°ä¸º$k_1$ï¼Œå·ç§¯æ ¸2çš„å¤§å°ä¸º$k2$ï¼Œå®ƒä»¬åˆ†åˆ«ä½œç”¨äºè¾“å…¥çŸ©é˜µ$x$ï¼Œå¾—åˆ°çš„è¾“å‡ºçŸ©é˜µåˆ†åˆ«ä¸º$y_1$å’Œ$y_2$ã€‚åˆ™å¯ä»¥å°†$y1$ä¸$y2$çš„æ¯ä¸€ä¸ªå…ƒç´ ç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºçŸ©é˜µ$y$ã€‚

&emsp;&emsp;å³ï¼š$$y[i][j] = y_1[i][j] + y_2[i][j]$$

&emsp;&emsp;å¯ä»¥å°†ä¸¤ä¸ªå·ç§¯æ ¸çš„å¤§å°ç›¸åŠ ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„å·ç§¯æ ¸å¤§å°ä¸º$(k_1+k_2-1)Ã—(k_1+k_2-1)$ã€‚ç„¶åå¯ä»¥å°†è¿™ä¸ªæ–°çš„å·ç§¯æ ¸åº”ç”¨äºè¾“å…¥å›¾åƒï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºå›¾åƒã€‚è¿™ä¸ªè¾“å‡ºå›¾åƒçš„å¤§å°ä¸º$(W-k_1-k_2+2)Ã—(H-k_1-k_2+2)$ã€‚

**ç¬¬2é—®ï¼š**

&emsp;&emsp;å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¤§å°ä¸º$(k_1+k_2-1)Ã—(k_1+k_2-1)$çš„å·ç§¯æ ¸æ¥è¡¨ç¤ºè¿™ä¸¤ä¸ªå·ç§¯æ ¸çš„è¿ç®—ã€‚

**ç¬¬3é—®ï¼š**

&emsp;&emsp;åä¹‹äº¦ç„¶ã€‚å¦‚æœæœ‰ä¸€ä¸ªå¤§å°ä¸º$k_1+k_2-1$çš„å·ç§¯æ ¸ï¼Œå¯ä»¥å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªå¤§å°åˆ†åˆ«ä¸º$k1$å’Œ$k2$çš„å·ç§¯æ ¸ã€‚è¿™ä¸¤ä¸ªå·ç§¯æ ¸ä¹‹é—´æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæ‰€ä»¥å®ƒä»¬çš„è¿ç®—å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„å·ç§¯æ ¸ã€‚

### ç»ƒä¹  6.4.2

å‡è®¾è¾“å…¥ä¸º$c_i\times h\times w$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º$c_o\times c_i\times k_h\times k_w$ï¼Œå¡«å……ä¸º$(p_h, p_w)$ï¼Œæ­¥å¹…ä¸º$(s_h, s_w)$ã€‚
1. å‰å‘ä¼ æ’­çš„è®¡ç®—æˆæœ¬ï¼ˆä¹˜æ³•å’ŒåŠ æ³•ï¼‰æ˜¯å¤šå°‘ï¼Ÿ
1. å†…å­˜å ç”¨æ˜¯å¤šå°‘ï¼Ÿ
1. åå‘ä¼ æ’­çš„å†…å­˜å ç”¨æ˜¯å¤šå°‘ï¼Ÿ
1. åå‘ä¼ æ’­çš„è®¡ç®—æˆæœ¬æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;å‰å‘è®¡ç®—æˆæœ¬ä¸º

$flops_{forward} = c_i \times c_o \times k_h \times k_w \times m_h \times m_w$

&emsp;&emsp;å…¶ä¸­$m_h=\lfloor \frac{h+2p_h-k_h}{s_h}+1 \rfloor$, $m_w=\lfloor \frac{w+2p_w-k_w}{s_w}+1 \rfloor$ 


**ç¬¬2é—®ï¼š**

&emsp;&emsp;è¾“å…¥éœ€è¦$c_i*h*w$ä¸ªæµ®ç‚¹æ•°ï¼Œå·ç§¯æ ¸éœ€è¦$c_o*c_i*k_h*k_w$ä¸ªæµ®ç‚¹æ•°ï¼Œè¾“å‡ºéœ€è¦$c_o*m_h*m_w$ä¸ªæµ®ç‚¹æ•°ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦å­˜å‚¨ä¸­é—´ç»“æœï¼Œå³å¡«å……åçš„è¾“å…¥å’Œåå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦ä¿¡æ¯ã€‚å› æ­¤ï¼Œæ€»å†…å­˜å ç”¨ä¸º
$$memory_{forward}=(c_i+k_h-1)*(h+k_w-1)*c_0+2*c_i*h*w$$

**ç¬¬3é—®ï¼š**

&emsp;&emsp;åå‘ä¼ æ’­çš„å†…å­˜ä½œç”¨ä¸å‰å‘ä¼ æ’­ç›¸åŒï¼Œæ€»å†…å­˜å ç”¨ä¸º
$$memory_{backward}=(c_i+k_h-1)*(h+k_w-1)*c_0+2*c_i*h*w$$

**ç¬¬4é—®ï¼š**

&emsp;&emsp;åå‘è®¡ç®—æˆæœ¬ä¸º

$flops_{backward} = c_i \times c_o \times k_h \times k_w \times m_h \times m_w$

&emsp;&emsp;å…¶ä¸­$m_h$å’Œ$m_w$çš„å®šä¹‰åŒä¸Š

### ç»ƒä¹  6.4.3

å¦‚æœæˆ‘ä»¬å°†è¾“å…¥é€šé“$c_i$å’Œè¾“å‡ºé€šé“$c_o$çš„æ•°é‡åŠ å€ï¼Œè®¡ç®—æ•°é‡ä¼šå¢åŠ å¤šå°‘ï¼Ÿå¦‚æœæˆ‘ä»¬æŠŠå¡«å……æ•°é‡ç¿»ä¸€ç•ªä¼šæ€ä¹ˆæ ·ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¦‚æœæˆ‘ä»¬å°†è¾“å…¥é€šé“$c_i$å’Œè¾“å‡ºé€šé“$c_o$çš„æ•°é‡åŠ å€ï¼Œè®¡ç®—æ•°é‡ä¼šå¢åŠ $4$å€ã€‚å¦‚æœæˆ‘ä»¬æŠŠå¡«å……æ•°é‡ç¿»ä¸€ç•ªï¼Œè®¡ç®—æ•°é‡ä¼šå¢åŠ $2$å€ã€‚

### ç»ƒä¹  6.4.4

å¦‚æœå·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦æ˜¯$k_h=k_w=1$ï¼Œå‰å‘ä¼ æ’­çš„è®¡ç®—å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š**

$$flops = c_i \times c_o \times \frac{h-p_h}{s_h} \times \frac{w-p_w}{s_w}$$

### ç»ƒä¹  6.4.5

æœ¬èŠ‚æœ€åä¸€ä¸ªç¤ºä¾‹ä¸­çš„å˜é‡`Y1`å’Œ`Y2`æ˜¯å¦å®Œå…¨ç›¸åŒï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š**

&emsp;&emsp;æµ®ç‚¹æ•°è®¡ç®—æœ‰è¯¯å·®ï¼Œå› è€Œä¸¤è€…ä¸å®Œå…¨ç›¸åŒã€‚

### ç»ƒä¹  6.4.6

å½“å·ç§¯çª—å£ä¸æ˜¯$1\times 1$æ—¶ï¼Œå¦‚ä½•ä½¿ç”¨çŸ©é˜µä¹˜æ³•å®ç°å·ç§¯ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¯ä»¥å°†è¾“å…¥å¼ é‡å’Œå·ç§¯æ ¸å¼ é‡åˆ†åˆ«å±•å¼€ä¸ºäºŒç»´çŸ©é˜µï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—ï¼Œå¾—åˆ°çš„ç»“æœå†å˜æ¢ä¸ºè¾“å‡ºå¼ é‡ã€‚

## 6.5 æ±‡èšå±‚ 

### ç»ƒä¹  6.5.1

å°è¯•å°†å¹³å‡æ±‡èšå±‚ä½œä¸ºå·ç§¯å±‚çš„ç‰¹æ®Šæƒ…å†µå®ç°ã€‚

**è§£ç­”ï¼š** 


```python
import torch.nn as nn 
import torch.nn.functional as F

class Net(nn.Module): 
    def init(self): 
      super(Net, self).init() 
      self.conv1 = nn.Conv2d(1, 6, 5) 
      self.pool = nn.Conv2d(6, 6, 5) 
      # å¹³å‡æ± åŒ–å±‚ 
      self.conv2 = nn.Conv2d(6, 16, 5) 
      self.fc1 = nn.Linear(16 * 5 * 5, 120) 
      self.fc2 = nn.Linear(120, 84) 
      self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.avg_pool2d(x, (2, 2)) # å¹³å‡æ± åŒ–å±‚
        x = F.relu(self.conv2(x))
        x = F.avg_pool2d(x, (2, 2)) # å¹³å‡æ± åŒ–å±‚
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
```

### ç»ƒä¹  6.5.2

å°è¯•å°†æœ€å¤§æ±‡èšå±‚ä½œä¸ºå·ç§¯å±‚çš„ç‰¹æ®Šæƒ…å†µå®ç°ã€‚

**è§£ç­”ï¼š** 


```python
import torch.nn as nn 
import torch.nn.functional as F

class Net(nn.Module): 
    def init(self): 
        super(Net, self).init() 
        self.conv1 = nn.Conv2d(1, 6, 5) 
        self.pool = nn.Conv2d(6, 6, 5) 
        # æœ€å¤§æ± åŒ–å±‚ 
        self.conv2 = nn.Conv2d(6, 16, 5) 
        self.fc1 = nn.Linear(16 * 5 * 5, 120) 
        self.fc2 = nn.Linear(120, 84) 
        self.fc3 = nn.Linear(84, 10)  
        
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, (2, 2)) # æœ€å¤§æ± åŒ–å±‚
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, (2, 2)) # æœ€å¤§æ± åŒ–å±‚
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
```

### ç»ƒä¹  6.5.3

å‡è®¾æ±‡èšå±‚çš„è¾“å…¥å¤§å°ä¸º$c\times h\times w$ï¼Œåˆ™æ±‡èšçª—å£çš„å½¢çŠ¶ä¸º$p_h\times p_w$ï¼Œå¡«å……ä¸º$(p_h, p_w)$ï¼Œæ­¥å¹…ä¸º$(s_h, s_w)$ã€‚è¿™ä¸ªæ±‡èšå±‚çš„è®¡ç®—æˆæœ¬æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š** 

$$flops=\frac{c \times h \times w \times p_h \times p_w}{s_h \times s_w}$$

### ç»ƒä¹  6.5.4

ä¸ºä»€ä¹ˆæœ€å¤§æ±‡èšå±‚å’Œå¹³å‡æ±‡èšå±‚çš„å·¥ä½œæ–¹å¼ä¸åŒï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;æœ€å¤§æ± åŒ–å±‚å’Œå¹³å‡æ± åŒ–å±‚çš„å·¥ä½œæ–¹å¼ä¸åŒï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨ä¸åŒçš„æ± åŒ–æ–¹æ³•ã€‚æœ€å¤§æ± åŒ–å±‚å°†è¾“å…¥å¼ é‡åˆ†æˆä¸é‡å çš„åŒºåŸŸï¼Œå¹¶åœ¨æ¯ä¸ªåŒºåŸŸä¸­é€‰æ‹©æœ€å¤§å€¼ã€‚å¹³å‡æ± åŒ–å±‚å°†è¾“å…¥å¼ é‡åˆ†æˆä¸é‡å çš„åŒºåŸŸï¼Œå¹¶è®¡ç®—æ¯ä¸ªåŒºåŸŸçš„å¹³å‡å€¼ã€‚è¿™äº›æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬å¦‚ä½•å¤„ç†è¾“å…¥å¼ é‡ä¸­çš„ä¿¡æ¯ã€‚æœ€å¤§æ± åŒ–å±‚é€šå¸¸ç”¨äºæå–è¾“å…¥å¼ é‡ä¸­çš„æ˜¾è‘—ç‰¹å¾ï¼Œè€Œå¹³å‡æ± åŒ–å±‚é€šå¸¸ç”¨äºå‡å°‘è¾“å…¥å¼ é‡çš„å¤§å°å¹¶æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚

### ç»ƒä¹  6.5.5

æˆ‘ä»¬æ˜¯å¦éœ€è¦æœ€å°æ±‡èšå±‚ï¼Ÿå¯ä»¥ç”¨å·²çŸ¥å‡½æ•°æ›¿æ¢å®ƒå—ï¼Ÿ

**è§£ç­”ï¼š** 


```python
import torch.nn.functional as F

def min_pool2d(x, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False): 
    neg_x = -x 
    neg_min_pool = F.max_pool2d(neg_x, kernel_size, stride=stride, padding=padding, dilation=dilation, ceil_mode=ceil_mode) 
    min_pool = -neg_min_pool 
    return min_pool
```

### ç»ƒä¹  6.5.6

é™¤äº†å¹³å‡æ±‡èšå±‚å’Œæœ€å¤§æ±‡èšå±‚ï¼Œæ˜¯å¦æœ‰å…¶å®ƒå‡½æ•°å¯ä»¥è€ƒè™‘ï¼ˆæç¤ºï¼šå›æƒ³ä¸€ä¸‹`softmax`ï¼‰ï¼Ÿä¸ºä»€ä¹ˆå®ƒä¸æµè¡Œï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;é™¤äº†å¹³å‡æ±‡èšå±‚å’Œæœ€å¤§æ±‡èšå±‚ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–çš„æ± åŒ–å‡½æ•°ï¼Œä¾‹å¦‚Lpæ± åŒ–å’Œéšæœºæ± åŒ–ã€‚Softmaxå‡½æ•°é€šå¸¸ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼Œå®ƒå°†æ¯ä¸ªè¾“å‡ºåˆ†ç±»çš„ç»“æœèµ‹äºˆä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œè¡¨ç¤ºå±äºæ¯ä¸ªç±»åˆ«çš„å¯èƒ½æ€§ã€‚ä½†æ˜¯ï¼ŒSoftmaxå‡½æ•°ä¸é€‚ç”¨äºæ± åŒ–å±‚ï¼Œå› ä¸ºå®ƒä¼šå°†æ‰€æœ‰è¾“å…¥æ•°æ®è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚å› æ­¤ï¼ŒSoftmaxå‡½æ•°ä¸æµè¡Œç”¨äºæ± åŒ–å±‚ã€‚

## 6.6 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆLeNetï¼‰ 

### ç»ƒä¹  6.6.1

å°†å¹³å‡æ±‡èšå±‚æ›¿æ¢ä¸ºæœ€å¤§æ±‡èšå±‚ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;è¾“å‡ºæ›´å¤§ï¼Œæ¢¯åº¦æ›´å¤§ï¼Œè®­ç»ƒæ›´å®¹æ˜“ï¼ˆAlexNetæ”¹è¿›çš„æ–¹å¼ä¹‹ä¸€ï¼‰

&emsp;&emsp; LeNetåŸå§‹ä»£ç å¦‚ä¸‹ï¼š


```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```


```python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

    Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
    Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])
    AvgPool2d output shape: 	 torch.Size([1, 6, 14, 14])
    Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
    Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])
    AvgPool2d output shape: 	 torch.Size([1, 16, 5, 5])
    Flatten output shape: 	 torch.Size([1, 400])
    Linear output shape: 	 torch.Size([1, 120])
    Sigmoid output shape: 	 torch.Size([1, 120])
    Linear output shape: 	 torch.Size([1, 84])
    Sigmoid output shape: 	 torch.Size([1, 84])
    Linear output shape: 	 torch.Size([1, 10])



```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

    /usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(_create_warning_msg(



```python
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """ä½¿ç”¨GPUè®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„ç²¾åº¦"""
    if isinstance(net, nn.Module):
        net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        if not device:
            device = next(iter(net.parameters())).device
    # æ­£ç¡®é¢„æµ‹çš„æ•°é‡ï¼Œæ€»é¢„æµ‹çš„æ•°é‡
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERTå¾®è°ƒæ‰€éœ€çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```


```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼Œæ ·æœ¬æ•°
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```


```python
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.463, train acc 0.825, test acc 0.779
    41852.5 examples/sec on cuda:0




![svg](ch06-6-1-1-original.svg)
    


&emsp;&emsp; ä»£ç éªŒè¯å¦‚ä¸‹ï¼š


```python
net_maxpool = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```


```python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net_maxpool:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

    Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
    Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])
    MaxPool2d output shape: 	 torch.Size([1, 6, 14, 14])
    Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
    Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])
    MaxPool2d output shape: 	 torch.Size([1, 16, 5, 5])
    Flatten output shape: 	 torch.Size([1, 400])
    Linear output shape: 	 torch.Size([1, 120])
    Sigmoid output shape: 	 torch.Size([1, 120])
    Linear output shape: 	 torch.Size([1, 84])
    Sigmoid output shape: 	 torch.Size([1, 84])
    Linear output shape: 	 torch.Size([1, 10])



```python
lr, num_epochs = 0.9, 10
train_ch6(net_maxpool, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.409, train acc 0.850, test acc 0.834
    43916.5 examples/sec on cuda:0




![svg](ch06-6-1-2-maxpool.svg)
    


&emsp;&emsp; å¯è§ç”¨æœ€å¤§æ±‡èšå±‚æ›¿æ¢å¹³å‡æ±‡èšå±‚åï¼Œåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šå‡å¾—åˆ°äº†æ›´å¥½çš„ç»“æœ

### ç»ƒä¹  6.6.2

å°è¯•æ„å»ºä¸€ä¸ªåŸºäºLeNetçš„æ›´å¤æ‚çš„ç½‘ç»œï¼Œä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚
1. è°ƒæ•´å·ç§¯çª—å£å¤§å°ã€‚
1. è°ƒæ•´è¾“å‡ºé€šé“çš„æ•°é‡ã€‚
1. è°ƒæ•´æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰ã€‚
1. è°ƒæ•´å·ç§¯å±‚çš„æ•°é‡ã€‚
1. è°ƒæ•´å…¨è¿æ¥å±‚çš„æ•°é‡ã€‚
1. è°ƒæ•´å­¦ä¹ ç‡å’Œå…¶ä»–è®­ç»ƒç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œåˆå§‹åŒ–å’Œè½®æ•°ï¼‰ã€‚

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp; `nn.Conv2d(1, 6, kernel_size = 7)`

**ç¬¬2é—®ï¼š**

&emsp;&emsp; `nn.Conv2d(1, 10, kernel_size = 5)`

**ç¬¬3é—®ï¼š**

&emsp;&emsp; `nn.Conv2d(1, 6, kernel_size = 5).ReLU()`æˆ–è€…ç›´æ¥å°†`nn.Sigmoid()`æ”¹ä¸º`nn.ReLU()`

**ç¬¬4é—®ï¼š**

&emsp;&emsp; æ·»åŠ `conv3`ä¸º`nn.Cov2d(16, 120, kernel_size = 5). ReLU()`

**ç¬¬5é—®ï¼š**

&emsp;&emsp; æ·»åŠ `nn.Linear(84, 20)`å¹¶å°†`nn.Linear(84, 10)`æ›¿æ¢ä¸º`nn.Linear(20, 10)`ï¼Œæ·»åŠ `nn.Sigmoid()`

**ç¬¬6é—®ï¼š**

&emsp;&emsp; `lr, num_epochs = 0.1, 50`

### ç»ƒä¹  6.6.3

åœ¨MNISTæ•°æ®é›†ä¸Šå°è¯•ä»¥ä¸Šæ”¹è¿›çš„ç½‘ç»œã€‚

**è§£ç­”ï¼š** 


```python
net_improve = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=3, padding=2), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 10, kernel_size=3), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(10, 16, kernel_size=3), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 2 * 2, 120), nn.ReLU(),
    nn.Linear(120, 84), nn.ReLU(),
    nn.Linear(84, 20), nn.ReLU(),
    nn.Linear(20, 10))
```


```python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net_improve:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

    Conv2d output shape: 	 torch.Size([1, 6, 30, 30])
    ReLU output shape: 	 torch.Size([1, 6, 30, 30])
    AvgPool2d output shape: 	 torch.Size([1, 6, 15, 15])
    Conv2d output shape: 	 torch.Size([1, 10, 13, 13])
    ReLU output shape: 	 torch.Size([1, 10, 13, 13])
    AvgPool2d output shape: 	 torch.Size([1, 10, 6, 6])
    Conv2d output shape: 	 torch.Size([1, 16, 4, 4])
    ReLU output shape: 	 torch.Size([1, 16, 4, 4])
    AvgPool2d output shape: 	 torch.Size([1, 16, 2, 2])
    Flatten output shape: 	 torch.Size([1, 64])
    Linear output shape: 	 torch.Size([1, 120])
    ReLU output shape: 	 torch.Size([1, 120])
    Linear output shape: 	 torch.Size([1, 84])
    ReLU output shape: 	 torch.Size([1, 84])
    Linear output shape: 	 torch.Size([1, 20])
    ReLU output shape: 	 torch.Size([1, 20])
    Linear output shape: 	 torch.Size([1, 10])



```python
lr, num_epochs = 0.1, 50
train_ch6(net_improve, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.275, train acc 0.897, test acc 0.825
    30035.9 examples/sec on cuda:0




![svg](ch06-6-3-1-improve.svg)
    


&emsp;&emsp; æ”¹è¿›ä¹‹åçš„ç½‘ç»œåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šå‡è¡¨ç°å‡ºäº†æ›´å¥½çš„æ•ˆæœ

### ç»ƒä¹  6.6.4

æ˜¾ç¤ºä¸åŒè¾“å…¥ï¼ˆä¾‹å¦‚æ¯›è¡£å’Œå¤–å¥—ï¼‰æ—¶ï¼ŒLeNetç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„æ¿€æ´»å€¼ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp; é€šè¿‡åœ¨æ‰€æœ‰epochç»“æŸåæ·»åŠ `d2l.show_images`å±•ç¤ºäº†æ¿€æ´»å€¼


```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼Œæ ·æœ¬æ•°
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    x_first_Sigmoid_layer = net[0:2](X)[0:9, 1, :, :]
    d2l.show_images(x_first_Sigmoid_layer.reshape(9, 28, 28).cpu().detach(), 1, 9)
    x_second_Sigmoid_layer = net[0:5](X)[0:9, 1, :, :]
    d2l.show_images(x_second_Sigmoid_layer.reshape(9, 10, 10).cpu().detach(), 1, 9)
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```


```python
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.475, train acc 0.821, test acc 0.815
    44287.0 examples/sec on cuda:0




![svg](ch06-6-4-1-show.svg)
    




![svg](ch06-6-4-2-row1.svg)
    




![svg](ch06-6-4-3-row2.svg)
    

