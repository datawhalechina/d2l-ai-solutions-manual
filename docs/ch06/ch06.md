# ç¬¬6ç«  å·ç§¯ç¥ç»ç½‘ç»œ

## 6.1 ä»å…¨è¿æ¥å±‚åˆ°å·ç§¯

### ç»ƒä¹  6.1.1

å‡è®¾å·ç§¯å±‚(6.3)è¦†ç›–çš„å±€éƒ¨åŒºåŸŸ$\Delta = 0$ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯æ˜å·ç§¯å†…æ ¸ä¸ºæ¯ç»„é€šé“ç‹¬ç«‹åœ°å®ç°ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp;å±€éƒ¨åŒºåŸŸ$\Delta=0$ è¡¨ç¤ºå·ç§¯æ ¸çš„å¤§å°ç­‰äºè¾“å…¥çš„å¤§å°ã€‚å®é™…å°±æ˜¯é—®ï¼Œ1Ã—1çš„å·ç§¯æ ¸æ˜¯å¦ç­‰ä»·äºå…¨è¿æ¥ï¼ˆå‚è§æœ¬ä¹¦7.3èŠ‚ï¼šNiNç½‘ç»œç»“æ„ï¼‰ã€‚å› æ­¤ï¼Œæ¯ä¸ªå·ç§¯æ ¸åªèƒ½è¦†ç›–ä¸€ä¸ªåƒç´ ç‚¹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå·ç§¯å±‚çš„è®¡ç®—æ–¹å¼ä¸å…¨è¿æ¥å±‚éå¸¸ç›¸ä¼¼ã€‚å› ä¸ºæ¯ä¸ªå·ç§¯æ ¸åªèƒ½çœ‹åˆ°ä¸€ä¸ªé€šé“çš„ä¿¡æ¯ï¼Œç›¸å½“äºæ¯ä¸ªå·ç§¯æ ¸åªæ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æƒé‡çŸ©é˜µã€‚ æ‰€ä»¥ï¼Œå·ç§¯å†…æ ¸å¯ä»¥çœ‹ä½œæ˜¯æ¯ç»„é€šé“ç‹¬ç«‹åœ°å®ç°ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚æ¯ä¸ªå·ç§¯æ ¸éƒ½æœ‰è‡ªå·±çš„æƒé‡ï¼Œæ¯ä¸ªè¾“å…¥é€šé“éƒ½è¢«ç‹¬ç«‹å¤„ç†ï¼Œè¾“å‡ºé€šé“æ˜¯å„ä¸ªè¾“å…¥é€šé“çš„åŠ æƒå’Œã€‚è¿™ç§ç‹¬ç«‹å¤„ç†çš„æ–¹å¼æœ‰æ•ˆåœ°å‡å°‘äº†æƒé‡çš„æ•°é‡ï¼Œä»è€Œé™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶ä¸”èƒ½å¤Ÿæå–å‡ºè¾“å…¥æ•°æ®ä¸­çš„ç©ºé—´ç‰¹å¾ã€‚


```python
# ä»£ç éªŒè¯
import torch
import torch.nn as nn


class MyNet1(nn.Module):
    def __init__(self, linear1, linear2):
        super(MyNet1, self).__init__()
        self.linear1 = linear1
        self.linear2 = linear2

    def forward(self, X):
        return self.linear2(self.linear1(nn.Flatten()(X)))


class MyNet2(nn.Module):
    def __init__(self, linear, conv2d):
        super(MyNet2, self).__init__()
        self.linear = linear
        self.conv2d = conv2d

    def forward(self, X):
        X = self.linear(nn.Flatten()(X))
        X = X.reshape(X.shape[0], -1, 1, 1)
        X = nn.Flatten()(self.conv2d(X))
        return X


linear1 = nn.Linear(15, 10)
linear2 = nn.Linear(10, 5)
conv2d = nn.Conv2d(10, 5, 1)

linear2.weight = nn.Parameter(conv2d.weight.reshape(linear2.weight.shape))
linear2.bias = nn.Parameter(conv2d.bias)

net1 = MyNet1(linear1, linear2)
net2 = MyNet2(linear1, conv2d)

X = torch.randn(2, 3, 5)
# ä¸¤ä¸ªç»“æœå®é™…å­˜åœ¨ä¸€å®šçš„è¯¯å·®ï¼Œç›´æ¥print(net1(X) == net2(X))å¾—åˆ°çš„ç»“æœä¸å…¨æ˜¯True
print(net1(X))
print(net2(X))
```

    tensor([[0.1190, 0.2377, 0.1443, 0.1020, 0.0702],
            [0.1301, 0.2734, 0.1215, 0.0839, 0.1271]], grad_fn=<AddmmBackward0>)
    tensor([[0.1190, 0.2377, 0.1443, 0.1020, 0.0702],
            [0.1301, 0.2734, 0.1215, 0.0839, 0.1271]],
           grad_fn=<ReshapeAliasBackward0>)


### ç»ƒä¹  6.1.2

ä¸ºä»€ä¹ˆå¹³ç§»ä¸å˜æ€§å¯èƒ½ä¹Ÿä¸æ˜¯å¥½ä¸»æ„å‘¢ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¹³ç§»ä¸å˜æ€§å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚å¯¹äºæŸäº›ä»»åŠ¡ï¼Œå¹³ç§»ä¸å˜æ€§å¹¶ä¸æ˜¯å¿…é¡»çš„ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«ç‰©ä½“çš„ä½ç½®å’Œå§¿æ€ï¼Œå¹¶æ ¹æ®è¿™äº›ä¿¡æ¯å¯¹å…¶è¿›è¡Œåˆ†ç±»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¹³ç§»ä¸å˜æ€§å¯èƒ½ä¼šé™ä½æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå› ä¸ºå®ƒå¿½ç•¥äº†ç‰©ä½“çš„ä½ç½®å’Œå§¿æ€ç­‰é‡è¦ä¿¡æ¯ã€‚ å…¶æ¬¡ï¼Œå¹³ç§»ä¸å˜æ€§å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚

&emsp;&emsp;å‚è€ƒï¼š[https://arxiv.org/pdf/1805.12177.pdf](https://arxiv.org/pdf/1805.12177.pdf)

### ç»ƒä¹  6.1.3

å½“ä»å›¾åƒè¾¹ç•Œåƒç´ è·å–éšè—è¡¨ç¤ºæ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ€è€ƒå“ªäº›é—®é¢˜ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;è€ƒè™‘æ˜¯å¦å¡«å……`padding`ï¼Œä»¥åŠå¡«å……å¤šå¤§çš„`padding`çš„é—®é¢˜ã€‚å¯ä»¥ä½¿ç”¨`torch.nn`æ¨¡å—ä¸­çš„`functional.pad`å‡½æ•°å¯¹å›¾åƒè¿›è¡Œå¡«å……æ“ä½œï¼Œä»¥ä¿è¯è¾¹ç•Œåƒç´ çš„ä¿¡æ¯å®Œæ•´ã€‚å¡«å……åè¿˜éœ€è¦è¿›è¡Œé¢å¤–çš„å¤„ç†ï¼Œä¾‹å¦‚ä½¿ç”¨å›¾åƒå¤åˆ¶ã€å¡«å……ã€å¹³æ»‘ç­‰æ–¹æ³•æ¥è·å–éšè—è¡¨ç¤ºã€‚

### ç»ƒä¹  6.1.4

æè¿°ä¸€ä¸ªç±»ä¼¼çš„éŸ³é¢‘å·ç§¯å±‚çš„æ¶æ„ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp;ä¸€ç§åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„éŸ³é¢‘ç‰¹å¾ç”Ÿæˆæ–¹æ³•ï¼Œé¦–å…ˆå¯¹å£°éŸ³ä¿¡å·è¿›è¡Œé¢„å¤„ç†å’Œç¦»æ•£å‚…é‡Œå¶å˜æ¢è®¡ç®—å£°éŸ³ä¿¡å·çš„å¹…åº¦è°±ï¼Œå½¢æˆäºŒç»´è°±å›¾ä¿¡å·ï¼›ç„¶åæ­å»ºä»¥ä¸Šè¿°äºŒç»´è°±å›¾ä¿¡å·ä¸ºè¾“å…¥çš„ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œå¹¶è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¾—åˆ°ç‰¹å¾ç”Ÿæˆå™¨æ¨¡å‹ï¼›æœ€åå¯¹å¾…æµ‹å£°éŸ³è¿›è¡Œé¢„å¤„ç†å’Œç¦»æ•£å‚…é‡Œå¶å˜æ¢å¾—åˆ°äºŒç»´è°±å›¾ä¿¡å·ï¼Œå¹¶å°†å…¶é€å…¥è®­ç»ƒå¥½çš„ä¸€ç»´å·ç§¯ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å·ç§¯ç½‘ç»œè®¡ç®—ï¼Œå¾—åˆ°è¾“å‡ºå³ä¸ºæ‰€è¦ç”Ÿæˆçš„éŸ³é¢‘ç‰¹å¾ï¼Œå®ç°å£°éŸ³ä¿¡å·çš„éŸ³é¢‘ç‰¹å¾ç”Ÿæˆã€‚

### ç»ƒä¹  6.1.5

å·ç§¯å±‚ä¹Ÿé€‚åˆäºæ–‡æœ¬æ•°æ®å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å·ç§¯å±‚ä¹Ÿé€‚åˆäºæ–‡æœ¬æ•°æ®ã€‚ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæ–‡æœ¬æ•°æ®é€šå¸¸è¡¨ç¤ºä¸ºè¯å‘é‡çŸ©é˜µï¼Œå…¶ä¸­æ¯è¡Œä»£è¡¨ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤ºã€‚å·ç§¯å±‚å¯ä»¥åœ¨è¿™ä¸ªçŸ©é˜µä¸Šè¿›è¡Œå·ç§¯æ“ä½œï¼Œç±»ä¼¼äºå›¾åƒå·ç§¯å±‚ä¸­å¯¹å›¾åƒè¿›è¡Œå·ç§¯æ“ä½œã€‚ åœ¨å·ç§¯å±‚ä¸­ï¼Œå·ç§¯æ ¸ä¼šåœ¨è¾“å…¥çŸ©é˜µä¸Šè¿›è¡Œæ»‘åŠ¨çª—å£è®¡ç®—ï¼Œè¾“å‡ºä¸€ä¸ªæ–°çš„ç‰¹å¾çŸ©é˜µã€‚åœ¨æ–‡æœ¬æ•°æ®ä¸­ï¼Œè¿™ä¸ªç‰¹å¾çŸ©é˜µå¯ä»¥çœ‹ä½œæ˜¯å¯¹è¾“å…¥æ–‡æœ¬çš„ä¸åŒn-gramç‰¹å¾çš„æå–ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå¤§å°ä¸º3çš„å·ç§¯æ ¸å¯ä»¥æå–å‡ºè¾“å…¥æ–‡æœ¬ä¸­æ¯ä¸ªé•¿åº¦ä¸º3çš„n-gramç‰¹å¾ã€‚è¿™äº›ç‰¹å¾å¯ä»¥ç”¨äºåç»­çš„åˆ†ç±»æˆ–è€…å›å½’ä»»åŠ¡ã€‚ æ­¤å¤–ï¼Œå·ç§¯å±‚è¿˜å¯ä»¥ä¸å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ç»“åˆä½¿ç”¨ï¼Œå½¢æˆå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„æ··åˆæ¨¡å‹ã€‚è¿™ç§æ¨¡å‹å¯ä»¥åŒæ—¶æ•æ‰æ–‡æœ¬ä¸­çš„å±€éƒ¨ç‰¹å¾å’Œå…¨å±€ç‰¹å¾ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚ å› æ­¤ï¼Œå·ç§¯å±‚é€‚ç”¨äºæ–‡æœ¬æ•°æ®ï¼Œå¯ä»¥å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œå·ç§¯æ“ä½œï¼Œæå–å‡ºä¸åŒn-gramç‰¹å¾ï¼Œå¹¶ä¸”å¯ä»¥ä¸RNNç»“åˆä½¿ç”¨ï¼Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

### ç»ƒä¹  6.1.6

è¯æ˜åœ¨å¼(6.6)ä¸­ï¼Œ$f * g = g * f$ã€‚

**è§£ç­”ï¼š** 

&emsp;&emsp;é€šè¿‡å¼(6.6)çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$(f * g)(x) = \int_{-\infty}^{\infty}f(y)g(x-y)dy$$

$$(g * f)(x) = \int_{-\infty}^{\infty}g(y)f(x-y)dy$$

&emsp;&emsp;è¦è¯æ˜$f * g = g * f$ï¼Œå³è¯æ˜ï¼š

$$\int_{-\infty}^{\infty}f(y)g(x-y)dy = \int_{-\infty}^{\infty}g(y)f(x-y)dy$$

&emsp;&emsp;ä¸ºäº†è¯æ˜ä¸Šå¼æˆç«‹ï¼Œæˆ‘ä»¬å°†å…¶ä¸­ä¸€ä¸ªç§¯åˆ†çš„å˜é‡åæ”¹ä¸º$t=x-y$ï¼Œåˆ™æœ‰ï¼š

$$\int_{-\infty}^{\infty}f(y)g(x-y)dy = \int_{-\infty}^{\infty}f(x-t)g(t)dt$$

&emsp;&emsp;å†å°†è¿™ä¸ªå¼å­ä»£å›å¼(6.6)ä¸­ï¼š

$$(f * g)(x) = \int_{-\infty}^{\infty}f(x-t)g(t)dt$$

&emsp;&emsp;å¯¹æ¯”å¼(6.6)å’Œä¸Šé¢çš„å¼å­ï¼Œå¯ä»¥å‘ç°å®ƒä»¬çš„å½¢å¼æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œåªæ˜¯ç§¯åˆ†å˜é‡åä¸åŒè€Œå·²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š

$$(f * g)(x) = \int_{-\infty}^{\infty}f(y)g(x-y)dy = \int_{-\infty}^{\infty}g(y)f(x-y)dy = (g * f)(x)$$

&emsp;&emsp;å› æ­¤ï¼Œ$f * g = g * f$ï¼Œè¯æ¯•ã€‚



## 6.2 å›¾åƒå·ç§¯

### ç»ƒä¹  6.2.1  

æ„å»ºä¸€ä¸ªå…·æœ‰å¯¹è§’çº¿è¾¹ç¼˜çš„å›¾åƒ`X`ã€‚
1. å¦‚æœå°†æœ¬èŠ‚ä¸­ä¸¾ä¾‹çš„å·ç§¯æ ¸`K`åº”ç”¨äº`X`ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿ
1. å¦‚æœè½¬ç½®`X`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
1. å¦‚æœè½¬ç½®`K`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;åœ¨å¯¹è§’çº¿å¤„æœ‰åˆ†åˆ«ä¸º1å’Œ-1çš„æ•°æ®ï¼Œå…¶ä»–åŒºåŸŸéƒ½ä¸º0ã€‚


```python
import torch
from torch import nn
from d2l import torch as d2l

def corr2d(X, K):  #@save
    """è®¡ç®—äºŒç»´äº’ç›¸å…³è¿ç®—"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y

# å¦‚æœå°†æœ¬èŠ‚ä¸­ä¸¾ä¾‹çš„å·ç§¯æ ¸Kåº”ç”¨äºXï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿ
X = torch.eye(8)
K = torch.tensor([[1.0, -1.0]])
Y = corr2d(X, K)
print(Y)
```

    tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
            [-1.,  1.,  0.,  0.,  0.,  0.,  0.],
            [ 0., -1.,  1.,  0.,  0.,  0.,  0.],
            [ 0.,  0., -1.,  1.,  0.,  0.,  0.],
            [ 0.,  0.,  0., -1.,  1.,  0.,  0.],
            [ 0.,  0.,  0.,  0., -1.,  1.,  0.],
            [ 0.,  0.,  0.,  0.,  0., -1.,  1.],
            [ 0.,  0.,  0.,  0.,  0.,  0., -1.]])


**ç¬¬2é—®ï¼š**

&emsp;&emsp;è½¬ç½®åç»“æœä¸å˜ã€‚


```python
Y = corr2d(X.T, K)
X, Y
```




    (tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
             [0., 1., 0., 0., 0., 0., 0., 0.],
             [0., 0., 1., 0., 0., 0., 0., 0.],
             [0., 0., 0., 1., 0., 0., 0., 0.],
             [0., 0., 0., 0., 1., 0., 0., 0.],
             [0., 0., 0., 0., 0., 1., 0., 0.],
             [0., 0., 0., 0., 0., 0., 1., 0.],
             [0., 0., 0., 0., 0., 0., 0., 1.]]),
     tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
             [-1.,  1.,  0.,  0.,  0.,  0.,  0.],
             [ 0., -1.,  1.,  0.,  0.,  0.,  0.],
             [ 0.,  0., -1.,  1.,  0.,  0.,  0.],
             [ 0.,  0.,  0., -1.,  1.,  0.,  0.],
             [ 0.,  0.,  0.,  0., -1.,  1.,  0.],
             [ 0.,  0.,  0.,  0.,  0., -1.,  1.],
             [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]))



**ç¬¬3é—®ï¼š**

&emsp;&emsp;Kè½¬ç½®åï¼Œç»“æœä¹Ÿè½¬ç½®äº† 


```python
Y = corr2d(X, K.T)
X, Y
```




    (tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
             [0., 1., 0., 0., 0., 0., 0., 0.],
             [0., 0., 1., 0., 0., 0., 0., 0.],
             [0., 0., 0., 1., 0., 0., 0., 0.],
             [0., 0., 0., 0., 1., 0., 0., 0.],
             [0., 0., 0., 0., 0., 1., 0., 0.],
             [0., 0., 0., 0., 0., 0., 1., 0.],
             [0., 0., 0., 0., 0., 0., 0., 1.]]),
     tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],
             [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],
             [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.],
             [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.]]))



### ç»ƒä¹  6.2.2 

åœ¨æˆ‘ä»¬åˆ›å»ºçš„`Conv2D`è‡ªåŠ¨æ±‚å¯¼æ—¶ï¼Œæœ‰ä»€ä¹ˆé”™è¯¯æ¶ˆæ¯ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;é”™è¯¯ä¿¡æ¯ï¼šThe size of tensor a (0) must match the size of tensor b (7) at non-singleton dimension 3

&emsp;&emsp;ä¼šæç¤ºç»´åº¦ä¸å¯¹ç§°çš„é”™è¯¯ä¿¡æ¯ï¼Œå› ä¸ºtorchæä¾›çš„äºŒç»´å·ç§¯å±‚æ˜¯nn.Conv2d() é‡‡ç”¨çš„æ˜¯å››ç»´è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ï¼ˆæ‰¹é‡å¤§å°ã€é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼‰,è€Œæˆ‘ä»¬è‡ªå®šä¹‰çš„ä»…ä»…æ˜¯äºŒç»´çš„ã€‚

### ç»ƒä¹  6.2.3

å¦‚ä½•é€šè¿‡æ”¹å˜è¾“å…¥å¼ é‡å’Œå·ç§¯æ ¸å¼ é‡ï¼Œå°†äº’ç›¸å…³è¿ç®—è¡¨ç¤ºä¸ºçŸ©é˜µä¹˜æ³•ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;é¢˜ç›®çš„æ„æ€åº”è¯¥æ˜¯å¦‚ä½•é€šè¿‡çŸ©é˜µä¹˜æ³•å¾—åˆ°äº’ç›¸å…³ï¼ˆå·ç§¯ï¼‰è¿ç®—


```python
def conv2d_by_mul(X, K):
    # è·å–å·ç§¯æ ¸å¤§å°
    h, w = K.shape
    # è®¡ç®—è¾“å‡ºå›¾åƒå¤§å°
    outh = X.shape[0] - h + 1
    outw = X.shape[1] - w + 1
    # è°ƒæ•´å·ç§¯æ ¸å½¢çŠ¶ä»¥ä¾¿åšä¹˜æ³•
    K = K.reshape(-1, 1)
    # å°†è¾“å…¥å›¾åƒåˆ‡æˆå·ç§¯æ ¸å¤§å°çš„å—ï¼Œæ‰“å¹³æˆä¸€ç»´ï¼Œå­˜æ”¾åœ¨åˆ—è¡¨ Y ä¸­
    Y = []
    for i in range(outh):
        for j in range(outw):
            Y.append(X[i:i + h, j:j + w].reshape(-1))
    # å°†åˆ—è¡¨ Y è½¬ä¸ºå¼ é‡ï¼Œæ¯è¡Œä»£è¡¨ä¸€å—çš„æ‰“å¹³ç»“æœ
    Y = torch.stack(Y, 0)
    # ç”¨çŸ©é˜µä¹˜æ³•è¡¨ç¤ºäº’ç›¸å…³è¿ç®—
    res = (torch.matmul(Y, K)).reshape(outh, outw)
    # è¿”å›è¾“å‡ºç»“æœ
    return res
```

### ç»ƒä¹  6.2.4

æ‰‹å·¥è®¾è®¡ä¸€äº›å·ç§¯æ ¸ã€‚
1. äºŒé˜¶å¯¼æ•°çš„æ ¸çš„å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ
1. ç§¯åˆ†çš„æ ¸çš„å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ
1. å¾—åˆ°$d$æ¬¡å¯¼æ•°çš„æœ€å°æ ¸çš„å¤§å°æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;äºŒé˜¶å¯¼æ•°çš„æ ¸çš„å½¢å¼æ˜¯ï¼š

$$\begin{bmatrix}-1 & 2 & -1\end{bmatrix}$$

**ç¬¬2é—®ï¼š**

&emsp;&emsp;ç§¯åˆ†çš„æ ¸çš„å½¢å¼æ˜¯ï¼š

$$\begin{bmatrix}1 & 1 & 1 & \cdots & 1\end{bmatrix}$$


**ç¬¬3é—®ï¼š**

&emsp;&emsp;å¾—åˆ° ğ‘‘ æ¬¡å¯¼æ•°çš„æœ€å°æ ¸çš„å¤§å°æ˜¯ $d+1$ã€‚ä¾‹å¦‚ï¼Œä¸€é˜¶å¯¼æ•°çš„æœ€å°æ ¸å¤§å°ä¸º $2$ï¼ŒäºŒé˜¶å¯¼æ•°çš„æœ€å°æ ¸å¤§å°ä¸º $3$ï¼Œä¸‰é˜¶å¯¼æ•°çš„æœ€å°æ ¸å¤§å°ä¸º $4$ï¼Œä»¥æ­¤ç±»æ¨ã€‚

## 6.3 å¡«å……å’Œæ­¥å¹… 

### ç»ƒä¹  6.3.1  

å¯¹äºæœ¬èŠ‚ä¸­çš„æœ€åä¸€ä¸ªç¤ºä¾‹ï¼Œè®¡ç®—å…¶è¾“å‡ºå½¢çŠ¶ï¼Œä»¥æŸ¥çœ‹å®ƒæ˜¯å¦ä¸å®éªŒç»“æœä¸€è‡´ã€‚

**è§£ç­”ï¼š** 

$$out_{shape}=\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.$$

&emsp;&emsp;ç¤ºä¾‹ä¸­$X.shape = [8, 8]$ï¼Œè®¡ç®—å¾—å‡º$out_shape = [(8-3+0+3)/3, (8-5+1+4)/4] = [2.67, 2]$,å‘ä¸‹å–æ•´ï¼Œæ‰€ä»¥ä¸º$[2, 2]$



### ç»ƒä¹  6.3.2

åœ¨æœ¬èŠ‚ä¸­çš„å®éªŒä¸­ï¼Œè¯•ä¸€è¯•å…¶ä»–å¡«å……å’Œæ­¥å¹…ç»„åˆã€‚

**è§£ç­”ï¼š** 

ç•¥ã€‚

### ç»ƒä¹  6.3.3

å¯¹äºéŸ³é¢‘ä¿¡å·ï¼Œæ­¥å¹…$2$è¯´æ˜ä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¯¹äºéŸ³é¢‘ä¿¡å·è€Œè¨€ï¼Œæ­¥å¹…ä¸º2å°±æ˜¯ä»¥2ä¸ºå‘¨æœŸå¯¹ä¿¡å·è¿›è¡Œé‡‡æ ·è®¡ç®—ã€‚

### ç»ƒä¹  6.3.4

æ­¥å¹…å¤§äº$1$çš„è®¡ç®—ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å‡å°è®¡ç®—é‡ï¼Œ å‡å°å†…å­˜å ç”¨ï¼Œ æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

## 6.4 å¤šè¾“å…¥å¤šè¾“å‡ºé€šé“ 

### ç»ƒä¹  6.4.1

å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªå·ç§¯æ ¸ï¼Œå¤§å°åˆ†åˆ«ä¸º$k_1$å’Œ$k_2$ï¼ˆä¸­é—´æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼‰ã€‚
1. è¯æ˜è¿ç®—å¯ä»¥ç”¨å•æ¬¡å·ç§¯æ¥è¡¨ç¤ºã€‚
1. è¿™ä¸ªç­‰æ•ˆçš„å•ä¸ªå·ç§¯æ ¸çš„ç»´æ•°æ˜¯å¤šå°‘å‘¢ï¼Ÿ
1. åä¹‹äº¦ç„¶å—ï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;å‡è®¾è¾“å…¥çš„å›¾åƒå¤§å°ä¸º$WÃ—H$ï¼Œè®¾å·ç§¯æ ¸1çš„å¤§å°ä¸º$k_1$ï¼Œå·ç§¯æ ¸2çš„å¤§å°ä¸º$k2$ï¼Œå®ƒä»¬åˆ†åˆ«ä½œç”¨äºè¾“å…¥çŸ©é˜µ$x$ï¼Œå¾—åˆ°çš„è¾“å‡ºçŸ©é˜µåˆ†åˆ«ä¸º$y_1$å’Œ$y_2$ã€‚åˆ™å¯ä»¥å°†$y1$ä¸$y2$çš„æ¯ä¸€ä¸ªå…ƒç´ ç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºçŸ©é˜µ$y$ã€‚

&emsp;&emsp;å³ï¼š$$y[i][j] = y_1[i][j] + y_2[i][j]$$

&emsp;&emsp;å¯ä»¥å°†ä¸¤ä¸ªå·ç§¯æ ¸çš„å¤§å°ç›¸åŠ ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„å·ç§¯æ ¸å¤§å°ä¸º$(k_1+k_2-1)Ã—(k_1+k_2-1)$ã€‚ç„¶åå¯ä»¥å°†è¿™ä¸ªæ–°çš„å·ç§¯æ ¸åº”ç”¨äºè¾“å…¥å›¾åƒï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºå›¾åƒã€‚è¿™ä¸ªè¾“å‡ºå›¾åƒçš„å¤§å°ä¸º$(W-k_1-k_2+2)Ã—(H-k_1-k_2+2)$ã€‚

**ç¬¬2é—®ï¼š**

&emsp;&emsp;å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¤§å°ä¸º$(k_1+k_2-1)Ã—(k_1+k_2-1)$çš„å·ç§¯æ ¸æ¥è¡¨ç¤ºè¿™ä¸¤ä¸ªå·ç§¯æ ¸çš„è¿ç®—ã€‚

**ç¬¬3é—®ï¼š**

&emsp;&emsp;åä¹‹äº¦ç„¶ã€‚å¦‚æœæœ‰ä¸€ä¸ªå¤§å°ä¸º$k_1+k_2-1$çš„å·ç§¯æ ¸ï¼Œå¯ä»¥å°†å…¶åˆ†è§£ä¸ºä¸¤ä¸ªå¤§å°åˆ†åˆ«ä¸º$k1$å’Œ$k2$çš„å·ç§¯æ ¸ã€‚è¿™ä¸¤ä¸ªå·ç§¯æ ¸ä¹‹é—´æ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œæ‰€ä»¥å®ƒä»¬çš„è¿ç®—å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå•ç‹¬çš„å·ç§¯æ ¸ã€‚

### ç»ƒä¹  6.4.2

å‡è®¾è¾“å…¥ä¸º$c_i\times h\times w$ï¼Œå·ç§¯æ ¸å¤§å°ä¸º$c_o\times c_i\times k_h\times k_w$ï¼Œå¡«å……ä¸º$(p_h, p_w)$ï¼Œæ­¥å¹…ä¸º$(s_h, s_w)$ã€‚
1. å‰å‘ä¼ æ’­çš„è®¡ç®—æˆæœ¬ï¼ˆä¹˜æ³•å’ŒåŠ æ³•ï¼‰æ˜¯å¤šå°‘ï¼Ÿ
1. å†…å­˜å ç”¨æ˜¯å¤šå°‘ï¼Ÿ
1. åå‘ä¼ æ’­çš„å†…å­˜å ç”¨æ˜¯å¤šå°‘ï¼Ÿ
1. åå‘ä¼ æ’­çš„è®¡ç®—æˆæœ¬æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp;å‰å‘è®¡ç®—æˆæœ¬ä¸º

$flops_{forward} = c_i \times c_o \times k_h \times k_w \times m_h \times m_w$

&emsp;&emsp;å…¶ä¸­$m_h=\lfloor \frac{h+2p_h-k_h}{s_h}+1 \rfloor$, $m_w=\lfloor \frac{w+2p_w-k_w}{s_w}+1 \rfloor$ 


**ç¬¬2é—®ï¼š**

&emsp;&emsp;è¾“å…¥éœ€è¦$c_i*h*w$ä¸ªæµ®ç‚¹æ•°ï¼Œå·ç§¯æ ¸éœ€è¦$c_o*c_i*k_h*k_w$ä¸ªæµ®ç‚¹æ•°ï¼Œè¾“å‡ºéœ€è¦$c_o*m_h*m_w$ä¸ªæµ®ç‚¹æ•°ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦å­˜å‚¨ä¸­é—´ç»“æœï¼Œå³å¡«å……åçš„è¾“å…¥å’Œåå‘ä¼ æ’­æ—¶çš„æ¢¯åº¦ä¿¡æ¯ã€‚å› æ­¤ï¼Œæ€»å†…å­˜å ç”¨ä¸º
$$memory_{forward}=(c_i+k_h-1)*(h+k_w-1)*c_0+2*c_i*h*w$$

**ç¬¬3é—®ï¼š**

&emsp;&emsp;åå‘ä¼ æ’­çš„å†…å­˜ä½œç”¨ä¸å‰å‘ä¼ æ’­ç›¸åŒï¼Œæ€»å†…å­˜å ç”¨ä¸º
$$memory_{backward}=(c_i+k_h-1)*(h+k_w-1)*c_0+2*c_i*h*w$$

**ç¬¬4é—®ï¼š**

&emsp;&emsp;åå‘è®¡ç®—æˆæœ¬ä¸º

$flops_{backward} = c_i \times c_o \times k_h \times k_w \times m_h \times m_w$

&emsp;&emsp;å…¶ä¸­$m_h$å’Œ$m_w$çš„å®šä¹‰åŒä¸Š

### ç»ƒä¹  6.4.3

å¦‚æœæˆ‘ä»¬å°†è¾“å…¥é€šé“$c_i$å’Œè¾“å‡ºé€šé“$c_o$çš„æ•°é‡åŠ å€ï¼Œè®¡ç®—æ•°é‡ä¼šå¢åŠ å¤šå°‘ï¼Ÿå¦‚æœæˆ‘ä»¬æŠŠå¡«å……æ•°é‡ç¿»ä¸€ç•ªä¼šæ€ä¹ˆæ ·ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¦‚æœæˆ‘ä»¬å°†è¾“å…¥é€šé“$c_i$å’Œè¾“å‡ºé€šé“$c_o$çš„æ•°é‡åŠ å€ï¼Œè®¡ç®—æ•°é‡ä¼šå¢åŠ $4$å€ã€‚å¦‚æœæˆ‘ä»¬æŠŠå¡«å……æ•°é‡ç¿»ä¸€ç•ªï¼Œè®¡ç®—æ•°é‡ä¼šå¢åŠ $2$å€ã€‚

### ç»ƒä¹  6.4.4

å¦‚æœå·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦æ˜¯$k_h=k_w=1$ï¼Œå‰å‘ä¼ æ’­çš„è®¡ç®—å¤æ‚åº¦æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š**

$$flops = c_i \times c_o \times \frac{h-p_h}{s_h} \times \frac{w-p_w}{s_w}$$

### ç»ƒä¹  6.4.5

æœ¬èŠ‚æœ€åä¸€ä¸ªç¤ºä¾‹ä¸­çš„å˜é‡`Y1`å’Œ`Y2`æ˜¯å¦å®Œå…¨ç›¸åŒï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š**

&emsp;&emsp;æµ®ç‚¹æ•°è®¡ç®—æœ‰è¯¯å·®ï¼Œå› è€Œä¸¤è€…ä¸å®Œå…¨ç›¸åŒã€‚

### ç»ƒä¹  6.4.6

å½“å·ç§¯çª—å£ä¸æ˜¯$1\times 1$æ—¶ï¼Œå¦‚ä½•ä½¿ç”¨çŸ©é˜µä¹˜æ³•å®ç°å·ç§¯ï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;å¯ä»¥å°†è¾“å…¥å¼ é‡å’Œå·ç§¯æ ¸å¼ é‡åˆ†åˆ«å±•å¼€ä¸ºäºŒç»´çŸ©é˜µï¼Œç„¶åå¯¹è¿™ä¸¤ä¸ªçŸ©é˜µè¿›è¡Œä¹˜æ³•è¿ç®—ï¼Œå¾—åˆ°çš„ç»“æœå†å˜æ¢ä¸ºè¾“å‡ºå¼ é‡ã€‚

## 6.5 æ±‡èšå±‚ 

### ç»ƒä¹  6.5.1

å°è¯•å°†å¹³å‡æ±‡èšå±‚ä½œä¸ºå·ç§¯å±‚çš„ç‰¹æ®Šæƒ…å†µå®ç°ã€‚

**è§£ç­”ï¼š** 


```python
import torch.nn as nn 
import torch.nn.functional as F

class Net(nn.Module): 
    def init(self): 
      super(Net, self).init() 
      self.conv1 = nn.Conv2d(1, 6, 5) 
      self.pool = nn.Conv2d(6, 6, 5) 
      # å¹³å‡æ± åŒ–å±‚ 
      self.conv2 = nn.Conv2d(6, 16, 5) 
      self.fc1 = nn.Linear(16 * 5 * 5, 120) 
      self.fc2 = nn.Linear(120, 84) 
      self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.avg_pool2d(x, (2, 2)) # å¹³å‡æ± åŒ–å±‚
        x = F.relu(self.conv2(x))
        x = F.avg_pool2d(x, (2, 2)) # å¹³å‡æ± åŒ–å±‚
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
```

### ç»ƒä¹  6.5.2

å°è¯•å°†æœ€å¤§æ±‡èšå±‚ä½œä¸ºå·ç§¯å±‚çš„ç‰¹æ®Šæƒ…å†µå®ç°ã€‚

**è§£ç­”ï¼š** 


```python
import torch.nn as nn 
import torch.nn.functional as F

class Net(nn.Module): 
    def init(self): 
        super(Net, self).init() 
        self.conv1 = nn.Conv2d(1, 6, 5) 
        self.pool = nn.Conv2d(6, 6, 5) 
        # æœ€å¤§æ± åŒ–å±‚ 
        self.conv2 = nn.Conv2d(6, 16, 5) 
        self.fc1 = nn.Linear(16 * 5 * 5, 120) 
        self.fc2 = nn.Linear(120, 84) 
        self.fc3 = nn.Linear(84, 10)  
        
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, (2, 2)) # æœ€å¤§æ± åŒ–å±‚
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, (2, 2)) # æœ€å¤§æ± åŒ–å±‚
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features
```

### ç»ƒä¹  6.5.3

å‡è®¾æ±‡èšå±‚çš„è¾“å…¥å¤§å°ä¸º$c\times h\times w$ï¼Œåˆ™æ±‡èšçª—å£çš„å½¢çŠ¶ä¸º$p_h\times p_w$ï¼Œå¡«å……ä¸º$(p_h, p_w)$ï¼Œæ­¥å¹…ä¸º$(s_h, s_w)$ã€‚è¿™ä¸ªæ±‡èšå±‚çš„è®¡ç®—æˆæœ¬æ˜¯å¤šå°‘ï¼Ÿ

**è§£ç­”ï¼š** 

$$flops=\frac{c \times h \times w \times p_h \times p_w}{s_h \times s_w}$$

### ç»ƒä¹  6.5.4

ä¸ºä»€ä¹ˆæœ€å¤§æ±‡èšå±‚å’Œå¹³å‡æ±‡èšå±‚çš„å·¥ä½œæ–¹å¼ä¸åŒï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;æœ€å¤§æ± åŒ–å±‚å’Œå¹³å‡æ± åŒ–å±‚çš„å·¥ä½œæ–¹å¼ä¸åŒï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨ä¸åŒçš„æ± åŒ–æ–¹æ³•ã€‚æœ€å¤§æ± åŒ–å±‚å°†è¾“å…¥å¼ é‡åˆ†æˆä¸é‡å çš„åŒºåŸŸï¼Œå¹¶åœ¨æ¯ä¸ªåŒºåŸŸä¸­é€‰æ‹©æœ€å¤§å€¼ã€‚å¹³å‡æ± åŒ–å±‚å°†è¾“å…¥å¼ é‡åˆ†æˆä¸é‡å çš„åŒºåŸŸï¼Œå¹¶è®¡ç®—æ¯ä¸ªåŒºåŸŸçš„å¹³å‡å€¼ã€‚è¿™äº›æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬å¦‚ä½•å¤„ç†è¾“å…¥å¼ é‡ä¸­çš„ä¿¡æ¯ã€‚æœ€å¤§æ± åŒ–å±‚é€šå¸¸ç”¨äºæå–è¾“å…¥å¼ é‡ä¸­çš„æ˜¾è‘—ç‰¹å¾ï¼Œè€Œå¹³å‡æ± åŒ–å±‚é€šå¸¸ç”¨äºå‡å°‘è¾“å…¥å¼ é‡çš„å¤§å°å¹¶æé«˜æ¨¡å‹çš„è®¡ç®—æ•ˆç‡ã€‚

### ç»ƒä¹  6.5.5

æˆ‘ä»¬æ˜¯å¦éœ€è¦æœ€å°æ±‡èšå±‚ï¼Ÿå¯ä»¥ç”¨å·²çŸ¥å‡½æ•°æ›¿æ¢å®ƒå—ï¼Ÿ

**è§£ç­”ï¼š** 


```python
import torch.nn.functional as F

def min_pool2d(x, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False): 
    neg_x = -x 
    neg_min_pool = F.max_pool2d(neg_x, kernel_size, stride=stride, padding=padding, dilation=dilation, ceil_mode=ceil_mode) 
    min_pool = -neg_min_pool 
    return min_pool
```

### ç»ƒä¹  6.5.6

é™¤äº†å¹³å‡æ±‡èšå±‚å’Œæœ€å¤§æ±‡èšå±‚ï¼Œæ˜¯å¦æœ‰å…¶å®ƒå‡½æ•°å¯ä»¥è€ƒè™‘ï¼ˆæç¤ºï¼šå›æƒ³ä¸€ä¸‹`softmax`ï¼‰ï¼Ÿä¸ºä»€ä¹ˆå®ƒä¸æµè¡Œï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;é™¤äº†å¹³å‡æ±‡èšå±‚å’Œæœ€å¤§æ±‡èšå±‚ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–çš„æ± åŒ–å‡½æ•°ï¼Œä¾‹å¦‚Lpæ± åŒ–å’Œéšæœºæ± åŒ–ã€‚Softmaxå‡½æ•°é€šå¸¸ç”¨äºå¤šåˆ†ç±»é—®é¢˜ï¼Œå®ƒå°†æ¯ä¸ªè¾“å‡ºåˆ†ç±»çš„ç»“æœèµ‹äºˆä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œè¡¨ç¤ºå±äºæ¯ä¸ªç±»åˆ«çš„å¯èƒ½æ€§ã€‚ä½†æ˜¯ï¼ŒSoftmaxå‡½æ•°ä¸é€‚ç”¨äºæ± åŒ–å±‚ï¼Œå› ä¸ºå®ƒä¼šå°†æ‰€æœ‰è¾“å…¥æ•°æ®è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚å› æ­¤ï¼ŒSoftmaxå‡½æ•°ä¸æµè¡Œç”¨äºæ± åŒ–å±‚ã€‚

## 6.6 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆLeNetï¼‰ 

### ç»ƒä¹  6.6.1

å°†å¹³å‡æ±‡èšå±‚æ›¿æ¢ä¸ºæœ€å¤§æ±‡èšå±‚ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

**è§£ç­”ï¼š** 

&emsp;&emsp;è¾“å‡ºæ›´å¤§ï¼Œæ¢¯åº¦æ›´å¤§ï¼Œè®­ç»ƒæ›´å®¹æ˜“ï¼ˆAlexNetæ”¹è¿›çš„æ–¹å¼ä¹‹ä¸€ï¼‰

&emsp;&emsp; LeNetåŸå§‹ä»£ç å¦‚ä¸‹ï¼š


```python
!pip install d2l
```

    Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/
    Requirement already satisfied: d2l in /usr/local/lib/python3.9/dist-packages (0.17.6)
    Requirement already satisfied: matplotlib==3.5.1 in /usr/local/lib/python3.9/dist-packages (from d2l) (3.5.1)
    Requirement already satisfied: numpy==1.21.5 in /usr/local/lib/python3.9/dist-packages (from d2l) (1.21.5)
    Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.9/dist-packages (from d2l) (1.0.0)
    Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.9/dist-packages (from d2l) (2.25.1)
    Requirement already satisfied: pandas==1.2.4 in /usr/local/lib/python3.9/dist-packages (from d2l) (1.2.4)
    Requirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)
    Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (6.5.4)
    Requirement already satisfied: qtconsole in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (5.4.2)
    Requirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (6.4.8)
    Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (5.5.6)
    Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)
    Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (4.39.3)
    Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (0.11.0)
    Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (8.4.0)
    Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (1.4.4)
    Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (2.8.2)
    Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (23.1)
    Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (3.0.9)
    Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.2.4->d2l) (2022.7.1)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (2022.12.7)
    Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (2.10)
    Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (4.0.0)
    Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (1.26.15)
    Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.16.0)
    Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.2)
    Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.0)
    Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)
    Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)
    Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.7.1)
    Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.4)
    Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.7)
    Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.14.0)
    Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.38)
    Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (6.0.0)
    Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)
    Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.2)
    Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.2)
    Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)
    Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)
    Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.3)
    Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.3.0)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.2)
    Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.8.0)
    Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.2)
    Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)
    Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)
    Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)
    Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (21.3.0)
    Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.2.1)
    Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)
    Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.16.0)
    Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)
    Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.5.6)
    Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.3.1)
    Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)
    Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.18.2)
    Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (67.6.1)
    Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)
    Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.0)
    Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)
    Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.1.6)
    Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (3.2.0)
    Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (4.3.3)
    Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.16.3)
    Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.6)
    Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)
    Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)
    Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.4.1)
    Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)
    Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.3)
    Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.19.3)
    Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (23.1.0)
    Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.15.1)
    Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.21)



```python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```


```python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

    Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
    Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])
    AvgPool2d output shape: 	 torch.Size([1, 6, 14, 14])
    Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
    Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])
    AvgPool2d output shape: 	 torch.Size([1, 16, 5, 5])
    Flatten output shape: 	 torch.Size([1, 400])
    Linear output shape: 	 torch.Size([1, 120])
    Sigmoid output shape: 	 torch.Size([1, 120])
    Linear output shape: 	 torch.Size([1, 84])
    Sigmoid output shape: 	 torch.Size([1, 84])
    Linear output shape: 	 torch.Size([1, 10])



```python
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)
```

    /usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
      warnings.warn(_create_warning_msg(



```python
def evaluate_accuracy_gpu(net, data_iter, device=None): #@save
    """ä½¿ç”¨GPUè®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„ç²¾åº¦"""
    if isinstance(net, nn.Module):
        net.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        if not device:
            device = next(iter(net.parameters())).device
    # æ­£ç¡®é¢„æµ‹çš„æ•°é‡ï¼Œæ€»é¢„æµ‹çš„æ•°é‡
    metric = d2l.Accumulator(2)
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(X, list):
                # BERTå¾®è°ƒæ‰€éœ€çš„ï¼ˆä¹‹åå°†ä»‹ç»ï¼‰
                X = [x.to(device) for x in X]
            else:
                X = X.to(device)
            y = y.to(device)
            metric.add(d2l.accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```


```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼Œæ ·æœ¬æ•°
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```


```python
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.463, train acc 0.825, test acc 0.779
    41852.5 examples/sec on cuda:0




![svg](./ch06_122_1.svg)
    


&emsp;&emsp; ä»£ç éªŒè¯å¦‚ä¸‹ï¼š


```python
net_maxpool = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```


```python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net_maxpool:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

    Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
    Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])
    MaxPool2d output shape: 	 torch.Size([1, 6, 14, 14])
    Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
    Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])
    MaxPool2d output shape: 	 torch.Size([1, 16, 5, 5])
    Flatten output shape: 	 torch.Size([1, 400])
    Linear output shape: 	 torch.Size([1, 120])
    Sigmoid output shape: 	 torch.Size([1, 120])
    Linear output shape: 	 torch.Size([1, 84])
    Sigmoid output shape: 	 torch.Size([1, 84])
    Linear output shape: 	 torch.Size([1, 10])



```python
lr, num_epochs = 0.9, 10
train_ch6(net_maxpool, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.409, train acc 0.850, test acc 0.834
    43916.5 examples/sec on cuda:0




![svg](./ch06_126_1.svg)
    


### ç»ƒä¹  6.6.2

å°è¯•æ„å»ºä¸€ä¸ªåŸºäºLeNetçš„æ›´å¤æ‚çš„ç½‘ç»œï¼Œä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚
1. è°ƒæ•´å·ç§¯çª—å£å¤§å°ã€‚
1. è°ƒæ•´è¾“å‡ºé€šé“çš„æ•°é‡ã€‚
1. è°ƒæ•´æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰ã€‚
1. è°ƒæ•´å·ç§¯å±‚çš„æ•°é‡ã€‚
1. è°ƒæ•´å…¨è¿æ¥å±‚çš„æ•°é‡ã€‚
1. è°ƒæ•´å­¦ä¹ ç‡å’Œå…¶ä»–è®­ç»ƒç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œåˆå§‹åŒ–å’Œè½®æ•°ï¼‰ã€‚

**è§£ç­”ï¼š** 

**ç¬¬1é—®ï¼š**

&emsp;&emsp; `nn.Conv2d(1, 6, kernel_size = 7)`

**ç¬¬2é—®ï¼š**

&emsp;&emsp; `nn.Conv2d(1, 10, kernel_size = 5)`

**ç¬¬3é—®ï¼š**

&emsp;&emsp; `nn.Conv2d(1, 6, kernel_size = 5).ReLU()`æˆ–è€…ç›´æ¥å°†`nn.Sigmoid()`æ”¹ä¸º`nn.ReLU()`

**ç¬¬4é—®ï¼š**

&emsp;&emsp; æ·»åŠ `conv3`ä¸º`nn.Cov2d(16, 120, kernel_size = 5). ReLU()`

**ç¬¬5é—®ï¼š**

&emsp;&emsp; æ·»åŠ `nn.Linear(84, 20)`å¹¶å°†`nn.Linear(84, 10)`æ›¿æ¢ä¸º`nn.Linear(20, 10)`ï¼Œæ·»åŠ `nn.Sigmoid()`

**ç¬¬6é—®ï¼š**

&emsp;&emsp; `lr, num_epochs = 0.1, 50`

### ç»ƒä¹  6.6.3

åœ¨MNISTæ•°æ®é›†ä¸Šå°è¯•ä»¥ä¸Šæ”¹è¿›çš„ç½‘ç»œã€‚

**è§£ç­”ï¼š** 


```python
net_improve = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=3, padding=2), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 10, kernel_size=3), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(10, 16, kernel_size=3), nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16 * 2 * 2, 120), nn.ReLU(),
    nn.Linear(120, 84), nn.ReLU(),
    nn.Linear(84, 20), nn.ReLU(),
    nn.Linear(20, 10))
```


```python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net_improve:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
```

    Conv2d output shape: 	 torch.Size([1, 6, 30, 30])
    ReLU output shape: 	 torch.Size([1, 6, 30, 30])
    AvgPool2d output shape: 	 torch.Size([1, 6, 15, 15])
    Conv2d output shape: 	 torch.Size([1, 10, 13, 13])
    ReLU output shape: 	 torch.Size([1, 10, 13, 13])
    AvgPool2d output shape: 	 torch.Size([1, 10, 6, 6])
    Conv2d output shape: 	 torch.Size([1, 16, 4, 4])
    ReLU output shape: 	 torch.Size([1, 16, 4, 4])
    AvgPool2d output shape: 	 torch.Size([1, 16, 2, 2])
    Flatten output shape: 	 torch.Size([1, 64])
    Linear output shape: 	 torch.Size([1, 120])
    ReLU output shape: 	 torch.Size([1, 120])
    Linear output shape: 	 torch.Size([1, 84])
    ReLU output shape: 	 torch.Size([1, 84])
    Linear output shape: 	 torch.Size([1, 20])
    ReLU output shape: 	 torch.Size([1, 20])
    Linear output shape: 	 torch.Size([1, 10])



```python
lr, num_epochs = 0.1, 50
train_ch6(net_improve, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.275, train acc 0.897, test acc 0.825
    30035.9 examples/sec on cuda:0




![svg](./ch06_145_1.svg)
    


### ç»ƒä¹  6.6.4

æ˜¾ç¤ºä¸åŒè¾“å…¥ï¼ˆä¾‹å¦‚æ¯›è¡£å’Œå¤–å¥—ï¼‰æ—¶ï¼ŒLeNetç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„æ¿€æ´»å€¼ã€‚

**è§£ç­”ï¼š** 


```python
#@save
def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):
    """ç”¨GPUè®­ç»ƒæ¨¡å‹(åœ¨ç¬¬å…­ç« å®šä¹‰)"""
    def init_weights(m):
        if type(m) == nn.Linear or type(m) == nn.Conv2d:
            nn.init.xavier_uniform_(m.weight)
    net.apply(init_weights)
    print('training on', device)
    net.to(device)
    optimizer = torch.optim.SGD(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],
                            legend=['train loss', 'train acc', 'test acc'])
    timer, num_batches = d2l.Timer(), len(train_iter)
    for epoch in range(num_epochs):
        # è®­ç»ƒæŸå¤±ä¹‹å’Œï¼Œè®­ç»ƒå‡†ç¡®ç‡ä¹‹å’Œï¼Œæ ·æœ¬æ•°
        metric = d2l.Accumulator(3)
        net.train()
        for i, (X, y) in enumerate(train_iter):
            timer.start()
            optimizer.zero_grad()
            X, y = X.to(device), y.to(device)
            y_hat = net(X)
            l = loss(y_hat, y)
            l.backward()
            optimizer.step()
            with torch.no_grad():
                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])
            timer.stop()
            train_l = metric[0] / metric[2]
            train_acc = metric[1] / metric[2]
            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                animator.add(epoch + (i + 1) / num_batches,
                             (train_l, train_acc, None))
        test_acc = evaluate_accuracy_gpu(net, test_iter)
        animator.add(epoch + 1, (None, None, test_acc))
    x_first_Sigmoid_layer = net[0:2](X)[0:9, 1, :, :]
    d2l.show_images(x_first_Sigmoid_layer.reshape(9, 28, 28).cpu().detach(), 1, 9)
    x_second_Sigmoid_layer = net[0:5](X)[0:9, 1, :, :]
    d2l.show_images(x_second_Sigmoid_layer.reshape(9, 10, 10).cpu().detach(), 1, 9)
    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '
          f'test acc {test_acc:.3f}')
    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '
          f'on {str(device)}')
```


```python
lr, num_epochs = 0.9, 10
train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())
```

    loss 0.475, train acc 0.821, test acc 0.815
    44287.0 examples/sec on cuda:0




![svg](./ch06_149_1.svg)
    




![svg](./ch06_149_2.svg)
    




![svg](./ch06_149_3.svg)
    

