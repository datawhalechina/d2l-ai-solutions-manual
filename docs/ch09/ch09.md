# 第9章 现代循环神经网络

## 9.1 门控循环单元（GRU） 

### 练习 9.1.1

假设我们只想使用时间步$t'$的输入来预测时间步$t > t'$的输出。对于每个时间步，重置门和更新门的最佳值是什么？

**解答：**

&emsp;&emsp;如果只想采用时间步$t'$的输出来预测时间步$t$的输出，则意味着模型需要减少之前时间步在计算中做出的贡献，根据候选隐状态计算公式：  
$$\tilde{H}_t = tanh(X_tW_{xh}+(R_t \odot H_{t-1})W_{hh}+b_r)$$
因此需要令重置门的值趋向于0，减少过去状态的影响，令模型只考虑当前时间步$t'$所做出的贡献。  
&emsp;&emsp;在确定候选隐状态后，需要结合更新门进一步确定新的隐状态$H_t$受过去隐状态$H_{t-1}$和候选隐状态$\tilde{H}_t$的影响程度，而在只考虑时间步$t'$的输入时，需要抛弃过去隐状态$H_{t-1}$，来跳过过去时间步。因此根据最终更新公式：  
$$H_t = Z_t \odot H_{t-1} + (1-Z_t) \odot \tilde{H}_t$$
需要更新门的值也趋向于0，使得新的隐状态更接近于候选隐状态。  
&emsp;&emsp;因此，只想使用时间步$t'$的输入来预测时间步$t$的输出时，对于每个时间步，重置门和更新门的最佳值都应为0。

### 练习 9.1.2

调整和分析超参数对运行时间、困惑度和输出顺序的影响。

**解答：**

&emsp;&emsp;修改模型超参数，代码及对应结果如下：


```python
import torch
from torch import nn
from d2l import torch as d2l
```


```python
# [迭代周期，隐藏单元数，批量大小，小批量数据时间步数，学习率]
hyper_0 = [500, 512, 35, 32, 1]
hyper_1 = [250, 512, 35, 32, 1]
hyper_2 = [500, 256, 35, 32, 1]
hyper_3 = [500, 512, 10, 32, 1]
hyper_4 = [500, 512, 35, 10, 1]
hyper_5 = [500, 512, 35, 32, 0.01]
hyper_params = [hyper_0, hyper_1, hyper_2, hyper_3, hyper_4, hyper_5]
```


```python
def train_ch8(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    """训练模型（定义见第8章）"""
    loss = nn.CrossEntropyLoss()
    # animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
    #                         legend=['train'], xlim=[10, num_epochs])
    # 初始化
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
    predict = lambda prefix: d2l.predict_ch8(prefix, 50, net, vocab, device)
    # 训练和预测
    for epoch in range(num_epochs):
        ppl, speed = d2l.train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)
        # if (epoch + 1) % 10 == 0:
        #     animator.add(epoch + 1, [ppl])
    print(predict('time traveller'))
    return ppl
```


```python
perplexity = []
times = []
for i in range(len(hyper_params)):
    num_epochs, num_hiddens, batch_size, num_steps, lr = hyper_params[i]
    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
    gru_layer = nn.GRU(len(vocab), num_hiddens)
    model = d2l.RNNModel(gru_layer, len(vocab))
    model = model.to(d2l.try_gpu())
    timer = d2l.Timer()
    timer.start()
    value = round(train_ch8(model, train_iter, vocab, lr, num_epochs, d2l.try_gpu()), 1)
    time = timer.stop()
    times.append(time)
    perplexity.append(value)
    print('hyper[%d]: Timer:%.2f, perplexity:%.1f'%(i, times[i], perplexity[i]))
```

    time traveller came back andfilby s anecdote collapsedthe thing 
    hyper[0]: Timer:20.63, perplexity:1.0
    time traveller smiled along the time dimension of so along there
    hyper[1]: Timer:10.33, perplexity:2.3
    time traveller came back andfilby s anecdote collapsedthe thing 
    hyper[2]: Timer:12.56, perplexity:1.1
    time traveller smiled round at us then still smiling faintlyand 
    hyper[3]: Timer:57.04, perplexity:1.0
    time traveller came back andfilby s anecdote collapsedthe thing 
    hyper[4]: Timer:35.33, perplexity:1.0
    time traveller   t  t  t  t  t  t  t  t  t  t  t  t  t  t  t  t 
    hyper[5]: Timer:22.51, perplexity:16.7
    

&emsp;&emsp;对比hyper[0]中的默认超参数配置，我们可以发现
- hyper[1]中缩小迭代周期，导致模型无法完全收敛，导致困惑度增大；
- hyper[2]中减少了隐藏层单元数，使得模型在很短时间内完成收敛，并且因为数据集较为简单的原因，困惑度也降低为1;
- hyper[3]中减小了批量大小，这使得模型的训练时间开销显著增加;
- hyper[4]中将小批量时间步的跨度缩小，虽然也造成了时间开销的增加，但少于批量减少带来的时间增长;
- hyper[5]中则对学习率进行了修改，可以发现过低的学习率会使得模型收敛的非常慢，造成困惑度暴涨。
&emsp;&emsp;综上可以发现，迭代周期和学习率的缩小会导致模型无法在对应周期内完成收敛，导致模型训练不完全，困惑度显著增加。批量大小即时间步跨度的降低则会导致模型每次加载少量数据，变相造成时间开销的增大；而对于简单的数据，采用一个简单的模型既可以降低时间开销，也可以使得模型很好收敛，确保性能。

### 练习 9.1.3

比较`rnn.RNN`和`rnn.GRU`的不同实现对运行时间、困惑度和输出字符串的影响。

**解答：**


```python
batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
timer = d2l.Timer()
```


```python
# RNN
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)

device = d2l.try_gpu()
net = d2l.RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)

num_epochs, lr = 500, 1
timer.start()
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)
time_rnn = timer.stop()
print('time_rnn:%.2f'%time_rnn)
```

    perplexity 1.3, 397371.0 tokens/sec on cuda:0
    time travellerit loug the wole coume snor if in if in in i move 
    traveller of grecivension our cintsice ard here arenreats a
    time_rnn:16.20
    


    
![svg](output_16_1.svg)
    



```python
num_inputs = len(vocab)
gru_layer = nn.GRU(num_inputs, num_hiddens)
model = d2l.RNNModel(gru_layer, len(vocab))
model = model.to(device)
timer.start()
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
time_gru = timer.stop()
print('time_gru:%.2f'%time_gru)
```

    perplexity 1.0, 358084.5 tokens/sec on cuda:0
    time traveller for so it will be convenient to speak of himwas e
    travelleryou can show black is white by argument said filby
    time_gru:17.35
    


    
![svg](output_17_1.svg)
    


&emsp;&emsp;相较于RNN，GRU在重置门和更新门的计算，以及计算候选隐状态和确定最终隐状态的过程中增加了大量矩阵运算，这些新增的矩阵运算会使得模型的训练时间出现增加。但是得益于GRU中重置门与更新门的结构优势，GRU生成结果的困惑度仅为1.0，低于RNN中的1.3困惑度，输出字符串也更好。

### 练习 9.1.4

如果仅仅实现门控循环单元的一部分，例如，只有一个重置门或一个更新门会怎样？

**练习：**


```python
# 仅重置门
def get_params_R(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    # W_xz, W_hz, b_z = three()  # 更新门参数
    W_xr, W_hr, b_r = three()  # 重置门参数
    W_xh, W_hh, b_h = three()  # 候选隐状态参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    # params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    params = [W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```


```python
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )

def gru(inputs, state, params):
    W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        # Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)
        H = H_tilda
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```


```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params_R,
                            init_gru_state, gru)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

    perplexity 1.0, 82887.9 tokens/sec on cuda:0
    time travelleryou can show black is white by argument said filby
    traveller with a slight accession ofcheerfulness really thi
    


    
![svg](output_23_1.svg)
    



```python
def get_params_Z(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device)*0.01

    def three():
        return (normal((num_inputs, num_hiddens)),
                normal((num_hiddens, num_hiddens)),
                torch.zeros(num_hiddens, device=device))

    W_xz, W_hz, b_z = three()  # 更新门参数
    # W_xr, W_hr, b_r = three()  # 重置门参数
    W_xh, W_hh, b_h = three()  # 候选隐状态参数
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    # params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]
    params = [W_xz, W_hz, b_z, W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```


```python
def init_gru_state(batch_size, num_hiddens, device):
    return (torch.zeros((batch_size, num_hiddens), device=device), )

def gru(inputs, state, params):
    W_xz, W_hz, b_z, W_xh, W_hh, b_h, W_hq, b_q = params
    H, = state
    outputs = []
    for X in inputs:
        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)
        #R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)
        H_tilda = torch.tanh((X @ W_xh) + (H @ W_hh) + b_h)
        H = Z * H + (1 - Z) * H_tilda
        Y = H @ W_hq + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
```


```python
vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()
num_epochs, lr = 500, 1
model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params_Z,
                            init_gru_state, gru)
d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

    perplexity 1.1, 73438.9 tokens/sec on cuda:0
    time traveller for so it will be convenient to speak of himwas e
    traveller with a slight accession ofcheerfulness really thi
    


    
![svg](output_26_1.svg)
    


&emsp;&emsp;门控循环单元（GRU）是一种递归神经网络，具有两个门控制状态的更新和重置，这些门可以控制输入的流入和隐藏状态的保留程度。如果仅实现其中一个门，则将无法实现完整的GRU模型。  
&emsp;&emsp;如果只实现了一个重置门，那么模型将只能重置隐藏状态，而无法更新其状态。这会导致模型无法学习长期依赖性并且可能无法产生准确的输出。而如果只实现了一个更新门，则模型可以更新其隐藏状态，但是无法选择要丢弃的信息，这也可能会导致模型无法正确地处理长期依赖性。  
&emsp;&emsp;因此，为了实现GRU模型，需要同时实现更新门和重置门。这样，模型就能够更新其隐藏状态并选择要保留和丢弃的信息，从而使其能够正确地处理长期依赖性。

## 9.2 长短期记忆网络（LSTM） 

### 练习 9.2.1

调整和分析超参数对运行时间、困惑度和输出顺序的影响。

**解答：**


```python
# [迭代周期，隐藏单元数，批量大小，小批量数据时间步数，学习率]
hyper_0 = [500, 512, 35, 32, 1]
hyper_1 = [250, 512, 35, 32, 1]
hyper_2 = [500, 256, 35, 32, 1]
hyper_3 = [500, 512, 10, 32, 1]
hyper_4 = [500, 512, 35, 10, 1]
hyper_5 = [500, 512, 35, 32, 0.01]
hyper_params = [hyper_0, hyper_1, hyper_2, hyper_3, hyper_4, hyper_5]
```


```python
perplexity = []
times = []
for i in range(len(hyper_params)):
    num_epochs, num_hiddens, batch_size, num_steps, lr = hyper_params[i]
    train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
    #
    lstm_layer = nn.LSTM(len(vocab), num_hiddens)
    model = d2l.RNNModel(lstm_layer, len(vocab))
    model = model.to(d2l.try_gpu())
    timer = d2l.Timer()
    timer.start()
    value = round(train_ch8(model, train_iter, vocab, lr, num_epochs, d2l.try_gpu()), 1)
    time = timer.stop()
    times.append(time)
    perplexity.append(value)
    print('hyper[%d]: Timer:%.2f, perplexity:%.1f'%(i, times[i], perplexity[i]))
```

    time traveller proceeded anyreal body must have extension in fou
    hyper[0]: Timer:26.65, perplexity:1.0
    time traveller smiled and herent and therens of the trought to e
    hyper[1]: Timer:13.33, perplexity:3.2
    time traveller came back andfilby s anecdote collapsedthe thing 
    hyper[2]: Timer:14.68, perplexity:1.1
    time traveller smiled round at us then still smiling faintlyand 
    hyper[3]: Timer:75.33, perplexity:1.0
    time traveller came back andfilby s anecdote collapsedthe thing 
    hyper[4]: Timer:44.26, perplexity:1.0
    time traveller                                                  
    hyper[5]: Timer:27.23, perplexity:17.4
    

&emsp;&emsp;结果与GRU结果类似，对比hyper[0]中的默认超参数配置，我们可以发现
- hyper[1]中缩小迭代周期虽然减少了训练时间，但是导致模型无法完全收敛，导致困惑度增大；
- hyper[2]中减半了隐藏层单元数，使得模型在很短时间内完成收敛，困惑度也降低为1.1;
- hyper[3]中减小了批量大小，虽然困惑度为1，但是模型的训练时间开销显著增加;
- hyper[4]中将小批量时间步的跨度缩小，虽然也造成了时间开销的增加，但少于批量减少带来的时间增长;
- hyper[5]中则对学习率进行了修改，过低的学习率会使得模型训练无法在有限周期内完成训练，造成困惑度暴涨。

&emsp;&emsp;综上可以发现，迭代周期和学习率的缩小会导致模型无法在对应周期内完成收敛，导致模型训练不完全，困惑度显著增加。批量大小即时间步跨度的降低则会导致模型每次加载少量数据，变相造成时间开销的增大；而对于简单的数据，采用一个简单的模型既可以降低时间开销，也可以使得模型很好收敛，确保性能。

### 练习 9.2.2

如何更改模型以生成适当的单词，而不是字符序列？

**解答：**

&emsp;&emsp;将文本的生成方式由字符序列改为单词，具体可采用以下方式：


```python
import torch
from torch import nn
from d2l import torch as d2l
```

1. 词元化处理时采用单词级别的词元化处理，使得模型学会单词间的关系而非字符间的关系。


```python
def load_corpus_time_machine(max_tokens=-1):  #@save
   """返回时光机器数据集的词元索引列表和词表"""
   lines = d2l.read_time_machine()
   tokens = d2l.tokenize(lines, 'word')
   vocab = d2l.Vocab(tokens)
   # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，
   # 所以将所有文本行展平到一个列表中
   corpus = [vocab[token] for line in tokens for token in line]
   if max_tokens > 0:
       corpus = corpus[:max_tokens]
   return corpus, vocab

corpus, vocab = load_corpus_time_machine()
len(corpus), len(vocab)
```




    (32775, 4580)



2. 使用双向语言模型，将前后文的单词序列纳入考虑范围，实现根据上下文预测单词，获得更准确的输出。  
&emsp;&emsp;`lstm_layer = nn.LSTM(num_inputs, num_hiddens, bidirectional=True)`

3. 使用WordEmbedding取代One-hot，将单词映射到向量空间，保留了近义词之间的相关性，可以学习单词间的语义关系，更准确的生成单词。


```python
class RNNModel_Embedding(nn.Module):
    """The RNN model.
    Defined in :numref:`sec_rnn-concise`"""
    def __init__(self, rnn_layer, vocab_size, embedding_dim, **kwargs):
        super(RNNModel_Embedding, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # If the RNN is bidirectional (to be introduced later),
        # `num_directions` should be 2, else it should be 1.
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        # X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = self.embedding(inputs.T.long())
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # The fully connected layer will first change the shape of `Y` to
        # (`num_steps` * `batch_size`, `num_hiddens`). Its output shape is
        # (`num_steps` * `batch_size`, `vocab_size`).
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # `nn.GRU` takes a tensor as hidden state
            return  torch.zeros((self.num_directions * self.rnn.num_layers,
                                 batch_size, self.num_hiddens),
                                device=device)
        else:
            # `nn.LSTM` takes a tuple of hidden states
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
```


```python
class SeqDataLoader:
    """An iterator to load sequence data."""
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        """Defined in :numref:`sec_language_model`"""
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps,
                           use_random_iter=False, max_tokens=10000):
    """Return the iterator and the vocabulary of the time machine dataset.

    Defined in :numref:`sec_language_model`"""
    data_iter = SeqDataLoader(
        batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```


```python
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = load_data_time_machine(batch_size, num_steps)

num_inputs, num_hiddens = len(vocab), 512
embedding_dim = len(vocab)
num_layers = 2
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
model = RNNModel_Embedding(lstm_layer, len(vocab), embedding_dim=embedding_dim)
model = model.to(device)
```


```python
def predict_word(prefix, num_preds, net, vocab, device):
    """Generate new words following the `prefix`.
    Defined in :numref:`sec_rnn_scratch`"""
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: d2l.reshape(d2l.tensor(
        [outputs[-1]], device=device), (1, 1))
    for y in prefix[1:]:  # Warm-up period
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds):  # Predict `num_preds` steps
        y, state = net(get_input(), state)
        word_idx = int(y.argmax(dim=1).reshape(1))
        word = vocab.idx_to_token[word_idx]
        if word == '<unk>':  # 跳过未知单词
            continue
        outputs.append(word_idx)
        if word.endswith(' '):  # 如果预测的单词以空格结尾，则停止
            break
    return ''.join([vocab.idx_to_token[i] for i in outputs])
```


```python
def train_ch8(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    """Train a model (defined in Chapter 8).

    Defined in :numref:`sec_rnn_scratch`"""
    loss = nn.CrossEntropyLoss()
    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',
                            legend=['train'], xlim=[10, num_epochs])
    # Initialize
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)
    predict = lambda prefix: predict_word(prefix, 50, net, vocab, device)
    # Train and predict
    for epoch in range(num_epochs):
        ppl, speed = d2l.train_epoch_ch8(
            net, train_iter, loss, updater, device, use_random_iter)
        if (epoch + 1) % 10 == 0:
            print(predict('time traveller'))
            animator.add(epoch + 1, [ppl])
    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')
    print(predict('time traveller'))
    print(predict('traveller'))
```


```python
num_epochs, lr = 500, 1
train_ch8(model, train_iter, vocab, lr, num_epochs, device)
```

    perplexity 1.0, 14706.3 tokens/sec on cuda:0
    tim<unk><unk>t<unk>av<unk><unk><unk><unk><unk>andafainterfaintertomorrowthelaboratoryseemedseemedtofalltofallawayfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfromfrom
    t<unk>av<unk><unk><unk><unk><unk>andthentheothersoftsoftsoftsofthairlessandyetandyetasitasitasifitifitifitifitifitifitifitifitifitifitifitifitifitifitifitifit
    


    
![svg](output_47_1.svg)
    


### 练习 9.2.3

在给定隐藏层维度的情况下，比较门控循环单元、长短期记忆网络和常规循环神经网络的计算成本。要特别注意训练和推断成本。

**解答：**

&emsp;&emsp;将隐藏层维度统一设置为256


```python
# [迭代周期，隐藏单元数，批量大小，小批量数据时间步数，学习率]
num_epochs, num_hiddens, batch_size, num_steps, lr = [500, 256, 35, 32, 1]
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```


```python
# RNN
rnn_layer = nn.RNN(len(vocab), num_hiddens)
rnn = d2l.RNNModel(rnn_layer, len(vocab))
rnn = rnn.to(d2l.try_gpu())

timer = d2l.Timer()
timer.start()
d2l.train_ch8(rnn, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
time = timer.stop()
print('Timer: %.2f'%time)
```

    perplexity 1.3, 389218.2 tokens/sec on cuda:0
    time traveller smiled round fore is that is just where hrong the
    traveller aboek the permeo saing tras ixins of thitence tim
    Timer: 16.08
    


    
![svg](output_52_1.svg)
    



```python
# GRU
gru_layer = nn.GRU(len(vocab), num_hiddens)
gru = d2l.RNNModel(gru_layer, len(vocab))
gru = gru.to(d2l.try_gpu())

timer = d2l.Timer()
timer.start()
d2l.train_ch8(gru, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
time = timer.stop()
print('Timer: %.2f'%time)
```

    perplexity 1.0, 358070.8 tokens/sec on cuda:0
    time traveller came back andfilby s anecdote collapsedthe thing 
    traveller came back andfilby s anecdote collapsedthe thing 
    Timer: 16.96
    


    
![svg](output_53_1.svg)
    



```python
# LSTM
lstm_layer = nn.LSTM(len(vocab), num_hiddens)
lstm = d2l.RNNModel(lstm_layer, len(vocab))
lstm = lstm.to(d2l.try_gpu())

timer = d2l.Timer()
timer.start()
d2l.train_ch8(lstm, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
time = timer.stop()
print('Timer: %.2f'%time)
```

    perplexity 1.1, 331550.9 tokens/sec on cuda:0
    time traveller proceeded anyreal body must have extension in fou
    traveller came back andfilby s anecdote collapsedthe thing 
    Timer: 18.11
    


    
![svg](output_54_1.svg)
    


&emsp;&emsp;可以看到结构最简单的RNN所需计算成本最低，运行时间最短。GRU在RNN基础上增加了更新门和重置门两个门控单元，结构复杂，参数量也有所增加，所需的计算成本要大于RNN，运行时间也更长，但困惑度有随之降低，模型准确度更高。而LSTM内含输入门、输出门以及遗忘门3个门，结构进一步复杂化，计算成本也是三个模型中最高的。

### 练习 9.2.4  

既然候选记忆元通过使用$\tanh$函数来确保值范围在$(-1,1)$之间，那么为什么隐状态需要再次使用$\tanh$函数来确保输出值范围在$(-1,1)$之间呢？

**解答：**

&emsp;&emsp;在整个LSTM计算中，候选记忆元$\tilde{C}_t$的计算公式如下：
$$\tilde{C}_t=tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c)$$
&emsp;&emsp;此时$\tanh$为激活函数，确保$\tilde{C}_t$的值域在$(-1,1)$之间。
&emsp;&emsp;而在计算记忆元$C_t$时，存在输入门和遗忘门对记忆元的值进行影响，具体计算公式如下：
$$C_t=F_t \odot C_{t-1}+I_t \odot \tilde{C}_t$$
&emsp;&emsp;上述公式中，输入门和遗忘门的值域为$(0,1)$，因此当$F_t,I_t$的值均为$1$，且$C_{t-1},\tilde{C}_t$的值也均为$1$时，整个记忆元的最终输出结果为$2$。此时对于整个隐状态而言，由于输出门的值域也为$(0,1)$，因此隐状态值很有可能超过$1$，因此需要对记忆元通过$\tanh$函数来进行约束取值范围，以此确保输出值范围在$(-1,1)$之间。

### 练习 9.2.5

实现一个能够基于时间序列进行预测而不是基于字符序列进行预测的长短期记忆网络模型。


**解答：**

&emsp;&emsp;这里使用[8.1.序列模型](https://zh.d2l.ai/chapter_recurrent-neural-networks/sequence.html#id11)中的序列数据


```python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

T = 1000  # 总共产生1000个点
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))
```


    
![svg](output_62_0.svg)
    



```python
tau = 4
features = torch.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = x[tau:].reshape((-1, 1))

batch_size, n_train = 16, 600
# 只有前n_train个样本用于训练
train_iter = d2l.load_array((features[:n_train], labels[:n_train]),
                            batch_size, is_train=True)
```


```python
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size)
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, input):
        lstm_out, _ = self.lstm(input.view(len(input), 1, -1))
        output = self.linear(lstm_out.view(len(input), -1))
        return output
```


```python
input_size = 4
hidden_size = 200
output_size = 1
learning_rate = 0.01
epochs = 50
```


```python
model = LSTM(input_size, hidden_size, output_size)
criterion = nn.MSELoss()
print(model)
```

    LSTM(
      (lstm): LSTM(4, 200)
      (linear): Linear(in_features=200, out_features=1, bias=True)
    )
    


```python
def train(model, train_iter, loss, epochs, learning_rate):
    trainer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    for epoch in range(epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            output = model(X)
            loss = criterion(output, y)
            loss.backward()
            trainer.step()
        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))
```


```python
train(model, train_iter, criterion, 50, 0.01)
```

    Epoch [1/50], Loss: 0.0619
    Epoch [2/50], Loss: 0.0811
    Epoch [3/50], Loss: 0.0292
    Epoch [4/50], Loss: 0.0745
    Epoch [5/50], Loss: 0.0158
    Epoch [6/50], Loss: 0.0509
    Epoch [7/50], Loss: 0.0541
    Epoch [8/50], Loss: 0.0551
    Epoch [9/50], Loss: 0.0285
    Epoch [10/50], Loss: 0.0694
    Epoch [11/50], Loss: 0.0379
    Epoch [12/50], Loss: 0.0965
    Epoch [13/50], Loss: 0.0415
    Epoch [14/50], Loss: 0.0442
    Epoch [15/50], Loss: 0.0511
    Epoch [16/50], Loss: 0.0339
    Epoch [17/50], Loss: 0.0150
    Epoch [18/50], Loss: 0.0351
    Epoch [19/50], Loss: 0.0383
    Epoch [20/50], Loss: 0.0879
    Epoch [21/50], Loss: 0.0119
    Epoch [22/50], Loss: 0.0794
    Epoch [23/50], Loss: 0.0614
    Epoch [24/50], Loss: 0.0757
    Epoch [25/50], Loss: 0.0614
    Epoch [26/50], Loss: 0.0382
    Epoch [27/50], Loss: 0.0663
    Epoch [28/50], Loss: 0.0411
    Epoch [29/50], Loss: 0.0671
    Epoch [30/50], Loss: 0.0249
    Epoch [31/50], Loss: 0.0390
    Epoch [32/50], Loss: 0.0181
    Epoch [33/50], Loss: 0.0387
    Epoch [34/50], Loss: 0.0564
    Epoch [35/50], Loss: 0.0607
    Epoch [36/50], Loss: 0.1232
    Epoch [37/50], Loss: 0.0568
    Epoch [38/50], Loss: 0.0555
    Epoch [39/50], Loss: 0.0244
    Epoch [40/50], Loss: 0.0494
    Epoch [41/50], Loss: 0.0449
    Epoch [42/50], Loss: 0.0307
    Epoch [43/50], Loss: 0.0902
    Epoch [44/50], Loss: 0.0928
    Epoch [45/50], Loss: 0.0574
    Epoch [46/50], Loss: 0.0590
    Epoch [47/50], Loss: 0.0259
    Epoch [48/50], Loss: 0.0387
    Epoch [49/50], Loss: 0.0773
    Epoch [50/50], Loss: 0.0299
    


```python
onestep_preds = model(features)
d2l.plot([time, time[tau:]],
         [x.detach().numpy(), onestep_preds.detach().numpy()], 'time',
         'x', legend=['data', '1-step preds'], xlim=[1, 1000],
         figsize=(6, 3))
```


    
![svg](output_69_0.svg)
    


## 9.3 深度循环神经网络 

### 练习 9.3.1 

基于我们在8.5节中讨论的单层实现，尝试从零开始实现两层循环神经网络。

**解答：**


```python
# 定义训练数据
import math
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```


```python
# 初始化模型参数
def get_params(vocab_size, num_hiddens, device):
    num_inputs = num_outputs = vocab_size

    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01

    # 第一层RNN参数
    W_xh1 = normal((num_inputs, num_hiddens))
    W_hh1 = normal((num_hiddens, num_hiddens))
    b_h1 = torch.zeros(num_hiddens, device=device)
    # 第二层RNN参数
    W_hh2 = normal((num_hiddens, num_hiddens))
    b_h2 = torch.zeros(num_hiddens, device=device)
    # 输出层参数
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xh1, W_hh1, b_h1, W_hh2, b_h2, W_hq, b_q]
    for param in params:
        param.requires_grad_(True)
    return params
```


```python
def init_rnn_state(batch_size, num_hiddens, device):
    # 返回初始化的双层RNN状态
    return (torch.zeros((batch_size, num_hiddens), device=device),
            torch.zeros((batch_size, num_hiddens), device=device))
```

&emsp;&emsp;定义一个双层RNN网络


```python
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量大小，词表大小)
    W_xh1, W_hh1, b_h1, W_hh2, b_h2, W_hq, b_q = params
    H1, H2 = state
    outputs = []
    # X的形状：(批量大小，词表大小)
    for X in inputs:
        H1 = torch.relu(torch.mm(X, W_xh1) + torch.mm(H1, W_hh1) + b_h1)
        H2 = torch.relu(torch.mm(H1, W_hh2) + b_h2)
        Y = torch.mm(H2, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H1, H2)
```


```python
num_hiddens = 512
net = d2l.RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,
                      init_rnn_state, rnn)
```


```python
num_epochs, lr = 500, 1
d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())
```

    perplexity 1.0, 96257.3 tokens/sec on cuda:0
    time traveller for so it will be convenient to speak of himwas e
    traveller with a slight accession ofcheerfulness really thi
    


    
![svg](output_79_1.svg)
    


### 练习 9.3.2

在本节训练模型中，比较使用门控循环单元替换长短期记忆网络后模型的精确度和训练速度。

**解答：**


```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size, num_steps = 32, 35
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)
```


```python
vocab_size, num_hiddens, num_layers = len(vocab), 256, 2
num_inputs = vocab_size
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
```


```python
num_epochs, lr = 500, 2
timer = d2l.Timer()
timer.start()
d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)
time = timer.stop()
print('LSTM_Timer: %.2f'%time)
```

    perplexity 1.0, 165471.8 tokens/sec on cuda:0
    time traveller for so it will be convenient to speak of himwas e
    traveller with a slight accession ofcheerfulness really thi
    LSTM_Timer: 31.35
    


    
![svg](output_84_1.svg)
    



```python
gru_layer = nn.GRU(num_inputs, num_hiddens, num_layers)
model_gru = d2l.RNNModel(gru_layer, len(vocab))
model_gru = model_gru.to(device)
```


```python
num_epochs, lr = 500, 2
timer = d2l.Timer()
timer.start()
d2l.train_ch8(model_gru, train_iter, vocab, lr*1.0, num_epochs, device)
time = timer.stop()
print('GRU_Timer: %.2f'%time)
```

    perplexity 1.0, 186493.1 tokens/sec on cuda:0
    time travelleryou can show black is white by argument said filby
    travelleryou can show black is white by argument said filby
    GRU_Timer: 29.00
    


    
![svg](output_86_1.svg)
    


&emsp;&emsp;对比双层GRU与双层LSTM的结果可以发现，将LSTM替换为GRU后模型的训练速度有所提高，因为相较于LSTM，GRU只有两个门，具有更少的参数量，因此训练速度更快。精确度方面，由于文本量限制，二者都在该文本上达到了1.0的困惑度，并未出现明显差别。

### 练习 9.3.3

如果增加训练数据，能够将困惑度降到多低？

**解答：**

&emsp;&emsp;这里将该作者的另一篇文章《The War of the Worlds》与《The Time Machine》进行混合，将增加后的文本作为训练数据。


```python
import torch
from torch import nn
from d2l import torch as d2l
import re
```


```python
def read_Mix():
    with open('../../notebooks/data/Mix_data.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]

def load_corpus_world(max_tokens=-1):
    lines = read_Mix()
    tokens = d2l.tokenize(lines, 'char')
    vocab = d2l.Vocab(tokens)

    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab
```


```python
class SeqDataLoader:
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = d2l.seq_data_iter_random
        else:
            self.data_iter_fn = d2l.seq_data_iter_sequential
        self.corpus, self.vocab = load_corpus_world(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps

    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_mix_data(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
```


```python
batch_size, num_steps = 32, 35
train_iter, vocab = load_mix_data(batch_size, num_steps)

num_inputs, num_hiddens, num_layers = len(vocab), 256, 2
device = d2l.try_gpu()
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
```


```python
num_epochs, lr = 500, 2
d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)
```

    perplexity 1.0, 168904.0 tokens/sec on cuda:0
    time traveller for so it will be convenient to speak of himwas e
    travelleryou can show black is white by argument said filby
    


    
![svg](output_95_1.svg)
    


&emsp;&emsp;可以看到，增加数据后，困惑度仍可以降低到1

### 练习 9.3.4

在为文本建模时，是否可以将不同作者的源数据合并？有何优劣呢？

**解答：**

&emsp;&emsp;在为文本建模时，将不同作者的源数据合并是可以的，但需要考虑一些因素。  
- 优点：
    - 增加训练数据量：将不同作者的源数据合并，可以增加训练数据量，从而更好地训练模型。  
    - 提高模型泛化能力：不同作者的风格、语言等可能不同，将这些数据合并可以让模型更好地学习到不同的文本特征，提高模型的泛化能力。  

- 缺点：
    - 可能影响模型训练效果：合并不同作者的数据，可能会引入更多的噪音，使得模型更难学习到有用的信息，从而影响训练效果。  
    - 可能会引入偏见：如果不同作者的数据不平衡，比如某一位作者的数据量比其他作者多很多，那么训练出来的模型可能会对该作者的文本表现更好，从而引入偏见。

&emsp;&emsp;综上所述，将不同作者的源数据合并在一起是可以的，但需要在合并前考虑好数据的平衡性和质量，并对数据进行适当的预处理，以确保训练出来的模型具有较好的泛化能力和稳定性。

## 9.4 双向循环神经网络 

### 练习 9.4.1

如果不同方向使用不同数量的隐藏单位，$\mathbf{H_t}$的形状会发生怎样的变化？

**解答：**

&emsp;&emsp;在双向循环神经网络中，最终隐藏状态$H_t$的维度由前向循环神经网络和反向循环神经网络中的隐藏层进行Concat得来，而通常前向和反向循环神经网络均选用相同的隐藏层数量，即$\overrightarrow{H_t}=\overleftarrow{H_t}$，因此直接进行翻倍，即可获得最终形状$H_t=2 * \overrightarrow{H_t} = 2 * \overleftarrow{H_t}$。  
&emsp;&emsp;而当不同方向使用不同数量的隐藏单位时，此时前向和反向的隐藏单元数量不同，$H_t$的维度并不等于前向或反向循环神经网络中隐藏层数量的$2$倍，此时需要将前向和反向的隐藏层维度拼接相加，获得最终$H_t$的维度，即$H_t = \overrightarrow{H_t} + \overleftarrow{H_t}$


```python
# 定义一个隐藏单元数量不同的BiLSTM
import torch
import torch.nn as nn
import torch.nn.functional as F

class BiLSTM(nn.Module):
    def __init__(self, input_size, hidden_size_forward, hidden_size_backward, num_layers=1):
        super(BiLSTM, self).__init__()
        # 开启batch_size=True后，输出的形状由[seq_length, batch_size, feature_dim] -> [batch_size, seq_length, feature_dim]
        self.lstm_forward = nn.LSTM(input_size, hidden_size_forward, num_layers=num_layers, batch_first=True)
        self.lstm_backward = nn.LSTM(input_size, hidden_size_backward, num_layers=num_layers, batch_first=True)

    def forward(self, x, x_lengths):
        # 前向部分
        # pack_padded_sequence() 负责将可变长度序列进行打包
        # pad_packed_sequence() 负责将填充的序列解包，以便在后续计算时去掉无用的填充部分
        packed_x_forward = nn.utils.rnn.pack_padded_sequence(x, x_lengths, batch_first=True, enforce_sorted=False)
        packed_h_forward, _ = self.lstm_forward(packed_x_forward)
        h_forward, _ = nn.utils.rnn.pad_packed_sequence(packed_h_forward, batch_first=True)

        # 反向部分
        packed_x_backward = nn.utils.rnn.pack_padded_sequence(torch.flip(x, [1]), x_lengths, batch_first=True, enforce_sorted=False)
        packed_h_backward, _ = self.lstm_backward(packed_x_backward)
        h_backward, _ = nn.utils.rnn.pad_packed_sequence(packed_h_backward, batch_first=True)
        h_backward = torch.flip(h_backward, [1])

        # Concat
        h = torch.cat((h_forward, h_backward), dim=2)

        return h
```


```python
batch_size = 3
seq_length = 5
input_size = 2
hidden_size_forward = 10
hidden_size_backward = 8

# 构造输入序列
x_lengths = [3, 5, 2]
x = [torch.randn(x_lengths[i], input_size) for i in range(batch_size)]
x = nn.utils.rnn.pad_sequence(x, batch_first=True)

model = BiLSTM(input_size, hidden_size_forward, hidden_size_backward)
print(model)
h = model(x, x_lengths)
print(h.shape)
```

    BiLSTM(
      (lstm_forward): LSTM(2, 10, batch_first=True)
      (lstm_backward): LSTM(2, 8, batch_first=True)
    )
    torch.Size([3, 5, 18])
    

### 练习 9.4.2

设计一个具有多个隐藏层的双向循环神经网络。

**解答：**


```python
import torch
from torch import nn
from d2l import torch as d2l

# 加载数据
batch_size, num_steps, device = 32, 35, d2l.try_gpu()
train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)

vocab_size, num_hiddens, num_layers = len(vocab), 256, 4
num_inputs = vocab_size
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
model = d2l.RNNModel(lstm_layer, len(vocab))
model = model.to(device)
print(model)
```

    RNNModel(
      (rnn): LSTM(28, 256, num_layers=4, bidirectional=True)
      (linear): Linear(in_features=512, out_features=28, bias=True)
    )
    

&emsp;&emsp;在上述模型中，隐藏层的数量被设置为4，实现了一个四层的双向循环神经网络

### 练习 9.4.3

在自然语言中一词多义很常见。例如，“bank”一词在不同的上下文“i went to the bank to deposit cash”和“i went to the bank to sit down”中有不同的含义。如何设计一个神经网络模型，使其在给定上下文序列和单词的情况下，返回该单词在此上下文中的向量表示？哪种类型的神经网络架构更适合处理一词多义？

**解答：**

&emsp;&emsp;处理一词多义的常见方法是结合上下文信息来对该位置的单词意思进行确认，例如在神经网络中，可以采用双向循环神经网络模型或者Transformer，将上下文中的单词序列映射到单词的向量表示。这些模型可以学到上下文中单词的复杂交互，以此实现根据上下文内容来动态生成单词的对应向量表示。  
&emsp;&emsp;这些上下文感知的双向模型能够更好地捕捉单词在不同上下文中的含义。例如，在双向LSTM中，每个单词的向量表示取决于整个上下文序列，因此可以更好地捕捉单词在上下文中的含义。

## 9.5 机器翻译与数据集 

### 练习 9.5.1

在`load_data_nmt`函数中尝试不同的`num_examples`参数值。这对源语言和目标语言的词表大小有何影响？

**解答：**


```python
import os
#@save
d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',
                           '94646ad1522d915e7b0f9296181140edcf86a4f5')

#@save
def read_data_nmt():
    """载入“英语－法语”数据集"""
    data_dir = d2l.download_extract('fra-eng')
    with open(os.path.join(data_dir, 'fra.txt'), 'r',
             encoding='utf-8') as f:
        return f.read()

#@save
def load_data_nmt(batch_size, num_steps, num_examples):
    """返回翻译数据集的迭代器和词表"""
    text = d2l.preprocess_nmt(read_data_nmt())
    source, target = d2l.tokenize_nmt(text, num_examples)
    src_vocab = d2l.Vocab(source, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    tgt_vocab = d2l.Vocab(target, min_freq=2,
                          reserved_tokens=['<pad>', '<bos>', '<eos>'])
    src_array, src_valid_len = d2l.build_array_nmt(source, src_vocab, num_steps)
    tgt_array, tgt_valid_len = d2l.build_array_nmt(target, tgt_vocab, num_steps)
    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)
    data_iter = d2l.load_array(data_arrays, batch_size)
    return data_iter, src_vocab, tgt_vocab
```


```python
num_examples = [100, 200, 300, 400, 500, 600]
src = []
tgt = []
for i in num_examples:
    train_iter_2, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8, num_examples=i)
    src.append(len(src_vocab))
    tgt.append(len(tgt_vocab))

print(src, tgt)
```

    [40, 69, 102, 130, 159, 184] [40, 67, 107, 125, 163, 201]
    


```python
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.bar(num_examples, src, width=8, align='edge', label='source')
ax.bar(num_examples, tgt, width=-8, align='edge', label='target')
ax.set_xlabel('num_examples')
ax.set_ylabel('Count')
ax.legend()
plt.show()
```


    
![svg](output_118_0.svg)
    


&emsp;&emsp;在tokenize_nmt函数中，num_examples主要用于规定进行词元化的序列的长度，因此num_examples越大，所获得的词表越大。

### 练习 9.5.2

某些语言（例如中文和日语）的文本没有单词边界指示符（例如空格）。对于这种情况，单词级词元化仍然是个好主意吗？为什么？

**解答：**

&emsp;&emsp;以中文为例，中文中不存在类似空格这样的单词边界指示符，但是中文可采用基于词典、基于规则或者基于统计的分词方法，针对中文文本进行分词，以此实现中文的词元化处理。  
&emsp;&emsp;这种方法可以将中文文本划分为单独的词语，以实现对中文的精确处理。例如在词义消歧时，单词级的中文分词可以帮助我们更准确的确定词语的上下文，以此来确定歧义词的准确意义。并且在信息检索任务中，很多检索都是基于关键词展开的，对中文进行单词级词元化，可以便于在索引中进行文本匹配。因此，即便中文和日语这些语言中没有单词边界指示符，但在进行自然语言处理时，单词级的词元化仍是非常关键的一步。

## 9.6 编码器-解码器架构

### 练习 9.6.1

假设我们使用神经网络来实现“编码器－解码器”架构，那么编码器和解码器必须是同一类型的神经网络吗？

**解答：**

&emsp;&emsp;不一定。虽然编码器和解码器通常是相同类型的神经网络，但它们可以是不同的网络架构。事实上，有些最先进的神经网络模型在编码器和解码器之间使用了不同的架构。例如，使用CNN作为编码器，然后使用RNN或Transformer作为解码器的架构在机器翻译任务中很常见。  
&emsp;&emsp;另外，编码器和解码器也可以在不同的领域应用不同类型的神经网络。例如，在图像到文本任务中，可以使用CNN作为编码器，将图像编码为特征向量，然后使用RNN或Transformer作为解码器生成相应的自然语言描述。因此，编码器和解码器可以是同一类型的神经网络，也可以是不同类型的神经网络，它实质上只是一种抽象的结构，该结构可以适用于多种神经网络，具体选择需要取决于具体的任务需求和设计思路。

### 练习 9.6.2

除了机器翻译，还有其它可以适用于”编码器－解码器“架构的应用吗？

**解答：**

&emsp;&emsp;目前，“编码器-解码器”结构已经在多种任务中取得了优秀的研究成果，并非仅仅局限于机器翻译一类任务中。例如语音识别任务，可以采用编码器接受音频信号，并将其编码为特征向量，再由解码器将其转化为文本内容；在图像生成任务中，编码器可以将输入的图像或文本内容进行编码，再由解码器根据编码的特征向量生成对应的图像；此外还有文本生成任务，编码器将输入文本进行编码后，再由解码器使用这些编码后的特征向量进行解码，生成与原始文本内容相似的文本。  
&emsp;&emsp;实际上，“编码器-解码器”是一个通用的抽象架构，其基本思想是将输入数据编码为一个便于神经网络学习和理解的中间表示，再由解码器将中间表示转换为所需要的输出。这种架构可以根据具体需求适用于不同的实际场景中。

## 9.7 序列到序列的学习（seq2seq） 

### 练习 9.7.1

试着通过调整超参数来改善翻译效果。

**解答：**

&emsp;&emsp;尝试将隐藏层和Embedding层维度修改为256，同时将batch_size修改为256，学习率降低至0.001


```python
import collections
import math
import torch
from torch import nn
from d2l import torch as d2l
```


```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列到序列学习的循环神经网络解码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, *args):
        return enc_outputs[1]

    def forward(self, X, state):
        # 输出'X'的形状：(batch_size,num_steps,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        # 广播context，使其具有与X相同的num_steps
        context = state[-1].repeat(X.shape[0], 1, 1)
        X_and_context = torch.cat((X, context), 2)
        output, state = self.rnn(X_and_context, state)
        output = self.dense(output).permute(1, 0, 2)
        # output的形状:(batch_size,num_steps,vocab_size)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```


```python
embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.1
batch_size, num_steps = 256, 10
lr, num_epochs, device = 0.001, 300, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

    loss 0.017, 24766.6 tokens/sec on cuda:0
    


    
![svg](output_136_1.svg)
    



```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {d2l.bleu(translation, fra, k=2):.3f}')
```

    go . => va !, bleu 1.000
    i lost . => j'ai perdu ., bleu 1.000
    he's calm . => il est mouillé mouillé ., bleu 0.548
    i'm home . => je suis chez <unk> moi qui ai ai <unk> gagné, bleu 0.434
    

### 练习 9.7.2

重新运行实验并在计算损失时不使用遮蔽，可以观察到什么结果？为什么会有这个结果？

**解答：**


```python
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    """带遮蔽的softmax交叉熵损失函数"""
    # pred的形状：(batch_size,num_steps,vocab_size)
    # label的形状：(batch_size,num_steps)
    # valid_len的形状：(batch_size,)
    def forward(self, pred, label, valid_len):
        weights = torch.ones_like(label)
        # weights = sequence_mask(weights, valid_len)
        self.reduction='none'
        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(
            pred.permute(0, 2, 1), label)
        weighted_loss = (unweighted_loss * weights).mean(dim=1)
        return weighted_loss
```


```python
loss = MaskedSoftmaxCELoss()
loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),
     torch.tensor([4, 2, 0]))
```




    tensor([2.3026, 2.3026, 2.3026])




```python
loss_s = d2l.MaskedSoftmaxCELoss()
loss_s(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),
     torch.tensor([4, 2, 0]))
```




    tensor([2.3026, 1.1513, 0.0000])



&emsp;&emsp;观察上面两组值，我们可以发现，如果计算损失时不使用遮蔽，将会输出三个相同的序列损失值，而非出现第一个序列的损失为第二个序列的两倍，第三个序列的损失为零。  
&emsp;&emsp;出现这种情况的原因在于，注释掉遮蔽函数 **sequence_mask()** 后，无法再根据有效长度 **valid_len** 对权重 **weights** 进行遮蔽操作，使得所有位置的权重都将参与计算，而非仅限于有效长度内的位置。这可能会导致在计算损失时，对序列中无效的部分产生误差，影响模型的训练效果。

### 练习 9.7.3

如果编码器和解码器的层数或者隐藏单元数不同，那么如何初始化解码器的隐状态？

**解答：**

&emsp;&emsp;如果编码器和解码器的层数或隐藏单元数不同，可以采取以下方法来初始化解码器的隐状态：
- 使用编码器隐藏状态的线性变换：如果编码器和解码器的隐藏单元数不同，可以通过线性变换将编码器隐藏状态的维度调整为解码器隐藏状态所需的维度。可以使用全连接层或线性映射来实现这个变换。
- 使用零向量进行初始化：如果编码器和解码器的隐藏单元数差异较大，或者无法进行有效的维度变换，可以使用零向量来初始化解码器的隐状态。这种方法可能会导致一开始解码器的性能较差，但随着训练的进行，解码器可以逐渐学习到适应目标任务的隐状态表示。

&emsp;&emsp;下面是引入全连接层进行修改的示例：


```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列到序列学习的循环神经网络解码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens_enc, num_hiddens_dec, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size + num_hiddens_dec, num_hiddens_dec, num_layers,
                          dropout=dropout)
        self.fc = nn.Linear(num_hiddens_enc, num_hiddens_dec)  # 添加全连接层
        self.dense = nn.Linear(num_hiddens_dec, vocab_size)

    def init_state(self, enc_outputs, *args):
        return self.fc(enc_outputs[1])  # 使用全连接层调整隐藏状态的维度

    def forward(self, X, state):
        # 输出'X'的形状：(batch_size, num_steps, embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        # 广播context，使其具有与X相同的num_steps
        context = state[-1].repeat(X.shape[0], 1, 1)
        X_and_context = torch.cat((X, context), 2)

        output, state = self.rnn(X_and_context, state)
        output = self.dense(output).permute(1, 0, 2)
        # output的形状: (batch_size, num_steps, vocab_size)
        # state的形状: (num_layers, batch_size, num_hiddens_dec)
        return output, state

```


```python
encoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
encoder.eval()
X = torch.zeros((4, 7), dtype=torch.long)
output, state = encoder(X)
output.shape
```




    torch.Size([7, 4, 16])




```python
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens_enc=16, num_hiddens_dec=10,
                         num_layers=2)
decoder.eval()
state = decoder.init_state(encoder(X))
output, state = decoder(X, state)
output.shape, state.shape
```




    (torch.Size([4, 7, 10]), torch.Size([2, 4, 10]))



###  练习 9.7.4

在训练中，如果用前一时间步的预测输入到解码器来代替强制教学，对性能有何影响？

**解答：**

&emsp;&emsp;在训练中，这种采用前一时间步的预测作为解码器输入的方法属于 **自回归** 方法，这种自回归的方法，对于性能会产生以下影响：
- 训练速度变慢：使用自回归训练方式会导致解码器逐步生成输出序列，而不是一次性使用整个目标序列。这会增加训练的时间，因为需要进行逐步的预测和反向传播。
- 累积误差：由于解码器的输入是前一时间步的预测结果，而不是真实目标序列，错误可能会逐步累积并扩大。这可能导致生成的序列与真实目标序列之间存在较大的差异。
- 降低训练稳定性：自回归训练方式可能会导致训练过程更不稳定，特别是在序列较长、任务复杂的情况下。错误的累积和误差传播可能会导致训练困难，并且模型可能会在某些时间步骤上产生不合理的输出。
- 降低性能：自回归训练方式可能会降低模型在测试集或实际应用中的性能。由于解码器在训练期间没有接触到真实的目标序列，它可能无法很好地适应真实数据的分布和模式。

&emsp;&emsp;即便如此，目前自回归方法仍被广泛应用于序列生成任务中，通过引入如注意力机制等改进方法后，模型可以有效提升性能，生成拟合度更好的序列。


```python
#@save
def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):
    """训练序列到序列模型"""
    def xavier_init_weights(m):
        if type(m) == nn.Linear:
            nn.init.xavier_uniform_(m.weight)
        if type(m) == nn.GRU:
            for param in m._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(m._parameters[param])

    net.apply(xavier_init_weights)
    net.to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = MaskedSoftmaxCELoss()
    net.train()
    animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                     xlim=[10, num_epochs])
    for epoch in range(num_epochs):
        timer = d2l.Timer()
        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量
        for batch in data_iter:
            optimizer.zero_grad()
            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]
            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],
                          device=device).reshape(-1, 1)
            # dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学

            dec_input = bos
            state = None
            output_seq = []
            for i in range(Y.shape[1]):
                Y_hat, state = net(X, dec_input, X_valid_len, state)    # 将<bos>输入，获得预测结果
                dec_input = Y_hat.argmax(dim=-1)  # 使用模型预测的结果作为下一个时间步的输入
                output_seq.append(Y_hat)
            output_seq = torch.cat(output_seq, dim=1)       # 拼接所有预测结果，获得完整序列
            Y_hat = output_seq

            # Y_hat, _ = net(X, dec_input, X_valid_len)
            l = loss(Y_hat, Y, Y_valid_len)
            l.sum().backward()      # 损失函数的标量进行“反向传播”
            d2l.grad_clipping(net, 1)
            num_tokens = Y_valid_len.sum()
            optimizer.step()
            with torch.no_grad():
                metric.add(l.sum(), num_tokens)
        if (epoch + 1) % 10 == 0:
            animator.add(epoch + 1, (metric[0] / metric[1],))
    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '
        f'tokens/sec on {str(device)}')
```


```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列到序列学习的循环神经网络解码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, *args):
        return enc_outputs[1]

    def forward(self, X, state):
        # 输出'X'的形状：(batch_size,num_steps,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        # 广播context，使其具有与X相同的num_steps
        context = state[-1].repeat(X.shape[0], 1, 1)
        X_and_context = torch.cat((X, context), 2)
        output, state = self.rnn(X_and_context, state)
        output = self.dense(output).permute(1, 0, 2)
        # output的形状:(batch_size,num_steps,vocab_size)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```


```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 300, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = d2l.EncoderDecoder(encoder, decoder)
train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

    loss 0.064, 9132.0 tokens/sec on cuda:0
    


    
![svg](output_155_1.svg)
    



```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {d2l.bleu(translation, fra, k=2):.3f}')
```

    go . => va va ne va ne trouve trouve trouve trouve trouve, bleu 0.000
    i lost . => j'ai je je tu tu tu tu tu tu tu, bleu 0.000
    he's calm . => c'est comme comme comme comme comme comme comme comme comme, bleu 0.000
    i'm home . => je je je je je je je je je je, bleu 0.000
    

### 练习 9.7.5

用长短期记忆网络替换门控循环单元重新运行实验。

**解答：**


```python
#@save
class Seq2SeqEncoder(d2l.Encoder):
    """用于序列到序列学习的循环神经网络编码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        # 嵌入层
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers,
                          dropout=dropout)

    def forward(self, X, *args):
        # 输出'X'的形状：(batch_size,num_steps,embed_size)
        X = self.embedding(X)
        # 在循环神经网络模型中，第一个轴对应于时间步
        X = X.permute(1, 0, 2)
        # 如果未提及状态，则默认为0
        output, state = self.rnn(X)
        # output的形状:(num_steps,batch_size,num_hiddens)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```


```python
encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
encoder.eval()
X = torch.zeros((4, 7), dtype=torch.long)
output, state = encoder(X)
output.shape
```




    torch.Size([7, 4, 16])




```python
state[0].shape
```




    torch.Size([2, 4, 16])




```python
class Seq2SeqDecoder(d2l.Decoder):
    """用于序列到序列学习的循环神经网络解码器"""
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size + num_hiddens, num_hiddens, num_layers,
                          dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, *args):
        state = enc_outputs[1]
        return state

    def forward(self, X, state):
        # 输出'X'的形状：(batch_size,num_steps,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        # 广播context，使其具有与X相同的num_steps
        context = state[-1][0].unsqueeze(0).repeat(X.shape[0], 1, 1)    # 在LSTM中，相较于隐藏状态，记忆元中存储了更多信息，有利于捕捉长期依赖关系，因此这里提取记忆元而非隐藏状态
        X_and_context = torch.cat((X, context), 2)
        output, state = self.rnn(X_and_context, state)
        output = self.dense(output).permute(1, 0, 2)
        # output的形状:(batch_size,num_steps,vocab_size)
        # state的形状:(num_layers,batch_size,num_hiddens)
        return output, state
```


```python
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,
                         num_layers=2)
decoder.eval()
state = decoder.init_state(encoder(X))

output, state = decoder(X, state)
output.shape, state[0].shape
```




    (torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))




```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1
batch_size, num_steps = 64, 10
lr, num_epochs, device = 0.005, 500, d2l.try_gpu()

train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)
encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,
                        dropout)
net = d2l.EncoderDecoder(encoder, decoder)
d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)
```

    loss 0.018, 19088.3 tokens/sec on cuda:0
    


    
![svg](output_164_1.svg)
    



```python
engs = ['go .', "i lost .", 'he\'s calm .', 'i\'m home .']
fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
for eng, fra in zip(engs, fras):
    translation, attention_weight_seq = d2l.predict_seq2seq(
        net, eng, src_vocab, tgt_vocab, num_steps, device)
    print(f'{eng} => {translation}, bleu {d2l.bleu(translation, fra, k=2):.3f}')
```

    go . => va !, bleu 1.000
    i lost . => j'ai perdu ., bleu 1.000
    he's calm . => il est calme ., bleu 1.000
    i'm home . => je suis chez moi ., bleu 1.000
    

### 练习 9.7.6

有没有其他方法来设计解码器的输出层？

**解答：**

&emsp;&emsp;可以在解码器的输出层应用注意力机制，通过计算注意力权重来对编码器的隐藏状态进行加权求和，从而生成输出。在类似机器翻译任务中，这种方法可以将解码器的注意力集中在输入序列的相关部分，有效捕捉重要信息。提高生成的准确性。

## 9.8 束搜索 

### 练习 9.8.1

我们可以把穷举搜索看作一种特殊的束搜索吗？为什么？

**解答：**

&emsp;&emsp;穷举与束搜索采用了不同的思路，并不能将其看作是特殊的束搜索。  
&emsp;&emsp;在进行穷举时，算法会对解空间中的所有解进行遍历搜索，然后计算出最优结果。因此导致计算量巨大。但是束搜索则是根据设置好的束宽$K$来进行搜索，在每一步选择$K$个最优结果，在保证找到最优解的前提下，尽量减少空间与时间复杂度。实际上，贪心和束搜索都属于启发式算法的一种，而穷举搜索并不包含任何启发式信息，因此不能看作是特殊的束搜搜。

### 练习 9.8.2

在9.7节中的机器翻译问题中应用束搜索。束宽是如何影响预测的速度和结果的？

**解答：**

&emsp;&emsp;在机器翻译任务中，虽然应用束搜索可以有效提高预测的准确性，生成更加合理的翻译结果。但是束搜索也会通过束宽来影响模型的预测速度和结果。  
&emsp;&emsp;作为控制候选项数量的参数，束宽较小时会减少搜索空间，带来更快的预测速度。而在较大时会保留更多的候选项，虽然可以带来更加准确的结果，但是也会增加计算的复杂性和预测时间。因此在设置束宽时需要权衡预测速度和结果精度，根据实际需求和性能进行调整，以取得满意的翻译结果。

### 练习 9.8.3

在8.5节中，我们基于用户提供的前缀，通过使用语言模型来生成文本。这个例子中使用了哪种搜索策略？可以改进吗？

**解答：**

&emsp;&emsp;在8.5节中，最后在生成文本时，在每一步预测中，使用 **y.argmax(dim=1)** 选择具有最高概率的字符作为下一个预测字符。这里的选择是基于当前时刻模型的输出概率最大化的策略，而没有考虑其他备选字符,这种搜索算法实质上属于贪心算法。  
&emsp;&emsp;可以通过修改策略来生成更具多样性的文本，例如通过束搜索的方式，在每一步选择多个候选字符，并根据概率得分进行排序。最终只保留得分最高的一部分候选字符，然后基于这些候选字符继续生成下一个字符。这种束搜索算法可以提供更好的生成文本质量和连贯性，但会增加计算复杂度。
