- 2æœˆ27æ—¥ï¼Œå¼€å§‹å­¦ä¹ æœ¬ä¹¦ï¼ŒåŠ æ²¹~

# cudaå®‰è£…

- é¦–å…ˆæŸ¥çœ‹è‡ªå·±ç”µè„‘æ”¯æŒçš„cudaæœ€é«˜ç‰ˆæœ¬ï¼ˆå‰ææ˜¯æœ‰nvidiaæ˜¾å¡ï¼‰

```Plain Text
å‘½ä»¤è¡Œè¾“å…¥ï¼šnvidia-smi

C:\Users\10213>nvidia-smi
Mon Feb 27 20:09:27 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 512.36       Driver Version: 512.36       CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
| N/A   38C    P0    26W /  N/A |   1645MiB /  6144MiB |      1%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

- æˆ‘è¿™é‡Œæœ€é«˜æ”¯æŒCUDA Version: 11.6

- `nvcc --version`:æŸ¥çœ‹`cuda`ç‰ˆæœ¬

- å®‰è£…torch-GPU

  - å»å®˜ç½‘å®‰è£…ã€‚

  ![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image.png)

  - ä¸Šå›¾æ˜¯åœ¨torchå®˜ç½‘çš„ç‰ˆæœ¬é€‰æ‹©ï¼Œä»¥ä¸‹æ˜¯å®˜ç½‘

  [PyTorch](https://pytorch.org/get-started/locally/)

  - `torch.cuda.is_available()`ï¼šå¯ä»¥æŸ¥çœ‹`torch`æ˜¯å¦æ”¯æŒ`GPU`è¿ç®—ã€‚

# ch-2

#### è¯¾åç»ƒä¹ 

```Python
# è¯¾åç»ƒä¹ 
X < Y, X > Y
```

```Python
(tensor([[ True, False,  True, False],
 Â  Â  Â  Â  [False, False, False, False],
 Â  Â  Â  Â  [False, False, False, False]]),
 tensor([[False, False, False, False],
 Â  Â  Â  Â  [ True,  True,  True,  True],
 Â  Â  Â  Â  [ True,  True,  True,  True]]))
```

```Python
# ç”¨å…¶ä»–å½¢çŠ¶ï¼ˆå¦‚ä¸‰é˜¶å¼ é‡ï¼‰æ›¿æ¢å¹¿æ’­ä¸­æŒ‰å…ƒç´ æ“ä½œçš„å¼ é‡ã€‚ç»“æœæ˜¯å¦ä¸é¢„æœŸç›¸åŒ
a = torch.arange(2*3*4).reshape((2, 3, 4))
b = torch.arange(1*3*4).reshape((1, 3, 4))
a, b
```

```Python
(tensor([[[ 0,  1,  2,  3],
 Â  Â  Â  Â   [ 4,  5,  6,  7],
 Â  Â  Â  Â   [ 8,  9, 10, 11]],
 
 Â  Â  Â  Â  [[12, 13, 14, 15],
 Â  Â  Â  Â   [16, 17, 18, 19],
 Â  Â  Â  Â   [20, 21, 22, 23]]]),
 tensor([[[ 0,  1,  2,  3],
 Â  Â  Â  Â   [ 4,  5,  6,  7],
 Â  Â  Â  Â   [ 8,  9, 10, 11]]]))
```

```Python
a + b, a*b
```

```Python
(tensor([[[ 0,  2,  4,  6],
 Â  Â  Â  Â   [ 8, 10, 12, 14],
 Â  Â  Â  Â   [16, 18, 20, 22]],
 
 Â  Â  Â  Â  [[12, 14, 16, 18],
 Â  Â  Â  Â   [20, 22, 24, 26],
 Â  Â  Â  Â   [28, 30, 32, 34]]]),
 tensor([[[  0, Â  1, Â  4, Â  9],
 Â  Â  Â  Â   [ 16,  25,  36,  49],
 Â  Â  Â  Â   [ 64,  81, 100, 121]],
 
 Â  Â  Â  Â  [[  0,  13,  28,  45],
 Â  Â  Â  Â   [ 64,  85, 108, 133],
 Â  Â  Â  Â   [160, 189, 220, 253]]]))
```

### 2.2 é¢„å¤‡çŸ¥è¯†

```Python
import torch
import pandas as pd
import os


os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
f.write('NumRooms,Alley,Price\n')  # åˆ—å
f.write('NA,Pave,127500\n')  # æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ•°æ®æ ·æœ¬
f.write('2,NA,106000\n')
f.write('4,NA,178100\n')
f.write('NA,NA,140000\n')


# è¯»å–æ•°æ®
data = pd.read_csv(data_file)

# å¤„ç†ç¼ºå¤±å€¼
inputs = data.iloc[:, :2]
outputs = data.iloc[:, -1]
inputs = inputs.fillna(inputs.mean())  # åœ¨nanå¤„å¡«å†™è¯¥åˆ—çš„å¹³å‡å€¼

# å¯¹äºinputsä¸­çš„ç±»åˆ«å€¼æˆ–ç¦»æ•£å€¼ï¼Œæˆ‘ä»¬å°†â€œNaNâ€è§†ä¸ºä¸€ä¸ªç±»åˆ«ã€‚ ç”±äºâ€œå··å­ç±»å‹â€ï¼ˆâ€œAlleyâ€ï¼‰åˆ—åªæ¥å—ä¸¤ç§ç±»å‹çš„ç±»åˆ«å€¼â€œPaveâ€å’Œâ€œNaNâ€ï¼Œ pandaså¯ä»¥è‡ªåŠ¨å°†æ­¤åˆ—è½¬æ¢ä¸ºä¸¤åˆ—â€œAlley_Paveâ€å’Œâ€œAlley_nanâ€ã€‚ å··å­ç±»å‹ä¸ºâ€œPaveâ€çš„è¡Œä¼šå°†â€œAlley_Paveâ€çš„å€¼è®¾ç½®ä¸º1ï¼Œâ€œAlley_nanâ€çš„å€¼è®¾ç½®ä¸º0ã€‚ ç¼ºå°‘å··å­ç±»å‹çš„è¡Œä¼šå°†â€œAlley_Paveâ€å’Œâ€œAlley_nanâ€åˆ†åˆ«è®¾ç½®ä¸º0å’Œ1ã€‚
inputs = pd.get_dummies(inputs, dummy_na=True)

# è½¬ä¸ºå¼ é‡æ ¼å¼
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)


```

#### è¯¾åä¹ é¢˜

```Python
# è¯¾åä¹ é¢˜
# åˆ›å»ºåŒ…å«åŒ…å«æ›´å¤šæ›´å¤šè¡Œå’Œåˆ—çš„æ•°æ®å¯„
# ï¼ˆ1ï¼‰åˆ é™¤ç¼ºå¤±å€¼æœ€å¤šçš„åˆ—
# ï¼ˆ2ï¼‰å°†å¤„ç†åçš„æ•°æ®é›†è½¬åŒ–ä¸ºå¼ é‡æ ¼å¼

# åˆ›å»ºæ›´å¤šçš„æ•°æ®
os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny_è¯¾åé¢˜.csv')
with open(data_file, 'w') as f:
    f.write('NumRooms,Alley,Price\n')  # åˆ—å
    for _ in range(12):
        f.write('NA,Pave,127500\n')  # æ¯è¡Œè¡¨ç¤ºä¸€ä¸ªæ•°æ®æ ·æœ¬
        f.write('2,NA,106000\n')
        f.write('4,NA,178100\n')
        f.write('NA,NA,140000\n')

data = pd.read_csv('../data/house_tiny_è¯¾åé¢˜.csv')
data.dropna(axis=1).head()        
```

## 2.3 çº¿æ€§ä»£æ•°

1. è¯æ˜ä¸€ä¸ªçŸ©é˜µ$\mathbf{A}$çš„è½¬ç½®çš„è½¬ç½®æ˜¯$\mathbf{A}$ï¼Œå³$(\mathbf{A}^\top)^\top = \mathbf{A}$ã€‚

2. ç»™å‡ºä¸¤ä¸ªçŸ©é˜µ$\mathbf{A}$å’Œ$\mathbf{B}$ï¼Œè¯æ˜â€œå®ƒä»¬è½¬ç½®çš„å’Œâ€ç­‰äºâ€œå®ƒä»¬å’Œçš„è½¬ç½®â€ï¼Œå³$\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top$ã€‚

3. ç»™å®šä»»æ„æ–¹é˜µ$\mathbf{A}$ï¼Œ$\mathbf{A} + \mathbf{A}^\top$æ€»æ˜¯å¯¹ç§°çš„å—?ä¸ºä»€ä¹ˆ?

4. æœ¬èŠ‚ä¸­å®šä¹‰äº†å½¢çŠ¶$(2,3,4)$çš„å¼ é‡`X`ã€‚`len(X)`çš„è¾“å‡ºç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ

5. å¯¹äºä»»æ„å½¢çŠ¶çš„å¼ é‡`X`,`len(X)`æ˜¯å¦æ€»æ˜¯å¯¹åº”äº`X`ç‰¹å®šè½´çš„é•¿åº¦?è¿™ä¸ªè½´æ˜¯ä»€ä¹ˆ?

6. è¿è¡Œ`A/A.sum(axis=1)`ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚è¯·åˆ†æä¸€ä¸‹åŸå› ï¼Ÿ

7. è€ƒè™‘ä¸€ä¸ªå…·æœ‰å½¢çŠ¶$(2,3,4)$çš„å¼ é‡ï¼Œåœ¨è½´0ã€1ã€2ä¸Šçš„æ±‚å’Œè¾“å‡ºæ˜¯ä»€ä¹ˆå½¢çŠ¶?

8. ä¸º`linalg.norm`å‡½æ•°æä¾›3ä¸ªæˆ–æ›´å¤šè½´çš„å¼ é‡ï¼Œå¹¶è§‚å¯Ÿå…¶è¾“å‡ºã€‚å¯¹äºä»»æ„å½¢çŠ¶çš„å¼ é‡è¿™ä¸ªå‡½æ•°è®¡ç®—å¾—åˆ°ä»€ä¹ˆ?

```Python
import torch
import numpy as np

# è¯æ˜ä¸€ä¸ªçŸ©é˜µ ğ€ çš„è½¬ç½®çš„è½¬ç½®æ˜¯ ğ€
A = torch.arange(12).reshape((3, 4))
A.T.T == A

tensor([[True, True, True, True],
        [True, True, True, True],
        [True, True, True, True]])

# ç»™å‡ºä¸¤ä¸ªçŸ©é˜µ ğ€å’Œ ğ ï¼Œè¯æ˜â€œå®ƒä»¬è½¬ç½®çš„å’Œâ€ç­‰äºâ€œå®ƒä»¬å’Œçš„è½¬ç½®â€
B = torch.randn((3, 4))
A.T + B.T == (A + B).T

tensor([[True, True, True],
        [True, True, True],
        [True, True, True],
        [True, True, True]])

# ç»™å®šä»»æ„æ–¹é˜µ ğ€ï¼Œ ğ€+ğ€âŠ¤,æ€»æ˜¯å¯¹ç§°çš„å—?ä¸ºä»€ä¹ˆ?
C = torch.arange(16).reshape(4, -1)
C + C.T

tensor([[ 0,  5, 10, 15],
        [ 5, 10, 15, 20],
        [10, 15, 20, 25],
        [15, 20, 25, 30]])

# æœ¬èŠ‚ä¸­å®šä¹‰äº†å½¢çŠ¶ (2,3,4)çš„å¼ é‡Xã€‚len(X)çš„è¾“å‡ºç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ
# ç­”ï¼šlen(X)ä¸ºå¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦å¤§å°
X = torch.arange(24).reshape((2, 3, 4))
len(X)

2

# å¯¹äºä»»æ„å½¢çŠ¶çš„å¼ é‡X,len(X)æ˜¯å¦æ€»æ˜¯å¯¹åº”äºXç‰¹å®šè½´çš„é•¿åº¦?è¿™ä¸ªè½´æ˜¯ä»€ä¹ˆ?
# è¿™ä¸ªè½´æ˜¯å¼ é‡çš„ç¬¬0ä¸ªè½´ï¼Œ å³dim=0æ—¶çš„ï¼Œé•¿åº¦

# è¿è¡ŒA/A.sum(axis=1)ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚è¯·åˆ†æä¸€ä¸‹åŸå› ï¼Ÿ
A, A.sum(axis=1)
# è¿è¡ŒA/A.sum(axis=1)ä¼šæŠ¥é”™ï¼ŒAçš„shapeä¸ºï¼ˆ3ï¼Œ4ï¼‰ï¼Œ A.sum(axis=1)æ˜¯æŒ‰ç¬¬äºŒä¸ªè½´è®¡ç®—æ€»å’Œé™ç»´ï¼Œä¸¤ä¸ªå¼ é‡çš„çº¬åº¦ä¸åŒï¼Œæ— æ³•å¹¿æ’­

(tensor([[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]]),
 tensor([ 6, 22, 38]))

# è€ƒè™‘ä¸€ä¸ªå…·æœ‰å½¢çŠ¶ (2,3,4)çš„å¼ é‡ï¼Œåœ¨è½´0ã€1ã€2ä¸Šçš„æ±‚å’Œè¾“å‡ºæ˜¯ä»€ä¹ˆå½¢çŠ¶?
X.sum(dim=0),X.sum(dim=0).shape, X.sum(dim=1),X.sum(dim=1).shape, X.sum(dim=2),X.sum(dim=2).shape,
# ä¼šæŒ‰dimçš„è½´å¯¹å¼ é‡è¿›è¡Œé™ç»´

(tensor([[12, 14, 16, 18],
         [20, 22, 24, 26],
         [28, 30, 32, 34]]),
 torch.Size([3, 4]),
 tensor([[12, 15, 18, 21],
         [48, 51, 54, 57]]),
 torch.Size([2, 4]),
 tensor([[ 6, 22, 38],
         [54, 70, 86]]),
 torch.Size([2, 3]))

# ä¸ºtorch.norm(L2èŒƒæ•°)å‡½æ•°æä¾›3ä¸ªæˆ–æ›´å¤šè½´çš„å¼ é‡ï¼Œå¹¶è§‚å¯Ÿå…¶è¾“å‡ºã€‚å¯¹äºä»»æ„å½¢çŠ¶çš„å¼ é‡è¿™ä¸ªå‡½æ•°è®¡ç®—å¾—åˆ°ä»€ä¹ˆ?
# torch.norm(),çŸ©é˜µå¿…é¡»ä¸ºfloatç±»å‹
D = torch.arange(24, dtype=torch.float64).reshape((2, 3, 4))
torch.norm(D)

tensor(65.7571, dtype=torch.float64)
```

## 2.4 å¾®ç§¯åˆ†

9. ç»˜åˆ¶å‡½æ•°$y = f(x) = x^3 - \frac{1}{x}$å’Œå…¶åœ¨$x = 1$å¤„åˆ‡çº¿çš„å›¾åƒã€‚

10. æ±‚å‡½æ•°$f(\mathbf{x}) = 3x_1^2 + 5e^{x_2}$çš„æ¢¯åº¦ã€‚

11. å‡½æ•°$f(\mathbf{x}) = \|\mathbf{x}\|_2$çš„æ¢¯åº¦æ˜¯ä»€ä¹ˆï¼Ÿ

12. å°è¯•å†™å‡ºå‡½æ•°$u = f(x, y, z)$ï¼Œå…¶ä¸­$x = x(a, b)$ï¼Œ$y = y(a, b)$ï¼Œ$z = z(a, b)$çš„é“¾å¼æ³•åˆ™ã€‚

```Python
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l

# ç»˜åˆ¶å‡½æ•° ğ‘¦=ğ‘“(ğ‘¥)=ğ‘¥3âˆ’1/ğ‘¥å’Œå…¶åœ¨ ğ‘¥=1å¤„åˆ‡çº¿çš„å›¾åƒã€‚
x = np.arange(0, 3, 0.1)
d2l.plot(x, [x**3 - 1/x, 4 * x - 4], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 1.png)

```Python
# æ±‚å‡½æ•° ğ‘“(ğ±)=3ğ‘¥21+5ğ‘’ğ‘¥2 çš„æ¢¯åº¦ã€‚
# [6x_1, 5e^x_2]

# å‡½æ•° ğ‘“(ğ±)=â€–ğ±â€–2 çš„æ¢¯åº¦æ˜¯ä»€ä¹ˆï¼Ÿ
# let y=f(x), Df(x) = x/y
```

## 2.5 è‡ªåŠ¨å¾®åˆ†

ä½¿$f(x)=\sin(x)$ï¼Œç»˜åˆ¶$f(x)$å’Œ$\frac{df(x)}{dx}$çš„å›¾åƒï¼Œå…¶ä¸­åè€…ä¸ä½¿ç”¨$f'(x)=\cos(x)$ã€‚

```Python
import torch
a = torch.arange(0,10,0.1, dtype=torch.float64, requires_grad=True)

a.grad
y = torch.sin(a)
y.sum().backward()
a.grad

d2l.plot(a.detach(), [y.detach(), a.grad], 'a', 'y_', legend=['a', 'y_'])

```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 2.png)

## 2.6 æ¦‚ç‡

è¿›è¡Œğ‘š=500ç»„å®éªŒï¼Œæ¯ç»„æŠ½å–ğ‘›=10ä¸ªæ ·æœ¬ã€‚æ”¹å˜ğ‘šå’Œğ‘›ï¼Œè§‚å¯Ÿå’Œåˆ†æå®éªŒç»“æœã€‚

```Python
%matplotlib inline
import torch
from torch.distributions import multinomial
from d2l import torch as d2l

# è¿›è¡Œ ğ‘š=500 ç»„å®éªŒï¼Œæ¯ç»„æŠ½å– ğ‘›=10 ä¸ªæ ·æœ¬ã€‚æ”¹å˜ ğ‘š å’Œ ğ‘› ï¼Œè§‚å¯Ÿå’Œåˆ†æå®éªŒç»“æœã€‚
fair_probs = torch.ones([6]) / 6
counts = multinomial.Multinomial(256, fair_probs).sample((10000,))
cum_counts = counts.cumsum(dim=0)
estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)

d2l.set_figsize((6, 4.5))
for i in range(6):
    d2l.plt.plot(estimates[:, i].numpy(),
                 label=("P(die=" + str(i + 1) + ")"))
d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')
d2l.plt.gca().set_xlabel('Groups of experiments')
d2l.plt.gca().set_ylabel('Estimated probability')
d2l.plt.legend();
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 3.png)

-  ç»™å®šä¸¤ä¸ªæ¦‚ç‡ä¸ºğ‘ƒ(A)å’Œğ‘ƒ(B)çš„äº‹ä»¶ï¼Œè®¡ç®—ğ‘ƒ(AâˆªB)å’Œğ‘ƒ(Aâˆ©B)çš„ä¸Šé™å’Œä¸‹é™ã€‚

$max(P(A), P(B)) \leq P(AUB)\leq P(A)+P(B)\\0<=P(A\cap B)<=min(P(A), P(B))$

# ch-3

## 3.1 çº¿æ€§å›å½’

- ç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š

- ï¼ˆ1ï¼‰åˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„å€¼ï¼Œå¦‚éšæœºåˆå§‹åŒ–ï¼› 

- ï¼ˆ2ï¼‰ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–å°æ‰¹é‡æ ·æœ¬ä¸”åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°ï¼Œå¹¶ä¸æ–­è¿­ä»£è¿™ä¸€æ­¥éª¤ã€‚ å¯¹äºå¹³æ–¹æŸå¤±å’Œä»¿å°„å˜æ¢ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®åœ°å†™æˆå¦‚ä¸‹å½¢å¼:

$$
\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}
$$



#### è¯¾åä¹ é¢˜

13. å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº›æ•°æ®$x_1, \ldots, x_n \in \mathbb{R}$ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªå¸¸æ•°$b$ï¼Œä½¿å¾—æœ€å°åŒ–$\sum_i (x_i - b)^2$ã€‚

  14. æ‰¾åˆ°æœ€ä¼˜å€¼$b$çš„è§£æè§£ã€‚

  15. è¿™ä¸ªé—®é¢˜åŠå…¶è§£ä¸æ­£æ€åˆ†å¸ƒæœ‰ä»€ä¹ˆå…³ç³»?

16. æ¨å¯¼å‡ºä½¿ç”¨å¹³æ–¹è¯¯å·®çš„çº¿æ€§å›å½’ä¼˜åŒ–é—®é¢˜çš„è§£æè§£ã€‚ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œå¯ä»¥å¿½ç•¥åç½®$b$ï¼ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡å‘$\mathbf X$æ·»åŠ æ‰€æœ‰å€¼ä¸º1çš„ä¸€åˆ—æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼‰ã€‚

  17. ç”¨çŸ©é˜µå’Œå‘é‡è¡¨ç¤ºæ³•å†™å‡ºä¼˜åŒ–é—®é¢˜ï¼ˆå°†æ‰€æœ‰æ•°æ®è§†ä¸ºå•ä¸ªçŸ©é˜µï¼Œå°†æ‰€æœ‰ç›®æ ‡å€¼è§†ä¸ºå•ä¸ªå‘é‡ï¼‰ã€‚

  18. è®¡ç®—æŸå¤±å¯¹$w$çš„æ¢¯åº¦ã€‚

  19. é€šè¿‡å°†æ¢¯åº¦è®¾ä¸º0ã€æ±‚è§£çŸ©é˜µæ–¹ç¨‹æ¥æ‰¾åˆ°è§£æè§£ã€‚

  20. ä»€ä¹ˆæ—¶å€™å¯èƒ½æ¯”ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ›´å¥½ï¼Ÿè¿™ç§æ–¹æ³•ä½•æ—¶ä¼šå¤±æ•ˆï¼Ÿ

21. å‡å®šæ§åˆ¶é™„åŠ å™ªå£°$\epsilon$çš„å™ªå£°æ¨¡å‹æ˜¯æŒ‡æ•°åˆ†å¸ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ$p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)$

  22. å†™å‡ºæ¨¡å‹$-\log P(\mathbf y \mid \mathbf X)$ä¸‹æ•°æ®çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ã€‚

  23. è¯·è¯•ç€å†™å‡ºè§£æè§£ã€‚

  24. æå‡ºä¸€ç§éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å“ªé‡Œå¯èƒ½å‡ºé”™ï¼Ÿï¼ˆæç¤ºï¼šå½“æˆ‘ä»¬ä¸æ–­æ›´æ–°å‚æ•°æ—¶ï¼Œåœ¨é©»ç‚¹é™„è¿‘ä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼‰è¯·å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

1.a
$$
\begin{array}{c}
\text { å³æ±‚ } \underset{b}{\operatorname{argmin}} \sum_{i=1}^{n}\left(x_{i}-b\right)^{2} \\
\Rightarrow \frac{\delta \sum_{i=1}^{n}\left(x_{i}-b\right)^{2}}{\delta b}=0 \\
\Rightarrow \sum_{i=1}^{n}\left(x_{i}-b\right)=0 \\
\Rightarrow \sum_{i=1}^{n} x_{i}-n b=0 \\
\Rightarrow b=\frac{\sum_{i=1}^{n} x_{i}}{n}
\end{array}
$$
$$

$\text { ä»¤ } x_i=b+\epsilon\left(\epsilon \sim N\left(0, \sigma^2\right)\right)$

                                                           åˆ™ $P\left(x_i \mid b\right)=\frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{1}{2 \sigma^2}\left(x^i-b\right)\right)$

## 3.2 ä»é›¶å®ç°çº¿æ€§å›å½’

- ç”Ÿæˆæ•°æ®é›†

```Python
def synthetic_data(w, b, num_examples):  #@save
    """ç”Ÿæˆy=Xw+b+å™ªå£°"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))
  
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 4.png)

```Python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # è¿™äº›æ ·æœ¬æ˜¯éšæœºè¯»å–çš„ï¼Œæ²¡æœ‰ç‰¹å®šçš„é¡ºåº
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

`indices:`ç´¢å¼•åˆ—è¡¨ï¼Œç„¶åå°†ç´¢å¼•åˆ—è¡¨æ‰“ä¹±ï¼Œ å†`features[batch_indices], labels[batch_indices]`æŠŠæ‰“ä¹±çš„æ ·æœ¬ä»æ•°æ®é›†ä¸­å–å‡ºã€‚

```Python
%matplotlib inline
import random
import torch
from d2l import torch as d2l

# æ•°æ®é›†çš„ç”Ÿæˆ
def synthetic_data(w, b, num_examples):  #@save
    """ç”Ÿæˆy=Xw+b+å™ªå£°"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1)) # è¿™é‡Œå›è¿”å›ä¸€ä¸ªnum_examplesè¡Œï¼Œ len(w)åˆ—çš„å¼ é‡ï¼›å’Œä¸€ä¸ªnum_examplesè¡Œï¼Œ1åˆ—çš„æ ‡ç­¾
  
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1001)

d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 5.png)

```Python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # æ‰“ä¹±ç´¢å¼•
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i:min(i+batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
        
next(iter(data_iter(10, features, labels)))
# åˆå§‹åŒ–å‚æ•°
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# å®šä¹‰æ¨¡å‹
def linreg(X, w, b):  #@save
    """çº¿æ€§å›å½’æ¨¡å‹"""
    return torch.matmul(X, w) + b
  
# æŸå¤±å‡½æ•°
def squared_loss(y_hat, y):  #@save
    """å‡æ–¹æŸå¤±"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
# ä¼˜åŒ–ç®—æ³•
def sgd(params, lr, batch_size):  #@save
    """å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
# è®­ç»ƒ
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss
batch_size = 10

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # Xå’Œyçš„å°æ‰¹é‡æŸå¤±
        # å› ä¸ºlå½¢çŠ¶æ˜¯(batch_size,1)ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚lä¸­çš„æ‰€æœ‰å…ƒç´ è¢«åŠ åˆ°ä¸€èµ·,
        # å¹¶ä»¥æ­¤è®¡ç®—å…³äº[w,b]çš„æ¢¯åº¦
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å‚æ•°çš„æ¢¯åº¦æ›´æ–°å‚æ•°
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

#### ä¹ é¢˜

**1.å¦‚æœæˆ‘ä»¬å°†æƒé‡åˆå§‹åŒ–ä¸ºé›¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç®—æ³•ä»ç„¶æœ‰æ•ˆå—ï¼Ÿ**

  å°è¯•è¿‡ï¼Œä»ç„¶æœ‰æ•ˆ

**2. å‡è®¾è¯•å›¾ä¸ºç”µå‹å’Œç”µæµçš„å…³ç³»å»ºç«‹ä¸€ä¸ªæ¨¡å‹ã€‚è‡ªåŠ¨å¾®åˆ†å¯ä»¥ç”¨æ¥å­¦ä¹ æ¨¡å‹çš„å‚æ•°å—?**

  å¯ä»¥ï¼Œå»ºç«‹æ¨¡å‹U=IW+bï¼Œç„¶åé‡‡é›†(Uï¼ŒI)çš„æ•°æ®é›†ï¼Œé€šè¿‡è‡ªåŠ¨å¾®åˆ†å³å¯å­¦ä¹ Wå’Œbçš„å‚æ•°ã€‚

**3.1. èƒ½åŸºäº[æ™®æœ—å…‹å®šå¾‹](https://en.wikipedia.org/wiki/Planck%27s_law)ä½¿ç”¨å…‰è°±èƒ½é‡å¯†åº¦æ¥ç¡®å®šç‰©ä½“çš„æ¸©åº¦å—ï¼Ÿ**

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 6.png)

**4.è®¡ç®—äºŒé˜¶å¯¼æ•°å¯èƒ½ä¼šé‡åˆ°ä»€ä¹ˆé—®é¢˜ï¼Ÿä½ ä¼šå¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼Ÿ**

  ä¸å¤ªæ‡‚ï¼Œç­‰å¤§ä½¬

**5.ä¸ºä»€ä¹ˆåœ¨`squared_loss`å‡½æ•°ä¸­éœ€è¦ä½¿ç”¨`reshape`å‡½æ•°ï¼Ÿ**

  ä¸€ä¸ªæ˜¯è¡Œå‘é‡ã€ä¸€ä¸ªæ˜¯åˆ—å‘é‡ï¼Œä½¿ç”¨reshapeï¼Œå¯ä»¥ç¡®ä¿shapeä¸€æ ·ã€‚

**6.å¦‚æœæ ·æœ¬ä¸ªæ•°ä¸èƒ½è¢«æ‰¹é‡å¤§å°æ•´é™¤ï¼Œ`data_iter`å‡½æ•°çš„è¡Œä¸ºä¼šæœ‰ä»€ä¹ˆå˜åŒ–ï¼Ÿ**

  å¹¶ä¸ä¼šæŠ¥é”™ï¼Œå› ä¸ºåœ¨**`data_iter`**å‡½æ•°ä¸­çš„batchæå–ä¸­æœ‰è¿™è¡Œä»£ç `indices[i: min(i + batch_size, num_examples)])`ï¼Œå½“æ ·æœ¬çš„æ•°é‡ä¸è¶³ä¸€ä¸ªbatchæ—¶ï¼Œå…¶ä»–çš„å…¨éƒ¨æ‹¿èµ°ï¼Œä½œä¸ºä¸€ä¸ªbatchã€‚

## 3.3 ç®€æ´å®ç°

```Python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l
from torch import nn

# ç”Ÿæˆæ•°æ®é›†
true_w = torch.tensor([2, -3.4])
true_b = torch.tensor(4.2)
feature, labels = d2l.synthetic_data(true_w, true_b, 1000)

def load_array(data_arrays, batch_size, is_train=True):  #@save
    """æ„é€ ä¸€ä¸ªPyTorchæ•°æ®è¿­ä»£å™¨"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

# è¯»å–æ•°æ®é›†
batch_size = 10
data_iter = d2l.load_array((feature, labels), batch_size)

# å®šä¹‰æ¨¡å‹
# æˆ‘ä»¬å°†ä¸¤ä¸ªå‚æ•°ä¼ é€’åˆ°nn.Linearä¸­ã€‚ ç¬¬ä¸€ä¸ªæŒ‡å®šè¾“å…¥ç‰¹å¾å½¢çŠ¶ï¼Œå³2ï¼Œç¬¬äºŒä¸ªæŒ‡å®šè¾“å‡ºç‰¹å¾å½¢çŠ¶ï¼Œè¾“å‡ºç‰¹å¾å½¢çŠ¶ä¸ºå•ä¸ªæ ‡é‡ï¼Œå› æ­¤ä¸º1ã€‚
net = nn.Sequential(nn.Linear(2, 1))

# å®šä¹‰æŸå¤±å‡½æ•°
loss = nn.MSELoss()

# å®šä¹‰ä¼˜åŒ–ç®—æ³•
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

# è®­ç»ƒ
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)  # æ¯ä¸ªbatchçš„æŸå¤±å‡½æ•°å€¼
        trainer.zero_grad()  # æ¢¯åº¦æ¸…é›¶
        l.backward()  # åå‘ä¼ æ’­
        trainer.step()  # æ›´æ–°å‚æ•°ï¼Œæ ¹æ®å½“å‰æ¢¯åº¦æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œä¸€ä¸ªbatchæ•°æ®ä¼šè®¡ç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œtrainer.step()æ›´æ–°
    l = loss(net(features), labels)  # ä¸€ä¸ªepochçš„æŸå¤±å‡½æ•°å€¼
    print(f'epoch {epoch + 1}, loss {l:f}')
    
w, b = net[0].weight.data, net[0].bias.data
print(f'wçš„è¯¯å·®ï¼š{true_w - w.reshape(true_w.shape)}')
print('bçš„è¯¯å·®ï¼š', true_b - b.reshape(true_b.shape))
```

### è¯¾åä¹ é¢˜

25. å¦‚æœå°†å°æ‰¹é‡çš„æ€»æŸå¤±æ›¿æ¢ä¸ºå°æ‰¹é‡æŸå¤±çš„å¹³å‡å€¼ï¼Œéœ€è¦å¦‚ä½•æ›´æ”¹å­¦ä¹ ç‡ï¼Ÿ

  `loss = nn.MSELoss(reduction='sum):`åº”è¯¥æŠŠå­¦ä¹ ç‡é™¤ä»¥batch_sizeï¼Œå› ä¸ºé»˜è®¤å‚æ•°æ˜¯`mean`ï¼Œæ›¿æ¢ä¸º`sum`éœ€è¦é™¤ä»¥æ‰¹é‡æ•°ã€‚

26. æŸ¥çœ‹æ·±åº¦å­¦ä¹ æ¡†æ¶æ–‡æ¡£ï¼Œå®ƒä»¬æä¾›äº†å“ªäº›æŸå¤±å‡½æ•°å’Œåˆå§‹åŒ–æ–¹æ³•ï¼Ÿç”¨HuberæŸå¤±ä»£æ›¿åŸæŸå¤±ï¼Œå³
$l(y,y') = \begin{cases}|y-y'| -\frac{\sigma}{2} & \text{ if } |y-y'| > \sigma \\ \frac{1}{2 \sigma} (y-y')^2 & \text{ å…¶å®ƒæƒ…å†µ}\end{cases}$

  è‡ªå¸¦å‡½æ•°`torch.nn.SmoothL1Loss()`

  [ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹å‚è€ƒç­”æ¡ˆ(ç¬¬äºŒç‰ˆ)-ç¬¬ä¸‰ç« ](https://www.jianshu.com/p/8b8206988655)

  ```Python
import torch.nn as nn
import torch.nn.functional as F

class HuberLoss(nn.Module):
    def __init__(self, sigma):
        super(HuberLoss, self).__init__()
        self.sigma = sigma
    def forward(self, y, y_hat):
        if F.l1_loss(y, y_hat) > self.sigma:
            loss = F.l1_loss(y, y_hat) - self.sigma/2
        else:
            loss = (1/(2*self.sigma))*F.mse_loss(y, y_hat)
        return loss
  ```

27. å¦‚ä½•è®¿é—®çº¿æ€§å›å½’çš„æ¢¯åº¦ï¼Ÿ

  ```Python
net[0].weight.grad
net[0].bias.grad
  ```

## 3.4 softmax

softmaxå‡½æ•°èƒ½å¤Ÿå°†æœªè§„èŒƒåŒ–çš„é¢„æµ‹å˜æ¢ä¸ºéè´Ÿæ•°å¹¶ä¸”æ€»å’Œä¸º1ï¼ŒåŒæ—¶è®©æ¨¡å‹ä¿æŒå¯å¯¼çš„æ€§è´¨ã€‚ä¸ºäº†å®Œæˆè¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹æ¯ä¸ªæœªè§„èŒƒåŒ–çš„é¢„æµ‹æ±‚å¹‚ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿è¾“å‡ºéè´Ÿã€‚ä¸ºäº†ç¡®ä¿æœ€ç»ˆè¾“å‡ºçš„æ¦‚ç‡å€¼æ€»å’Œä¸º1ï¼Œæˆ‘ä»¬å†è®©æ¯ä¸ªæ±‚å¹‚åçš„ç»“æœé™¤ä»¥å®ƒä»¬çš„æ€»å’Œã€‚å¦‚ä¸‹å¼ï¼š

                                                           $\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{å…¶ä¸­}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$

è¿™é‡Œï¼Œå¯¹äºæ‰€æœ‰çš„$j$æ€»æœ‰$0 \leq \hat{y}_j \leq 1$ã€‚å› æ­¤ï¼Œ$\hat{\mathbf{y}}$å¯ä»¥è§†ä¸ºä¸€ä¸ªæ­£ç¡®çš„æ¦‚ç‡åˆ†å¸ƒã€‚`softmax`è¿ç®—ä¸ä¼šæ”¹å˜æœªè§„èŒƒåŒ–çš„é¢„æµ‹$\mathbf{o}$ä¹‹é—´çš„å¤§å°æ¬¡åºï¼Œåªä¼šç¡®å®šåˆ†é…ç»™æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œåœ¨é¢„æµ‹è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ç”¨ä¸‹å¼æ¥é€‰æ‹©æœ€æœ‰å¯èƒ½çš„ç±»åˆ«ã€‚

$\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.$

å°½ç®¡softmaxæ˜¯ä¸€ä¸ªéçº¿æ€§å‡½æ•°ï¼Œä½†***softmaxå›å½’çš„è¾“å‡ºä»ç„¶ç”±è¾“å…¥ç‰¹å¾çš„ä»¿å°„å˜æ¢å†³å®š***ã€‚å› æ­¤ï¼Œsoftmaxå›å½’æ˜¯ä¸€ä¸ª*çº¿æ€§æ¨¡å‹*ï¼ˆlinear modelï¼‰ã€‚

## 3.5 å›¾åƒåˆ†ç±»æ•°æ®é›†

28. å‡å°‘`batch_size`ï¼ˆå¦‚å‡å°‘åˆ°1ï¼‰æ˜¯å¦ä¼šå½±å“è¯»å–æ€§èƒ½ï¼Ÿ

  ä¼š

29. æ•°æ®è¿­ä»£å™¨çš„æ€§èƒ½éå¸¸é‡è¦ã€‚å½“å‰çš„å®ç°è¶³å¤Ÿå¿«å—ï¼Ÿæ¢ç´¢å„ç§é€‰æ‹©æ¥æ”¹è¿›å®ƒã€‚

  è¶³å¤Ÿå¿«äº†

30. æŸ¥é˜…æ¡†æ¶çš„åœ¨çº¿APIæ–‡æ¡£ã€‚è¿˜æœ‰å“ªäº›å…¶ä»–æ•°æ®é›†å¯ç”¨ï¼Ÿ

  ImageNetï¼Œ Qmnist

## 3.6 softmaxä»é›¶å®ç°

- [**å®ç°softmax**]ç”±ä¸‰ä¸ªæ­¥éª¤ç»„æˆï¼š

31. å¯¹æ¯ä¸ªé¡¹æ±‚å¹‚ï¼ˆä½¿ç”¨`exp`ï¼‰ï¼›

32. å¯¹æ¯ä¸€è¡Œæ±‚å’Œï¼ˆå°æ‰¹é‡ä¸­æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€è¡Œï¼‰ï¼Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„è§„èŒƒåŒ–å¸¸æ•°ï¼›

33. å°†æ¯ä¸€è¡Œé™¤ä»¥å…¶è§„èŒƒåŒ–å¸¸æ•°ï¼Œç¡®ä¿ç»“æœçš„å’Œä¸º1ã€‚

åœ¨æŸ¥çœ‹ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¿™ä¸ªè¡¨è¾¾å¼ï¼š

$\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}.$

```Python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # è¿™é‡Œåº”ç”¨äº†å¹¿æ’­æœºåˆ¶
```

```Python
X = torch.normal(0, 1, (2, 5))
X_prob = softmax(X)
X_prob, X_prob.sum(1)

(tensor([[0.1450, 0.2775, 0.4076, 0.1002, 0.0698],
         [0.1125, 0.2700, 0.3372, 0.2130, 0.0673]]),
 tensor([1., 1.]))
```

```Python
def accuracy(y_hat, y):  #@save
    """è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y  # y_hatä¸­ä¸yç›¸åŒçš„åˆ¤æ–­ï¼ˆæ­£ç¡®çš„ï¼‰
    return float(cmp.type(y.dtype).sum())  # åŠ èµ·æ¥çš„ä¸ªæ•°
  
  accuracy(y_hat, y)/len(y)  # å‡†ç¡®ç‡
```

```Python
def evaluate_accuracy(net, data_iter):  #@save
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]  # åˆ†ç±»æ­£ç¡®çš„æ ·æœ¬æ•°å’Œæ€»æ ·æœ¬æ•°ç›¸é™¤ï¼Œå°±æ˜¯æ¨¡å‹ç²¾åº¦
```

```Python
class Accumulator:  #@save
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

```Python
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]
```

```Python
class Animator:  #@save
    """åœ¨åŠ¨ç”»ä¸­ç»˜åˆ¶æ•°æ®"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # å¢é‡åœ°ç»˜åˆ¶å¤šæ¡çº¿
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # ä½¿ç”¨lambdaå‡½æ•°æ•è·å‚æ•°
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # å‘å›¾è¡¨ä¸­æ·»åŠ å¤šä¸ªæ•°æ®ç‚¹
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

```Python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

**1.åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥å®ç°äº†åŸºäºæ•°å­¦å®šä¹‰softmaxè¿ç®—çš„`softmax`å‡½æ•°ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿæç¤ºï¼šå°è¯•è®¡ç®— exp(50) çš„å¤§å°ã€‚**

  å¦‚æœç½‘ç»œå‚æ•°åˆå§‹åŒ–ä¸æ°å½“ï¼Œæˆ–è€…è¾“å…¥æœ‰æ•°å€¼è¾ƒå¤§çš„å™ªéŸ³ï¼ŒåŸºäºæ•°å­¦å®šä¹‰çš„softmaxè¿ç®—å¯èƒ½é€ æˆæº¢å‡ºï¼Œå› ä¸ºåˆ†æ¯è¦è®¡ç®—å¤šä¸ªexpçš„å€¼æ±‚å’Œï¼Œè§£å†³æ–¹æ³•å¯ä»¥å‚è€ƒlog_softmaxã€‚

**2.æœ¬èŠ‚ä¸­çš„å‡½æ•° `cross_entropy` æ˜¯æ ¹æ®äº¤å‰ç†µæŸå¤±å‡½æ•°çš„å®šä¹‰å®ç°çš„ã€‚è¿™ä¸ªå®ç°å¯èƒ½æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿæç¤ºï¼šè€ƒè™‘å¯¹æ•°çš„å€¼åŸŸã€‚**

  y^ä¸­è‹¥æŸè¡Œæœ€å¤§çš„å€¼ä¹Ÿæ¥è¿‘0çš„è¯ï¼Œlossçš„å€¼ä¹Ÿå¯èƒ½é€ æˆæº¢å‡ºï¼Œå¯ä»¥å‚è€ƒnlllosså’Œlog_softmaxä¸€èµ·ä½¿ç”¨ã€‚

**3.ä½ å¯ä»¥æƒ³åˆ°ä»€ä¹ˆè§£å†³æ–¹æ¡ˆæ¥è§£å†³ä¸Šè¿°ä¸¤ä¸ªé—®é¢˜ï¼Ÿ**

  å‚è€ƒnlllosså’Œlog_softmaxä¸€èµ·ä½¿ç”¨ã€‚

**4.è¿”å›æ¦‚ç‡æœ€å¤§çš„æ ‡ç­¾æ€»æ˜¯ä¸€ä¸ªå¥½ä¸»æ„å—ï¼Ÿä¾‹å¦‚ï¼ŒåŒ»ç–—è¯Šæ–­åœºæ™¯ä¸‹ä½ ä¼šè¿™æ ·åšå—ï¼Ÿ**

  è¿”å›æœ€å¤§æ¦‚ç‡æ ‡ç­¾ä¸æ€»æ˜¯ä¸ªå¥½ä¸»æ„ï¼ŒåŒ»ç–—è¯Šæ–­åœºæ™¯ä¹Ÿæœ‰å°½å¯èƒ½é¿å…å°æ¦‚ç‡äº‹ä»¶çš„å‘ç”Ÿã€‚

**5.å‡è®¾æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨softmaxå›å½’æ¥åŸºäºæŸäº›ç‰¹å¾é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚è¯æ±‡é‡å¤§å¯èƒ½ä¼šå¸¦æ¥å“ªäº›é—®é¢˜?**

  è¯æ±‡é‡å¤§æ„å‘³ç€classçš„ç±»åˆ«å¾ˆå¤šï¼Œè¿™å®¹æ˜“å¸¦æ¥ä¸¤ä¸ªé—®é¢˜ã€‚ä¸€æ˜¯é€ æˆè¾ƒå¤§çš„è®¡ç®—å‹åŠ›ï¼ŒäºŒæ˜¯æ‰€æœ‰çš„å•è¯æ‰€å¾—æ¦‚ç‡å®¹æ˜“å¾ˆæ¥è¿‘0ï¼Œå•è¯é—´æ¦‚ç‡å·®åˆ«ä¸å¤§ï¼Œå¾ˆéš¾åˆ¤æ–­åº”è¯¥è¾“å‡ºå“ªä¸ªç»“æœã€‚

## 3.7 ç®€æ´å®ç°

```Python
import torch
from torch import nn
from d2l import torch as d2l

# åŠ è½½æ•°æ®
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# åˆå§‹åŒ–æ¨¡å‹åŠå‚æ•°

net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

# å…¶å®è¿™é‡Œä¹Ÿå¯ä»¥ä¸åˆå§‹åŒ–å‚æ•°

# def init_weights(m):
#     if type(m) == nn.Linear:
#         nn.init.normal_(m.weight, std=0.01)
# net.apply(init_weights)

# æŸå¤±å‡½æ•°
loss = nn.CrossEntropyLoss(reduction='none')
# ä¼˜åŒ–ç®—æ³•
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
# è®­ç»ƒ
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 7.png)

# ch-4

## 4.1 å¤šå±‚æ„ŸçŸ¥æœº

é€šè¿‡åœ¨ç½‘ç»œä¸­åŠ å…¥ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚æ¥å…‹æœçº¿æ€§æ¨¡å‹çš„é™åˆ¶ï¼Œ ä½¿å…¶èƒ½å¤„ç†æ›´æ™®éçš„å‡½æ•°å…³ç³»ç±»å‹ã€‚ è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯å°†è®¸å¤šå…¨è¿æ¥å±‚å †å åœ¨ä¸€èµ·ã€‚ æ¯ä¸€å±‚éƒ½è¾“å‡ºåˆ°ä¸Šé¢çš„å±‚ï¼Œç›´åˆ°ç”Ÿæˆæœ€åçš„è¾“å‡ºã€‚ æˆ‘ä»¬å¯ä»¥æŠŠå‰ğ¿âˆ’1å±‚çœ‹ä½œè¡¨ç¤ºï¼ŒæŠŠæœ€åä¸€å±‚çœ‹ä½œçº¿æ€§é¢„æµ‹å™¨ã€‚ è¿™ç§æ¶æ„é€šå¸¸ç§°ä¸º*å¤šå±‚æ„ŸçŸ¥æœº*ï¼ˆmultilayer perceptronï¼‰ï¼Œé€šå¸¸ç¼©å†™ä¸º*MLP*ã€‚

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 8.png)

- Reluå‡½æ•°

  $\operatorname{ReLU}(x) = \max(x, 0).$

  ```Python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 9.png)

  å½“è¾“å…¥ä¸ºè´Ÿæ—¶ï¼ŒReLUå‡½æ•°çš„å¯¼æ•°ä¸º0ï¼Œè€Œå½“è¾“å…¥ä¸ºæ­£æ—¶ï¼ŒReLUå‡½æ•°çš„å¯¼æ•°ä¸º1ã€‚ æ³¨æ„ï¼Œå½“è¾“å…¥å€¼ç²¾ç¡®ç­‰äº0æ—¶ï¼ŒReLUå‡½æ•°ä¸å¯å¯¼ã€‚ åœ¨æ­¤æ—¶ï¼Œæˆ‘ä»¬é»˜è®¤ä½¿ç”¨å·¦ä¾§çš„å¯¼æ•°ï¼Œå³å½“è¾“å…¥ä¸º0æ—¶å¯¼æ•°ä¸º0ã€‚ æˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™ç§æƒ…å†µï¼Œå› ä¸ºè¾“å…¥å¯èƒ½æ°¸è¿œéƒ½ä¸ä¼šæ˜¯0ã€‚

***"å¦‚æœå¾®å¦™çš„è¾¹ç•Œæ¡ä»¶å¾ˆé‡è¦ï¼Œæˆ‘ä»¬å¾ˆå¯èƒ½æ˜¯åœ¨ç ”ç©¶æ•°å­¦è€Œéå·¥ç¨‹"***

  ```Python
d2l.plot(x.detach() ,x.grad.detach(), 'x', 'grad of relu', figsize=(5, 2.5))
```

  ![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 10.png)

- Sigmoidå‡½æ•°

                                                                    $\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.$

  ```Python
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

  ![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 11.png)

  sigmoidå‡½æ•°çš„å¯¼æ•°ä¸ºä¸‹é¢çš„å…¬å¼ï¼š

                                   $\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).$

  ```Python
# æ¸…é™¤ä»¥å‰çš„æ¢¯åº¦
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))
```

  ![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 12.png)

- tanhå‡½æ•°

                                                                  **$\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.$**

  ```Python
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

  ![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 13.png)

  tanhå‡½æ•°çš„å¯¼æ•°æ˜¯ï¼š

  $\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).$

  ```Python
# æ¸…é™¤ä»¥å‰çš„æ¢¯åº¦
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))
```

  ![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 14.png)

### ä¹ é¢˜

34. **è®¡ç®—pReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°ã€‚**

  $\operatorname{PRelu}(x) = \max(0, x) + a * \min(0, x)$

  å½“xï¼0æ—¶ï¼Œå¯¼æ•°ä¸º1ï¼›å½“xå°äº0æ—¶ï¼Œå€’æ•°ä¸ºaã€‚

35. **è¯æ˜ä¸€ä¸ªä»…ä½¿ç”¨ReLUï¼ˆæˆ–pReLUï¼‰çš„å¤šå±‚æ„ŸçŸ¥æœºæ„é€ äº†ä¸€ä¸ªè¿ç»­çš„åˆ†æ®µçº¿æ€§å‡½æ•°ã€‚**

  ReLUçš„ç¥ç»ç½‘ç»œå¯ä»¥ä¸¥æ ¼ç­‰äºä»»ä½•(æœ‰é™æ®µ)åˆ†æ®µçº¿æ€§å‡½æ•°ã€‚ä»»ä½•(æœ‰é™åŒºé—´ä¸Š)è¿ç»­å‡½æ•°ï¼Œæ€»å¯ä»¥åœ¨ä»»ç»™å®šçš„è¯¯å·®ä¸‹ï¼Œç”¨(æœ‰é™æ®µ)åˆ†æ®µçº¿æ€§å‡½æ•°è¿‘ä¼¼ã€‚å¯¹äºæœ‰é™ä¸ªé—´æ–­ç‚¹çš„å‡½æ•°ï¼Œå¯ä»¥åœ¨é—´æ–­ç‚¹é™„è¿‘æŒ–æ‰ä»»æ„å°çš„å°åŒºé—´ï¼Œåœ¨å¤–é¢ç”¨è¿ç»­å‡½æ•°è¿‘ä¼¼ã€‚

36. **è¯æ˜tanh(ğ‘¥)+1=2sigmoid(2ğ‘¥)ã€‚**

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 15.png)

  ä¸ºä»€ä¹ˆæ•°å€¼ä¸Šæ˜¯ä¸ç›¸ç­‰çš„ï¼Œç–‘æƒ‘

37. **å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªéçº¿æ€§å•å…ƒï¼Œå°†å®ƒä¸€æ¬¡åº”ç”¨äºä¸€ä¸ªå°æ‰¹é‡çš„æ•°æ®ã€‚è¿™ä¼šå¯¼è‡´ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Ÿ**

  æ•°æ®å¯èƒ½ä¼šè¢«å‰§çƒˆçš„æ‹‰ä¼¸æˆ–è€…å‹ç¼©ï¼Œå¯èƒ½ä¼šå¯¼è‡´åˆ†å¸ƒçš„åç§»ï¼Œå¹¶ä¸”ä¸åé¢çš„ç¥ç»å…ƒå¯¹æ¥åå¯èƒ½ä¼šæŸå¤±ä¸€å®šçš„ç‰¹å¾ã€‚

## 4.2 å¤šå±‚æ„ŸçŸ¥æœºä»é›¶å®ç°

```Python
import torch
from torch import nn
from d2l import torch as d2l

# åŠ è½½æ•°æ®
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
num_inputs, num_outputs, num_hiddens = 784, 10, 256
W1 = nn.Parameter(torch.randn(
    num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(
    num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))
params = [W1, b1, W2, b2]

# æ¿€æ´»å‡½æ•°ï¼šrelu
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
  
# æ¨¡å‹
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1)  # è¿™é‡Œâ€œ@â€ä»£è¡¨çŸ©é˜µä¹˜æ³•
    return (H@W2 + b2)
  
# æŸå¤±å‡½æ•°
loss = nn.CrossEntropyLoss(reduction='none')

# è®­ç»ƒ
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)

# æ¨¡å‹å±•ç¤º
d2l.predict_ch3(net, test_iter)
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 16.png)

### ç»ƒä¹ 

38. **åœ¨æ‰€æœ‰å…¶ä»–å‚æ•°ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ï¼Œæ›´æ”¹è¶…å‚æ•°`num_hiddens`çš„å€¼ï¼Œå¹¶æŸ¥çœ‹æ­¤è¶…å‚æ•°çš„å˜åŒ–å¯¹ç»“æœæœ‰ä½•å½±å“ã€‚ç¡®å®šæ­¤è¶…å‚æ•°çš„æœ€ä½³å€¼ã€‚**

  å®éªŒæ“ä½œä¸€ä¸‹

39. **å°è¯•æ·»åŠ æ›´å¤šçš„éšè—å±‚ï¼Œå¹¶æŸ¥çœ‹å®ƒå¯¹ç»“æœæœ‰ä½•å½±å“ã€‚**

  å®éªŒæ“ä½œ

40. **æ”¹å˜å­¦ä¹ é€Ÿç‡ä¼šå¦‚ä½•å½±å“ç»“æœï¼Ÿä¿æŒæ¨¡å‹æ¶æ„å’Œå…¶ä»–è¶…å‚æ•°ï¼ˆåŒ…æ‹¬è½®æ•°ï¼‰ä¸å˜ï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸ºå¤šå°‘ä¼šå¸¦æ¥æœ€å¥½çš„ç»“æœï¼Ÿ**

  å®éªŒæ“ä½œ

41. **é€šè¿‡å¯¹æ‰€æœ‰è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€è½®æ•°ã€éšè—å±‚æ•°ã€æ¯å±‚çš„éšè—å•å…ƒæ•°ï¼‰è¿›è¡Œè”åˆä¼˜åŒ–ï¼Œå¯ä»¥å¾—åˆ°çš„æœ€ä½³ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ**

  å®éªŒæ“ä½œ

42. **æè¿°ä¸ºä»€ä¹ˆæ¶‰åŠå¤šä¸ªè¶…å‚æ•°æ›´å…·æŒ‘æˆ˜æ€§ã€‚**

43. **å¦‚æœæƒ³è¦æ„å»ºå¤šä¸ªè¶…å‚æ•°çš„æœç´¢æ–¹æ³•ï¼Œè¯·æƒ³å‡ºä¸€ä¸ªèªæ˜çš„ç­–ç•¥ã€‚**

  å¯å‘å¼

## 4.3 å¤šå±‚æ„ŸçŸ¥æœºç®€æ´å®ç°

- è¿™ä¸ªåœ°æ–¹å»ºè®®å¢åŠ ä¸€ä¸ªnum_workçš„è‡ªå®šä¹‰é€‰é¡¹.

![b01a5010e778c9dac9ccab8be78ba67.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/b01a5010e778c9dac9ccab8be78ba67.png)

```Python
import torch
from torch import nn
from d2l import torch as d2l

# å»ºç«‹æ¨¡å‹
net = nn.Sequential(nn.Flatten(),
                    nn.Linear(784, 256),
                    nn.ReLU(),
                    nn.Linear(256, 10))

# æ­¤å¤„å³ä½¿ä¸åˆå§‹åŒ–å‚æ•°ï¼Œtorchä¹Ÿä¼šè‡ªå·±åˆå§‹åŒ–
def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights)

# æŸå¤±å‡½æ•° å­¦ä¹ ç‡ ä¼˜åŒ–å™¨
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=lr)

# åŠ è½½æ•°æ® è®­ç»ƒæ¨¡å‹
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 17.png)



## 4.4æ¨¡å‹é€‰æ‹©

- ***åªæœ‰å½“æ¨¡å‹çœŸæ­£å‘ç°äº†ä¸€ç§æ³›åŒ–æ¨¡å¼æ—¶ï¼Œæ‰ä¼šä½œå‡ºæœ‰æ•ˆçš„é¢„æµ‹ã€‚***

- ***å°†æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆçš„æ¯”åœ¨æ½œåœ¨åˆ†å¸ƒä¸­æ›´æ¥è¿‘çš„ç°è±¡ç§°ä¸ºè¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ï¼Œ ç”¨äºå¯¹æŠ—è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ç§°ä¸ºæ­£åˆ™åŒ–ï¼ˆregularizationï¼‰ã€‚***



44. **å¯è°ƒæ•´å‚æ•°çš„æ•°é‡ã€‚å½“å¯è°ƒæ•´å‚æ•°çš„æ•°é‡ï¼ˆæœ‰æ—¶ç§°ä¸º*è‡ªç”±åº¦*ï¼‰å¾ˆå¤§æ—¶ï¼Œæ¨¡å‹å¾€å¾€æ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚**

45. **å‚æ•°é‡‡ç”¨çš„å€¼ã€‚å½“æƒé‡çš„å–å€¼èŒƒå›´è¾ƒå¤§æ—¶ï¼Œæ¨¡å‹å¯èƒ½æ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚**

46. **è®­ç»ƒæ ·æœ¬çš„æ•°é‡ã€‚å³ä½¿æ¨¡å‹å¾ˆç®€å•ï¼Œä¹Ÿå¾ˆå®¹æ˜“è¿‡æ‹ŸåˆåªåŒ…å«ä¸€ä¸¤ä¸ªæ ·æœ¬çš„æ•°æ®é›†ã€‚è€Œè¿‡æ‹Ÿåˆä¸€ä¸ªæœ‰æ•°ç™¾ä¸‡ä¸ªæ ·æœ¬çš„æ•°æ®é›†åˆ™éœ€è¦ä¸€ä¸ªæå…¶çµæ´»çš„æ¨¡å‹ã€‚**

## 4.5 æƒé‡è¡°å‡

å®é™…ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡*æ­£åˆ™åŒ–å¸¸æ•°*$\lambda$æ¥æè¿°è¿™ç§æƒè¡¡ï¼Œè¿™æ˜¯ä¸€ä¸ªéè´Ÿè¶…å‚æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨éªŒè¯æ•°æ®æ‹Ÿåˆï¼š

$L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,$

å¯¹äº$\lambda = 0$ï¼Œæˆ‘ä»¬æ¢å¤äº†åŸæ¥çš„æŸå¤±å‡½æ•°ã€‚å¯¹äº$\lambda > 0$ï¼Œæˆ‘ä»¬é™åˆ¶$\| \mathbf{w} \|$çš„å¤§å°ã€‚

- ***æ­£åˆ™åŒ–æ˜¯å¤„ç†è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ–¹æ³•ï¼šåœ¨è®­ç»ƒé›†çš„æŸå¤±å‡½æ•°ä¸­åŠ å…¥æƒ©ç½šé¡¹ï¼Œä»¥é™ä½å­¦ä¹ åˆ°çš„æ¨¡å‹çš„å¤æ‚åº¦ã€‚***

- ***ä¿æŒæ¨¡å‹ç®€å•çš„ä¸€ä¸ªç‰¹åˆ«çš„é€‰æ‹©æ˜¯ä½¿ç”¨ğ¿2æƒ©ç½šçš„æƒé‡è¡°å‡ã€‚è¿™ä¼šå¯¼è‡´å­¦ä¹ ç®—æ³•æ›´æ–°æ­¥éª¤ä¸­çš„æƒé‡è¡°å‡ã€‚***

- ***æƒé‡è¡°å‡åŠŸèƒ½åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä¼˜åŒ–å™¨ä¸­æä¾›ã€‚***

- ***åœ¨åŒä¸€è®­ç»ƒä»£ç å®ç°ä¸­ï¼Œä¸åŒçš„å‚æ•°é›†å¯ä»¥æœ‰ä¸åŒçš„æ›´æ–°è¡Œä¸ºã€‚***

## 4.6 æš‚é€€æ³• Dropout

*åœ¨ Pytorch ä¸­ç»§æ‰¿ torch.nn.Module å, è¦æ‰§è¡Œ self(è‡ªå·±, self).__init__( ) æ‰èƒ½é€šè¿‡è°ƒç”¨å®ä¾‹åŒ–çš„å¯¹è±¡çš„ å®ä¾‹åŒ–å¯¹è±¡( ) çš„æ–¹æ³•è°ƒç”¨ forward å‡½æ•°*

**1.å¦‚æœæ›´æ”¹ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„æš‚é€€æ³•æ¦‚ç‡ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿå…·ä½“åœ°è¯´ï¼Œå¦‚æœäº¤æ¢è¿™ä¸¤ä¸ªå±‚ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿè®¾è®¡ä¸€ä¸ªå®éªŒæ¥å›ç­”è¿™äº›é—®é¢˜ï¼Œå®šé‡æè¿°ä½ çš„ç»“æœï¼Œå¹¶æ€»ç»“å®šæ€§çš„ç»“è®ºã€‚**

åœ¨å®éªŒä¸­å‘ç°ï¼Œåœ¨è¯¯å·®å…è®¸çš„èŒƒå›´å†…å…¶å®å·®åˆ«ä¸å¤§ï¼Œç¬¬ä¸€å±‚æš‚é€€æ³•æ¦‚ç‡å¤§ç‚¹ï¼Œç¬¬äºŒå±‚æš‚é€€æ³•æ¦‚ç‡å°ç‚¹æ•ˆæœä¼šç•¥å¾®ä¼˜ä¸€ç‚¹ã€‚çŒœæµ‹åŸå› å¯èƒ½æœ‰ï¼šâ‘ å‰é¢å±‚æŠ½å–çš„æ˜¯æ¯”è¾ƒåº•å±‚çš„è¯­ä¹‰ä¿¡æ¯ï¼Œæœ‰è¾ƒå¤šçš„æ— ç”¨ä¿¡æ¯å†—ä½™é€šè¿‡å¼ºç¥ç»å…ƒï¼Œä»è€Œä½¿å¾—ç½‘ç»œè®°ä½è¿™äº›å†—ä½™ä¿¡æ¯è€Œå­¦ä¸åˆ°å…³é”®ä¿¡æ¯(å¯¼è‡´è¿‡æ‹Ÿåˆ)ï¼Œç”¨è¾ƒå¤§Dropoutè¾ƒå¥½ï¼Œåé¢å±‚ä¸»ç®¡é«˜å±‚æŠ½è±¡è¯­ä¹‰ä¿¡æ¯ï¼Œè¾ƒä¸ºå…³é”®ï¼Œæ˜¯æŠŠæ¡è¯†åˆ«æ•´ä½“çš„å…³é”®éƒ¨åˆ†ï¼Œç”¨è¾ƒå°Dropoutè¾ƒå¥½ï¼›â‘¡ä¸€èˆ¬å‰é¢å±‚å…¨è¿æ¥æ•°ç›®æ¯”è¾ƒå¤§ï¼ŒæŠ½å–ä¿¡æ¯é‡æ¯”è¾ƒå¤šï¼Œè‡ªç„¶å¸¦æ¥å†—ä½™ä¿¡æ¯è¾ƒå¤šï¼Œé‚£ä¹ˆå¤šçš„æ•°ç›®è¿æ¥ï¼Œå¯ä»¥é€šè¿‡è¾ƒå¤§Dropoutä¸¢å¼ƒæ‰å¤§éƒ¨åˆ†çš„å…¨è¿æ¥ï¼Œå®é™…ä¸Šå› ä¸ºåŸºæ•°å¤§ï¼Œå‰©ä¸‹çš„æ²¡æœ‰ç½®0çš„è¿æ¥æ•°ç›®è¿˜æ˜¯å¾ˆå¤šçš„ã€‚

å…·ä½“å®éªŒè¿‡ç¨‹å¯çœ‹ä¸‹é¢è¿™ä¸ªé“¾æ¥ï¼š[å‚è€ƒé“¾æ¥](https://links.jianshu.com/go?to=https%3A%2F%2Fgist.github.com%2Frandomgeek78%2F89ec0242ac756db88d14e2d1982abae9)

**2.å¢åŠ è®­ç»ƒè½®æ•°ï¼Œå¹¶å°†ä½¿ç”¨æš‚é€€æ³•å’Œä¸ä½¿ç”¨æš‚é€€æ³•æ—¶è·å¾—çš„ç»“æœè¿›è¡Œæ¯”è¾ƒã€‚**

éšç€è®­ç»ƒè½®æ•°çš„å¢åŠ ï¼Œä½¿ç”¨Dropoutçš„losså°†ä¼šé€æ¸è¶‹äºä¸€ä¸ªç¨³å®šå€¼å¹¶æœ‰å°å¹…åº¦çš„æ³¢åŠ¨ï¼Œä¸ä½¿ç”¨Dropoutçš„losså°†ä¼šä¸€ç›´å°å¹…åº¦çš„ä¸‹é™ï¼Œæœ€ååœ¨è®­ç»ƒé›†ä¸Šï¼Œå¯ä»¥çœ‹åˆ°ä¸ä½¿ç”¨Dropoutçš„ä¼šæ¯”ä½¿ç”¨Dropoutçš„losså°ï¼Œä½†æ˜¯åœ¨æµ‹è¯•é›†ä¸Šé¢ï¼Œå‰è€…çš„å‡†ç¡®ç‡ä¼šæ¯”åè€…å°ã€‚è¿™æ˜æ˜¾å‘ç”Ÿäº†è¿‡æ‹Ÿåˆã€‚

**3.å½“åº”ç”¨æˆ–ä¸åº”ç”¨æš‚é€€æ³•æ—¶ï¼Œæ¯ä¸ªéšè—å±‚ä¸­æ¿€æ´»å€¼çš„æ–¹å·®æ˜¯å¤šå°‘ï¼Ÿç»˜åˆ¶ä¸€ä¸ªæ›²çº¿å›¾ï¼Œä»¥æ˜¾ç¤ºè¿™ä¸¤ä¸ªæ¨¡å‹çš„æ¯ä¸ªéšè—å±‚ä¸­æ¿€æ´»å€¼çš„æ–¹å·®æ˜¯å¦‚ä½•éšæ—¶é—´å˜åŒ–çš„ã€‚**

Dropoutä¼šå‡å°‘å¯¹å¼ºç¥ç»å…ƒçš„ä¾èµ–ï¼Œå› æ­¤å¼ºç¥ç»å…ƒçš„æ¿€æ´»å€¼ä¼šé™ä½(åœ¨æœ¬è½®è®­ç»ƒä¸­è¢«ç½®é›¶ï¼Œæ¢¯åº¦æ›´æ–°ä¸ä¼šæ›´æ–°)ï¼Œä½†æ¿€æ´»å¼±ç¥ç»å…ƒçš„å€¼ä¼šå¢åŠ ï¼Œè¿™æ ·å¼ºã€å¼±ç¥ç»å…ƒä¹‹é—´æ–¹å·®å˜å°äº†ã€‚

**4.ä¸ºä»€ä¹ˆåœ¨æµ‹è¯•æ—¶é€šå¸¸ä¸ä½¿ç”¨æš‚é€€æ³•ï¼Ÿ**

å› ä¸ºæµ‹è¯•çš„æ—¶å€™ç½‘ç»œæ¨¡å‹æ˜¯å›ºå®šçš„ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ç§å·²çŸ¥çš„å…ˆéªŒï¼Œé‚£å½“ç„¶æ˜¯ä¸ç”¨DropoutåŠ å…¥å™ªå£°çš„ã€‚

**5.ä»¥æœ¬èŠ‚ä¸­çš„æ¨¡å‹ä¸ºä¾‹ï¼Œæ¯”è¾ƒä½¿ç”¨æš‚é€€æ³•å’Œæƒé‡è¡°å‡çš„æ•ˆæœã€‚å¦‚æœåŒæ—¶ä½¿ç”¨æš‚é€€æ³•å’Œæƒé‡è¡°å‡ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿç»“æœæ˜¯ç´¯åŠ çš„å—ï¼Ÿæ”¶ç›Šæ˜¯å¦å‡å°‘ï¼ˆæˆ–è€…è¯´æ›´ç³Ÿï¼‰ï¼Ÿå®ƒä»¬äº’ç›¸æŠµæ¶ˆäº†å—ï¼Ÿ**

ç»“æœä¼šæ›´å¥½ï¼Œå› ä¸ºå®ƒä»¬é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ€è€ƒç‚¹ä¸åŒï¼Œæš‚é€€æ³•æ€è€ƒç‚¹å¼•å…¥ä¸€å®šçš„å™ªå£°ï¼Œå¢åŠ æ¨¡å‹å¯¹è¾“å…¥æ•°æ®çš„æ‰°åŠ¨é²æ£’ï¼Œä»è€Œå¢å¼ºæ³›åŒ–æ€§ï¼›æƒé‡è¡°å‡åœ¨äºçº¦æŸæ¨¡å‹å‚æ•°æ¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚

**6.å¦‚æœæˆ‘ä»¬å°†æš‚é€€æ³•åº”ç”¨åˆ°æƒé‡çŸ©é˜µçš„å„ä¸ªæƒé‡ï¼Œè€Œä¸æ˜¯æ¿€æ´»å€¼ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ**

æˆ‘æ„Ÿè§‰è¿™ä¸ªé¢˜çš„é¢˜ç›®æ„æ€åº”è¯¥æ˜¯ï¼šâ€œDropoutåº”è¯¥æ”¾åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰è¿˜æ˜¯ä¹‹åâ€ã€‚

ç›®å‰ä»£ç ä¸Šé¢Dropoutéƒ½æ˜¯ç”¨åœ¨æ¿€æ´»å‡½æ•°ä¹‹åçš„ï¼Œå› ä¸ºæœ‰äº›æ¿€æ´»å‡½æ•°åœ¨0å¤„çš„æ¢¯åº¦å¹¶ä¸æ˜¯0ï¼Œè¿™æ ·çš„è¯å…ˆç”¨Dropoutç½®é›¶åå†ç»è¿‡æ¿€æ´»å‡½æ•°ï¼Œè¿™æ ·é‚£ä¸ªç¥ç»å…ƒå°±ä¼šæœ‰æ¢¯åº¦äº†(æˆ‘ä»¬ä¸€èˆ¬æŠŠç¥ç»å±‚+Batchnormå±‚+æ¿€æ´»å‡½æ•°ä¸ºä¸€å±‚ï¼Œæ‰€ä»¥æŠŠDropoutæ”¾åœ¨æ¿€æ´»å‡½æ•°ä¹‹åä¹Ÿæ˜¯æœ‰é“ç†çš„)ï¼Œèµ·ä¸åˆ°äº†Dropoutçš„ä½œç”¨äº†ï¼Œåº”è¯¥åœ¨æ¿€æ´»å‡½æ•°ä¹‹åç”¨Dropoutã€‚

**7.å‘æ˜å¦ä¸€ç§ç”¨äºåœ¨æ¯ä¸€å±‚æ³¨å…¥éšæœºå™ªå£°çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯ä¸åŒäºæ ‡å‡†çš„æš‚é€€æ³•æŠ€æœ¯ã€‚å°è¯•å¼€å‘ä¸€ç§åœ¨Fashion-MNISTæ•°æ®é›†ï¼ˆå¯¹äºå›ºå®šæ¶æ„ï¼‰ä¸Šæ€§èƒ½ä¼˜äºæš‚é€€æ³•çš„æ–¹æ³•ã€‚**

å¯ä»¥å°è¯•ä¸‹æŠŠæ¯ä¸€å±‚ç®—å®ƒçš„æ–¹å·®å’Œå‡å€¼ï¼Œç„¶åä½œä¸€ä¸ªé«˜æ–¯å™ªå£°å åŠ ä¸Šå»ã€‚

å‚è€ƒ(å‚è€ƒ)

[ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹å‚è€ƒç­”æ¡ˆ(ç¬¬äºŒç‰ˆ)-ç¬¬å››ç« ](https://www.jianshu.com/p/a8cab5a45b2e)

## 4.8 æ¢¯åº¦æ¶ˆå¤±

æ›¾ç»sigmoidå‡½æ•°$1/(1 + \exp(-x))$ï¼ˆ :numref:`sec_mlp`æåˆ°è¿‡ï¼‰å¾ˆæµè¡Œï¼Œå› ä¸ºå®ƒç±»ä¼¼äºé˜ˆå€¼å‡½æ•°ã€‚ç”±äºæ—©æœŸçš„äººå·¥ç¥ç»ç½‘ç»œå—åˆ°ç”Ÿç‰©ç¥ç»ç½‘ç»œçš„å¯å‘ï¼Œç¥ç»å…ƒè¦ä¹ˆå®Œå…¨æ¿€æ´»è¦ä¹ˆå®Œå…¨ä¸æ¿€æ´»ï¼ˆå°±åƒç”Ÿç‰©ç¥ç»å…ƒï¼‰çš„æƒ³æ³•å¾ˆæœ‰å¸å¼•åŠ›ã€‚ç„¶è€Œï¼Œå®ƒå´æ˜¯å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„ä¸€ä¸ªå¸¸è§çš„åŸå› ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹sigmoidå‡½æ•°ä¸ºä»€ä¹ˆä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚

```Python
%matplotlib inline
import torch
from d2l import torch as d2l

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)
y.backward(torch.ones_like(x))

d2l.plot(x.detach().numpy(), [y.detach().numpy(), x.grad.numpy()],
         legend=['sigmoid', 'gradient'], figsize=(4.5, 2.5))
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 18.png)

***å½“sigmoidå‡½æ•°çš„è¾“å…¥å¾ˆå¤§æˆ–æ˜¯å¾ˆå°æ—¶ï¼Œå®ƒçš„æ¢¯åº¦éƒ½ä¼šæ¶ˆå¤±ã€‚***

# ch-5

## 5.1 å±‚å’Œå—

- è‡ªå®šä¹‰å—

```Python
class MLP(nn.Module):
    # ç”¨æ¨¡å‹å‚æ•°å£°æ˜å±‚ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å£°æ˜ä¸¤ä¸ªå…¨è¿æ¥çš„å±‚
    def __init__(self):
        # è°ƒç”¨MLPçš„çˆ¶ç±»Moduleçš„æ„é€ å‡½æ•°æ¥æ‰§è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚
        # è¿™æ ·ï¼Œåœ¨ç±»å®ä¾‹åŒ–æ—¶ä¹Ÿå¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°å‚æ•°ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°paramsï¼ˆç¨åå°†ä»‹ç»ï¼‰
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # éšè—å±‚
        self.out = nn.Linear(256, 10)  # è¾“å‡ºå±‚

    # å®šä¹‰æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥Xè¿”å›æ‰€éœ€çš„æ¨¡å‹è¾“å‡º
    def forward(self, X):
        # æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ReLUçš„å‡½æ•°ç‰ˆæœ¬ï¼Œå…¶åœ¨nn.functionalæ¨¡å—ä¸­å®šä¹‰ã€‚
        return self.out(F.relu(self.hidden(X)))
```

- é¡ºåºå¿«

```Python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # è¿™é‡Œï¼Œmoduleæ˜¯Moduleå­ç±»çš„ä¸€ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬æŠŠå®ƒä¿å­˜åœ¨'Module'ç±»çš„æˆå‘˜
            # å˜é‡_modulesä¸­ã€‚_moduleçš„ç±»å‹æ˜¯OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDictä¿è¯äº†æŒ‰ç…§æˆå‘˜æ·»åŠ çš„é¡ºåºéå†å®ƒä»¬
        for block in self._modules.values():
            X = block(X)
        return X
```

- **åœ¨å‰å‘ä¼ æ’­å‡½æ•°ä¸­æ‰§è¡Œä»£ç **

```Python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸è®¡ç®—æ¢¯åº¦çš„éšæœºæƒé‡å‚æ•°ã€‚å› æ­¤å…¶åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸å˜
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # ä½¿ç”¨åˆ›å»ºçš„å¸¸é‡å‚æ•°ä»¥åŠreluå’Œmmå‡½æ•°
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # å¤ç”¨å…¨è¿æ¥å±‚ã€‚è¿™ç›¸å½“äºä¸¤ä¸ªå…¨è¿æ¥å±‚å…±äº«å‚æ•°
        X = self.linear(X)
        # æ§åˆ¶æµ
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

- æ··æ­ã€ç»„åˆå—

```Python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

### ç»ƒä¹ 

47. ***å¦‚æœå°†`MySequential`ä¸­å­˜å‚¨å—çš„æ–¹å¼æ›´æ”¹ä¸ºPythonåˆ—è¡¨ï¼Œä¼šå‡ºç°ä»€ä¹ˆæ ·çš„é—®é¢˜ï¼Ÿ***

  æŠ¥é”™ï¼Œè¯•è¿‡äº†ï¼Œä½†ä¸çŸ¥é“ä¸ºå•¥ã€‚

48. ***å®ç°ä¸€ä¸ªå—ï¼Œå®ƒä»¥ä¸¤ä¸ªå—ä¸ºå‚æ•°ï¼Œä¾‹å¦‚`net1`å’Œ`net2`ï¼Œå¹¶è¿”å›å‰å‘ä¼ æ’­ä¸­ä¸¤ä¸ªç½‘ç»œçš„ä¸²è”è¾“å‡ºã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºå¹³è¡Œå—ã€‚***

  ```Python
class myMLP(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for block in args:
            self._modules[block] = block
    
    def forward(self, X):
        outputs = []
        for block in self._modules.values():
            outputs.append(block)
        return outputs
net = myMLP(nn.Linear(20, 256), nn.Sequential(nn.Linear(20, 128), nn.ReLU(), nn.Linear(128, 256)))
net(X)

[Linear(in_features=20, out_features=256, bias=True),
 Sequential(
   (0): Linear(in_features=20, out_features=128, bias=True)
   (1): ReLU()
   (2): Linear(in_features=128, out_features=256, bias=True)
 )]
  ```

  è¯¾åé¢˜å‚è€ƒ

  ```Python
class Bigkuai(nn.Module):
    def __init__(self):
        super().__init__()
        self.net1=nn.Sequential(nn.Linear(20,66),nn.ReLU())
        self.net2=nn.Sequential(nn.Linear(20,33),nn.ReLU())
        
    def forward(self,X):
        x1= self.net1(X)
        x2= self.net2(X)
        x3= torch.cat((x1,x2),1)
        Twao=nn.Sequential(nn.Linear(x3.shape[1],30),nn.ReLU())
        return Twao(x3)

X = torch.rand(2,20)
k=Bigkuai()
print(k(X))
k(X).shape

tensor([[0.0079, 0.1055, 0.0116, 0.0000, 0.1243, 0.0000, 0.0000, 0.0000, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0519, 0.1405, 0.1829, 0.0000, 0.3230, 0.0000,
         0.0000, 0.0000, 0.0000, 0.0000, 0.0099, 0.3829, 0.0150, 0.2154, 0.0000,
         0.0000, 0.3000, 0.0210],
        [0.0000, 0.0799, 0.0315, 0.0000, 0.0242, 0.0000, 0.0000, 0.0000, 0.0222,
         0.0000, 0.0674, 0.0000, 0.0000, 0.1620, 0.0587, 0.0000, 0.3102, 0.0000,
         0.0000, 0.0050, 0.0000, 0.0000, 0.0217, 0.2538, 0.0432, 0.1730, 0.0000,
         0.0000, 0.1923, 0.1408]], grad_fn=<ReluBackward0>)
torch.Size([2, 30])
  ```

  ```Python
class MyParallel(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            self._modules[str(idx)] = module

    def forward(self, X):
        return torch.concat((self._modules['0'](X), self._modules['1'](X)),1)

X = torch.rand(2, 20)
n1 = nn.Linear(20, 256)
n2 = nn.Linear(20, 128)
p = MyParallel(n1,n2)
print(p(X).shape, p(X))
print(p)
  ```

49. ***å‡è®¾æˆ‘ä»¬æƒ³è¦è¿æ¥åŒä¸€ç½‘ç»œçš„å¤šä¸ªå®ä¾‹ã€‚å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°ç”ŸæˆåŒä¸€ä¸ªå—çš„å¤šä¸ªå®ä¾‹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºæ›´å¤§çš„ç½‘ç»œã€‚***

  ```Python
class Factory(nn.Module):
    def __init__(self, net, ins, outs, k):
        super().__init__()
        for idx in range(k):
            self._modules[str(idx)] = net(ins, outs)

    def forward(self, X):
        res = list()
        for idx in range(len(self._modules)):
            res.append(self._modules[str(idx)](X))
        return torch.concat(res, 1)

X = torch.rand(2, 20)
f = Factory(nn.Linear, 20, 256, 3)
print(f(X).shape, f(X))
print(f)

torch.Size([2, 768]) tensor([[ 0.3325, -0.7102,  0.8746,  ...,  0.2731,  0.5325, -0.2417],
        [-0.0586, -0.8537,  0.7218,  ...,  0.4606,  0.5317, -0.1221]],
       grad_fn=<CatBackward0>)
Factory(
  (0): Linear(in_features=20, out_features=256, bias=True)
  (1): Linear(in_features=20, out_features=256, bias=True)
  (2): Linear(in_features=20, out_features=256, bias=True)
)
  ```

## 5.2 å‚æ•°ç®¡ç†

- å‚æ•°åˆå§‹åŒ–æ–¹æ³•

```Python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
```

- ä¹Ÿå¯ä»¥å°†å‚æ•°åˆå§‹åŒ–ä¸ºå¸¸æ•°

```Python
def init_constant(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 1)
        nn.init.zeros_(m.bias)
net.apply(init_constant)
net[0].weight.data[0], net[0].bias.data[0]
```

- å¯¹æŸäº›å—åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•

```Python
def init_xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(init_xavier)
net[2].apply(init_42)
print(net[0].weight.data[0])
print(net[2].weight.data)
```

### ç»ƒä¹ 

50. **ä½¿ç”¨ :numref:`sec_model_construction` ä¸­å®šä¹‰çš„`FancyMLP`æ¨¡å‹ï¼Œè®¿é—®å„ä¸ªå±‚çš„å‚æ•°ã€‚**

  ```Python
from torch.nn import functional as F
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸è®¡ç®—æ¢¯åº¦çš„éšæœºæƒé‡å‚æ•°ã€‚å› æ­¤å…¶åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸å˜
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # ä½¿ç”¨åˆ›å»ºçš„å¸¸é‡å‚æ•°ä»¥åŠreluå’Œmmå‡½æ•°
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # å¤ç”¨å…¨è¿æ¥å±‚ã€‚è¿™ç›¸å½“äºä¸¤ä¸ªå…¨è¿æ¥å±‚å…±äº«å‚æ•°
        X = self.linear(X)
        # æ§åˆ¶æµ
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()

X = torch.rand(2, 20)
net = FixedHiddenMLP()
net(X)
net.state_dict()
  ```

51. **æŸ¥çœ‹åˆå§‹åŒ–æ¨¡å—æ–‡æ¡£ä»¥äº†è§£ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•ã€‚**

52. **æ„å»ºåŒ…å«å…±äº«å‚æ•°å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºå¹¶å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè§‚å¯Ÿæ¨¡å‹å„å±‚çš„å‚æ•°å’Œæ¢¯åº¦ã€‚**

53. **ä¸ºä»€ä¹ˆå…±äº«å‚æ•°æ˜¯ä¸ªå¥½ä¸»æ„ï¼Ÿ**

  å…±äº«å‚æ•°é€šå¸¸å¯ä»¥èŠ‚çœå†…å­˜ï¼Œå¹¶åœ¨ä»¥ä¸‹æ–¹é¢å…·æœ‰ç‰¹å®šçš„å¥½å¤„ï¼š

  - å¯¹äºå›¾åƒè¯†åˆ«ä¸­çš„CNNï¼Œå…±äº«å‚æ•°ä½¿ç½‘ç»œèƒ½å¤Ÿåœ¨å›¾åƒä¸­çš„ä»»ä½•åœ°æ–¹è€Œä¸æ˜¯ä»…åœ¨æŸä¸ªåŒºåŸŸä¸­æŸ¥æ‰¾ç»™å®šçš„åŠŸèƒ½ã€‚

  - å¯¹äºRNNï¼Œå®ƒåœ¨åºåˆ—çš„å„ä¸ªæ—¶é—´æ­¥ä¹‹é—´å…±äº«å‚æ•°ï¼Œå› æ­¤å¯ä»¥å¾ˆå¥½åœ°æ¨å¹¿åˆ°ä¸åŒåºåˆ—é•¿åº¦çš„ç¤ºä¾‹ã€‚

  - å¯¹äºè‡ªåŠ¨ç¼–ç å™¨ï¼Œç¼–ç å™¨å’Œè§£ç å™¨å…±äº«å‚æ•°ã€‚ åœ¨å…·æœ‰çº¿æ€§æ¿€æ´»çš„å•å±‚è‡ªåŠ¨ç¼–ç å™¨ä¸­ï¼Œå…±äº«æƒé‡ä¼šåœ¨æƒé‡çŸ©é˜µçš„ä¸åŒéšè—å±‚ä¹‹é—´å¼ºåˆ¶æ­£äº¤ã€‚

## 5.4 è‡ªå®šä¹‰å±‚

- ä¸å¸¦å‚æ•°çš„å±‚

```Python
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

- å¸¦å‚æ•°çš„å±‚

```Python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

### ç»ƒä¹ 

54. **è®¾è®¡ä¸€ä¸ªæ¥å—è¾“å…¥å¹¶è®¡ç®—å¼ é‡é™ç»´çš„å±‚ï¼Œå®ƒè¿”å›$y_k = \sum_{i, j} W_{ijk} x_i x_j$ã€‚**

  æˆ‘çœ‹ä¸æ‡‚è¿™ä¸ªå…¬å¼ã€‚å‘œå‘œå‘œ

55. **è®¾è®¡ä¸€ä¸ªè¿”å›è¾“å…¥æ•°æ®çš„å‚…ç«‹å¶ç³»æ•°å‰åŠéƒ¨åˆ†çš„å±‚ã€‚**

  åˆ«è¯´äº† è¿™ä¸ªä¹Ÿä¸æ‡‚

## 5.5 è¯»å†™æ–‡ä»¶

- æ•´ä¸€ä¸ªå°æ¨¡å‹

```Python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
Y
```

- å°†æ¨¡å‹çš„å‚æ•°å­˜å‚¨åœ¨ä¸€ä¸ªå«åšâ€œmlp.paramsâ€çš„æ–‡ä»¶ä¸­

```Python
torch.save(net.state_dict(), 'mlp.params')
```

- è¯»å–æ¨¡å‹

```Python
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval(), clone(X)


(MLP(
   (hidden): Linear(in_features=20, out_features=256, bias=True)
   (output): Linear(in_features=256, out_features=10, bias=True)
 ),
 tensor([[ 0.7277,  0.3096, -0.0589,  0.2266,  0.1683, -0.1756, -0.2814, -0.3548,
          -0.2932,  0.3911],
         [ 0.2269, -0.0754, -0.1885, -0.0034, -0.0981,  0.1551,  0.0071, -0.0637,
          -0.1451,  0.0316]], grad_fn=<AddmmBackward0>))
```

### å°ç»“

- `save`å’Œ`load`å‡½æ•°å¯ç”¨äºå¼ é‡å¯¹è±¡çš„æ–‡ä»¶è¯»å†™ã€‚

- æˆ‘ä»¬å¯ä»¥é€šè¿‡å‚æ•°å­—å…¸ä¿å­˜å’ŒåŠ è½½ç½‘ç»œçš„å…¨éƒ¨å‚æ•°ã€‚

- ä¿å­˜æ¶æ„å¿…é¡»åœ¨ä»£ç ä¸­å®Œæˆï¼Œè€Œä¸æ˜¯åœ¨å‚æ•°ä¸­å®Œæˆã€‚

## 5.6 GPU

```Python
!nvidia-smi

Sat Mar  4 11:12:36 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 528.49       Driver Version: 528.49       CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
| N/A   33C    P8    13W /  94W |   1079MiB /  6144MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      2244    C+G   C:\Windows\explorer.exe         N/A      |
|    0   N/A  N/A      2492    C+G   ...y\ShellExperienceHost.exe    N/A      |
|    0   N/A  N/A      2968    C+G   ...8wekyb3d8bbwe\Cortana.exe    N/A      |
|    0   N/A  N/A      3420    C+G   ...2gh52qy24etm\Nahimic3.exe    N/A      |
|    0   N/A  N/A      3504    C+G   ...artMenuExperienceHost.exe    N/A      |
|    0   N/A  N/A      3528    C+G   ...587.57\msedgewebview2.exe    N/A      |
|    0   N/A  N/A      5444    C+G   ...2txyewy\TextInputHost.exe    N/A      |
|    0   N/A  N/A      7792    C+G   ...7pnf6hceqser\snipaste.exe    N/A      |
|    0   N/A  N/A      9108    C+G   ...cw5n1h2txyewy\LockApp.exe    N/A      |
|    0   N/A  N/A     10108    C+G   ...me\Application\chrome.exe    N/A      |
|    0   N/A  N/A     18992    C+G   ...perience\NVIDIA Share.exe    N/A      |
|    0   N/A  N/A     20012    C+G   ...oft\OneDrive\OneDrive.exe    N/A      |
|    0   N/A  N/A     20236    C+G   ...8bbwe\WindowsTerminal.exe    N/A      |
|    0   N/A  N/A     20824    C+G   ...d\runtime\WeChatAppEx.exe    N/A      |
|    0   N/A  N/A     21260    C+G   ...e\PhoneExperienceHost.exe    N/A      |
|    0   N/A  N/A     21724    C+G   ..._dt26b99r8h8gj\RtkUWP.exe    N/A      |
|    0   N/A  N/A     24524    C+G   ...8wekyb3d8bbwe\msteams.exe    N/A      |
|    0   N/A  N/A     24968    C+G   ...n1h2txyewy\SearchHost.exe    N/A      |
|    0   N/A  N/A     25212    C+G   ...CloudMusic\cloudmusic.exe    N/A      |
|    0   N/A  N/A     25380    C+G   ...perience\NVIDIA Share.exe    N/A      |
|    0   N/A  N/A     25460    C+G   D:\flowus\FlowUs.exe            N/A      |
+-----------------------------------------------------------------------------+
```

# ch-6

## 6.2 å›¾åƒå·ç§¯

- è¾“å…¥æ˜¯é«˜åº¦ä¸º3ã€å®½åº¦ä¸º3çš„äºŒç»´å¼ é‡ï¼ˆå³å½¢çŠ¶ä¸º3Ã—3ï¼‰ã€‚å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦éƒ½æ˜¯2ï¼Œè€Œå·ç§¯æ ¸çª—å£ï¼ˆæˆ–å·ç§¯çª—å£ï¼‰çš„å½¢çŠ¶ç”±å†…æ ¸çš„é«˜åº¦å’Œå®½åº¦å†³å®šï¼ˆå³2Ã—2ï¼‰ã€‚

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 19.png)

- åŸºäºå®šä¹‰çš„`corr2d`å‡½æ•°[**å®ç°äºŒç»´å·ç§¯å±‚**]ã€‚åœ¨`__init__`æ„é€ å‡½æ•°ä¸­ï¼Œå°†`weight`å’Œ`bias`å£°æ˜ä¸ºä¸¤ä¸ªæ¨¡å‹å‚æ•°ã€‚å‰å‘ä¼ æ’­å‡½æ•°è°ƒç”¨`corr2d`å‡½æ•°å¹¶æ·»åŠ åç½®ã€‚

```Python
def corr2d(X, K):  #@save
    """è®¡ç®—äºŒç»´äº’ç›¸å…³è¿ç®—"""
    h, w = K.shape
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
    return Y

class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
        self.bias = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        return corr2d(x, self.weight) + self.bias

net = Conv2D((3,3))
net(X)
```

```Python
# æ„é€ ä¸€ä¸ªäºŒç»´å·ç§¯å±‚ï¼Œå®ƒå…·æœ‰1ä¸ªè¾“å‡ºé€šé“å’Œå½¢çŠ¶ä¸ºï¼ˆ1ï¼Œ2ï¼‰çš„å·ç§¯æ ¸
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)

# è¿™ä¸ªäºŒç»´å·ç§¯å±‚ä½¿ç”¨å››ç»´è¾“å…¥å’Œè¾“å‡ºæ ¼å¼ï¼ˆæ‰¹é‡å¤§å°ã€é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼‰ï¼Œ
# å…¶ä¸­æ‰¹é‡å¤§å°å’Œé€šé“æ•°éƒ½ä¸º1
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
lr = 3e-2  # å­¦ä¹ ç‡

for i in range(10):
    Y_hat = conv2d(X)
    l = (Y_hat - Y) ** 2
    conv2d.zero_grad()
    l.sum().backward()
    # è¿­ä»£å·ç§¯æ ¸
    conv2d.weight.data[:] -= lr * conv2d.weight.grad
    if (i + 1) % 2 == 0:
        print(f'epoch {i+1}, loss {l.sum():.3f}')
```

- ***åœ¨å·ç§¯è¿‡ç¨‹ä¸­ï¼Œæ˜¯åœ¨å­¦ä¹ `kernel_size`çš„å‚æ•°ã€‚ï¼ˆå¯¹`kernel_size`å†…çš„å‚æ•°æ¢¯åº¦ä¸‹é™ï¼Œæ›´æ–°ï¼‰***

### ç»ƒä¹ 

56. æ„å»ºä¸€ä¸ªå…·æœ‰å¯¹è§’çº¿è¾¹ç¼˜çš„å›¾åƒ`X`ã€‚

  57. å¦‚æœå°†æœ¬èŠ‚ä¸­ä¸¾ä¾‹çš„å·ç§¯æ ¸`K`åº”ç”¨äº`X`ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆæƒ…å†µï¼Ÿ

    ```Python
X = torch.eye(8)
K = torch.tensor([[1.0, -1.0]])
Y = corr2d(X, K)
X, Y

(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0., 0., 0., 0.],
         [0., 0., 0., 0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0., 0., 0., 1.]]),
 tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
         [-1.,  1.,  0.,  0.,  0.,  0.,  0.],
         [ 0., -1.,  1.,  0.,  0.,  0.,  0.],
         [ 0.,  0., -1.,  1.,  0.,  0.,  0.],
         [ 0.,  0.,  0., -1.,  1.,  0.,  0.],
         [ 0.,  0.,  0.,  0., -1.,  1.,  0.],
         [ 0.,  0.,  0.,  0.,  0., -1.,  1.],
         [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]))
```

  58. å¦‚æœè½¬ç½®`X`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

    nothing happend

    ```Python
Y = corr2d(X.T, K)
X, Y

(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0., 0., 0., 0.],
         [0., 0., 0., 0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0., 0., 0., 1.]]),
 tensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.],
         [-1.,  1.,  0.,  0.,  0.,  0.,  0.],
         [ 0., -1.,  1.,  0.,  0.,  0.,  0.],
         [ 0.,  0., -1.,  1.,  0.,  0.,  0.],
         [ 0.,  0.,  0., -1.,  1.,  0.,  0.],
         [ 0.,  0.,  0.,  0., -1.,  1.,  0.],
         [ 0.,  0.,  0.,  0.,  0., -1.,  1.],
         [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]))
```

  59. å¦‚æœè½¬ç½®`K`ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

    Yçš„ç»“æœä¼šè½¬ç½®
    
    ```Python
Y = corr2d(X, K.T)
X, Y

(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
         [0., 1., 0., 0., 0., 0., 0., 0.],
         [0., 0., 1., 0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0., 0., 0., 0.],
         [0., 0., 0., 0., 1., 0., 0., 0.],
         [0., 0., 0., 0., 0., 1., 0., 0.],
         [0., 0., 0., 0., 0., 0., 1., 0.],
         [0., 0., 0., 0., 0., 0., 0., 1.]]),
 tensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],
         [ 0.,  1., -1.,  0.,  0.,  0.,  0.,  0.],
         [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],
         [ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.],
         [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  1., -1.,  0.],
         [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.]]))
```

## 6.3 æ­¥å¹…ã€å¡«å……

### ç»ƒä¹ 

60. å¯¹äºæœ¬èŠ‚ä¸­çš„æœ€åä¸€ä¸ªç¤ºä¾‹ï¼Œè®¡ç®—å…¶è¾“å‡ºå½¢çŠ¶ï¼Œä»¥æŸ¥çœ‹å®ƒæ˜¯å¦ä¸å®éªŒç»“æœä¸€è‡´ã€‚

  è¾“å…¥(8,8)ï¼Œè¾“å‡ºï¼š0ç»´ï¼š(8-3+0)/3+1=2(å–ä¸‹æ•´æ•°)ï¼Œ1ç»´:(8-5+1)/4+1=2

61. åœ¨æœ¬èŠ‚ä¸­çš„å®éªŒä¸­ï¼Œè¯•ä¸€è¯•å…¶ä»–å¡«å……å’Œæ­¥å¹…ç»„åˆã€‚

62. å¯¹äºéŸ³é¢‘ä¿¡å·ï¼Œæ­¥å¹…$2$è¯´æ˜ä»€ä¹ˆï¼Ÿ

  å¯èƒ½éŸ³é¢‘çš„æœ€å°å•ä½æ˜¯2ï¼Ÿ

63. æ­¥å¹…å¤§äº$1$çš„è®¡ç®—ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ

  å‡å°é‡‡æ ·ç‡å’Œè¾“å‡ºå¤§å°ï¼Œæé«˜è®¡ç®—é€Ÿåº¦

## 6.4 å¤šè¾“å…¥ã€å¤šè¾“å‡ºé€šé“

***å½“è¾“å…¥åŒ…å«å¤šä¸ªé€šé“æ—¶ï¼Œéœ€è¦æ„é€ ä¸€ä¸ªä¸è¾“å…¥æ•°æ®å…·æœ‰ç›¸åŒè¾“å…¥é€šé“æ•°çš„å·ç§¯æ ¸ï¼Œä»¥ä¾¿ä¸è¾“å…¥æ•°æ®è¿›è¡Œäº’ç›¸å…³è¿ç®—ã€‚å‡è®¾è¾“å…¥çš„é€šé“æ•°ä¸º$c_i$ï¼Œé‚£ä¹ˆå·ç§¯æ ¸çš„è¾“å…¥é€šé“æ•°ä¹Ÿéœ€è¦ä¸º$c_i$ã€‚å¦‚æœå·ç§¯æ ¸çš„çª—å£å½¢çŠ¶æ˜¯$k_h\times k_w$ï¼Œé‚£ä¹ˆå½“$c_i=1$æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå·ç§¯æ ¸çœ‹ä½œå½¢çŠ¶ä¸º$k_h\times k_w$çš„äºŒç»´å¼ é‡ã€‚***

***ç„¶è€Œï¼Œå½“$c_i>1$æ—¶ï¼Œæˆ‘ä»¬å·ç§¯æ ¸çš„æ¯ä¸ªè¾“å…¥é€šé“å°†åŒ…å«å½¢çŠ¶ä¸º$k_h\times k_w$çš„å¼ é‡ã€‚å°†è¿™äº›å¼ é‡$c_i$è¿ç»“åœ¨ä¸€èµ·å¯ä»¥å¾—åˆ°å½¢çŠ¶ä¸º$c_i\times k_h\times k_w$çš„å·ç§¯æ ¸ã€‚ç”±äºè¾“å…¥å’Œå·ç§¯æ ¸éƒ½æœ‰$c_i$ä¸ªé€šé“ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªé€šé“è¾“å…¥çš„äºŒç»´å¼ é‡å’Œå·ç§¯æ ¸çš„äºŒç»´å¼ é‡è¿›è¡Œäº’ç›¸å…³è¿ç®—ï¼Œå†å¯¹é€šé“æ±‚å’Œï¼ˆå°†$c_i$çš„ç»“æœç›¸åŠ ï¼‰å¾—åˆ°äºŒç»´å¼ é‡ã€‚è¿™æ˜¯å¤šé€šé“è¾“å…¥å’Œå¤šè¾“å…¥é€šé“å·ç§¯æ ¸ä¹‹é—´è¿›è¡ŒäºŒç»´äº’ç›¸å…³è¿ç®—çš„ç»“æœã€‚***

***æˆ‘ä»¬æ¼”ç¤ºäº†ä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªè¾“å…¥é€šé“çš„äºŒç»´äº’ç›¸å…³è¿ç®—çš„ç¤ºä¾‹ã€‚é˜´å½±éƒ¨åˆ†æ˜¯ç¬¬ä¸€ä¸ªè¾“å‡ºå…ƒç´ ä»¥åŠç”¨äºè®¡ç®—è¿™ä¸ªè¾“å‡ºçš„è¾“å…¥å’Œæ ¸å¼ é‡å…ƒç´ ï¼š$(1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56$ã€‚***

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 20.png)

## 6.5 æ±‡èšå±‚(æ± åŒ–)

- ***å®ƒå…·æœ‰åŒé‡ç›®çš„ï¼šé™ä½å·ç§¯å±‚å¯¹ä½ç½®çš„æ•æ„Ÿæ€§ï¼ŒåŒæ—¶é™ä½å¯¹ç©ºé—´é™é‡‡æ ·è¡¨ç¤ºçš„æ•æ„Ÿæ€§ã€‚***

## 6.6 LeNet-5

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 21.png)

æ¯ä¸ªå·ç§¯å—ä¸­çš„åŸºæœ¬å•å…ƒæ˜¯ä¸€ä¸ªå·ç§¯å±‚ã€ä¸€ä¸ªsigmoidæ¿€æ´»å‡½æ•°å’Œå¹³å‡æ±‡èšå±‚ã€‚è¯·æ³¨æ„ï¼Œè™½ç„¶ReLUå’Œæœ€å¤§æ±‡èšå±‚æ›´æœ‰æ•ˆï¼Œä½†å®ƒä»¬åœ¨20ä¸–çºª90å¹´ä»£è¿˜æ²¡æœ‰å‡ºç°ã€‚æ¯ä¸ªå·ç§¯å±‚ä½¿ç”¨5Ã—5å·ç§¯æ ¸å’Œä¸€ä¸ªsigmoidæ¿€æ´»å‡½æ•°ã€‚è¿™äº›å±‚å°†è¾“å…¥æ˜ å°„åˆ°å¤šä¸ªäºŒç»´ç‰¹å¾è¾“å‡ºï¼Œé€šå¸¸åŒæ—¶å¢åŠ é€šé“çš„æ•°é‡ã€‚ç¬¬ä¸€å·ç§¯å±‚æœ‰6ä¸ªè¾“å‡ºé€šé“ï¼Œè€Œç¬¬äºŒä¸ªå·ç§¯å±‚æœ‰16ä¸ªè¾“å‡ºé€šé“ã€‚æ¯ä¸ª2Ã—2æ± æ“ä½œï¼ˆæ­¥å¹…2ï¼‰é€šè¿‡ç©ºé—´ä¸‹é‡‡æ ·å°†ç»´æ•°å‡å°‘4å€ã€‚å·ç§¯çš„è¾“å‡ºå½¢çŠ¶ç”±æ‰¹é‡å¤§å°ã€é€šé“æ•°ã€é«˜åº¦ã€å®½åº¦å†³å®šã€‚

ä¸ºäº†å°†å·ç§¯å—çš„è¾“å‡ºä¼ é€’ç»™ç¨ å¯†å—ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨å°æ‰¹é‡ä¸­å±•å¹³æ¯ä¸ªæ ·æœ¬ã€‚æ¢è¨€ä¹‹ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªå››ç»´è¾“å…¥è½¬æ¢æˆå…¨è¿æ¥å±‚æ‰€æœŸæœ›çš„äºŒç»´è¾“å…¥ã€‚è¿™é‡Œçš„äºŒç»´è¡¨ç¤ºçš„ç¬¬ä¸€ä¸ªç»´åº¦ç´¢å¼•å°æ‰¹é‡ä¸­çš„æ ·æœ¬ï¼Œç¬¬äºŒä¸ªç»´åº¦ç»™å‡ºæ¯ä¸ªæ ·æœ¬çš„å¹³é¢å‘é‡è¡¨ç¤ºã€‚LeNetçš„ç¨ å¯†å—æœ‰ä¸‰ä¸ªå…¨è¿æ¥å±‚ï¼Œåˆ†åˆ«æœ‰120ã€84å’Œ10ä¸ªè¾“å‡ºã€‚å› ä¸ºæˆ‘ä»¬åœ¨æ‰§è¡Œåˆ†ç±»ä»»åŠ¡ï¼Œæ‰€ä»¥è¾“å‡ºå±‚çš„10ç»´å¯¹åº”äºæœ€åè¾“å‡ºç»“æœçš„æ•°é‡ã€‚

é€šè¿‡ä¸‹é¢çš„LeNetä»£ç ï¼Œå¯ä»¥çœ‹å‡ºç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶å®ç°æ­¤ç±»æ¨¡å‹éå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€è¦å®ä¾‹åŒ–ä¸€ä¸ª`Sequential`å—å¹¶å°†éœ€è¦çš„å±‚è¿æ¥åœ¨ä¸€èµ·ã€‚

```Python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), # 6*28*28
    nn.AvgPool2d(kernel_size=2, stride=2), # 6*14*14
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), # 16*10*10
    nn.AvgPool2d(kernel_size=2, stride=2), # 16*5*5
    nn.Flatten(),
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 22.png)

```Python
X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape: \t',X.shape)
    
    
Conv2d output shape: 	 torch.Size([1, 6, 28, 28])
Sigmoid output shape: 	 torch.Size([1, 6, 28, 28])
AvgPool2d output shape: 	 torch.Size([1, 6, 14, 14])
Conv2d output shape: 	 torch.Size([1, 16, 10, 10])
Sigmoid output shape: 	 torch.Size([1, 16, 10, 10])
AvgPool2d output shape: 	 torch.Size([1, 16, 5, 5])
Flatten output shape: 	 torch.Size([1, 400])
Linear output shape: 	 torch.Size([1, 120])
Sigmoid output shape: 	 torch.Size([1, 120])
Linear output shape: 	 torch.Size([1, 84])
Sigmoid output shape: 	 torch.Size([1, 84])
Linear output shape: 	 torch.Size([1, 10])
```

### å°ç»“

- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ˜¯ä¸€ç±»ä½¿ç”¨å·ç§¯å±‚çš„ç½‘ç»œã€‚

- åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œæˆ‘ä»¬ç»„åˆä½¿ç”¨å·ç§¯å±‚ã€éçº¿æ€§æ¿€æ´»å‡½æ•°å’Œæ±‡èšå±‚ã€‚

- ä¸ºäº†æ„é€ é«˜æ€§èƒ½çš„å·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬é€šå¸¸å¯¹å·ç§¯å±‚è¿›è¡Œæ’åˆ—ï¼Œé€æ¸é™ä½å…¶è¡¨ç¤ºçš„ç©ºé—´åˆ†è¾¨ç‡ï¼ŒåŒæ—¶å¢åŠ é€šé“æ•°ã€‚

- åœ¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œä¸­ï¼Œå·ç§¯å—ç¼–ç å¾—åˆ°çš„è¡¨å¾åœ¨è¾“å‡ºä¹‹å‰éœ€ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå…¨è¿æ¥å±‚è¿›è¡Œå¤„ç†ã€‚

- LeNetæ˜¯æœ€æ—©å‘å¸ƒçš„å·ç§¯ç¥ç»ç½‘ç»œä¹‹ä¸€ã€‚

### ç»ƒä¹ 

64. å°†å¹³å‡æ±‡èšå±‚æ›¿æ¢ä¸ºæœ€å¤§æ±‡èšå±‚ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

  ç›´æ¥å¯„æ‰

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 23.png)

65. å°è¯•æ„å»ºä¸€ä¸ªåŸºäºLeNetçš„æ›´å¤æ‚çš„ç½‘ç»œï¼Œä»¥æé«˜å…¶å‡†ç¡®æ€§ã€‚

  66. è°ƒæ•´å·ç§¯çª—å£å¤§å°ã€‚

  67. è°ƒæ•´è¾“å‡ºé€šé“çš„æ•°é‡ã€‚

  68. è°ƒæ•´æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰ã€‚

  69. è°ƒæ•´å·ç§¯å±‚çš„æ•°é‡ã€‚

  70. è°ƒæ•´å…¨è¿æ¥å±‚çš„æ•°é‡ã€‚

  71. è°ƒæ•´å­¦ä¹ ç‡å’Œå…¶ä»–è®­ç»ƒç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œåˆå§‹åŒ–å’Œè½®æ•°ï¼‰ã€‚

72. åœ¨MNISTæ•°æ®é›†ä¸Šå°è¯•ä»¥ä¸Šæ”¹è¿›çš„ç½‘ç»œã€‚

73. æ˜¾ç¤ºä¸åŒè¾“å…¥ï¼ˆä¾‹å¦‚æ¯›è¡£å’Œå¤–å¥—ï¼‰æ—¶ï¼ŒLeNetç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚çš„æ¿€æ´»å€¼ã€‚

# ch-7

## AlexNet

- æ¨¡å‹

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 24.png)

```Python
import torch
from torch import nn
from d2l import torch as d2l

net = nn.Sequential(
    # è¿™é‡Œä½¿ç”¨ä¸€ä¸ª11*11çš„æ›´å¤§çª—å£æ¥æ•æ‰å¯¹è±¡ã€‚
    # åŒæ—¶ï¼Œæ­¥å¹…ä¸º4ï¼Œä»¥å‡å°‘è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦ã€‚
    # å¦å¤–ï¼Œè¾“å‡ºé€šé“çš„æ•°ç›®è¿œå¤§äºLeNet
    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # å‡å°å·ç§¯çª—å£ï¼Œä½¿ç”¨å¡«å……ä¸º2æ¥ä½¿å¾—è¾“å…¥ä¸è¾“å‡ºçš„é«˜å’Œå®½ä¸€è‡´ï¼Œä¸”å¢å¤§è¾“å‡ºé€šé“æ•°
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    # ä½¿ç”¨ä¸‰ä¸ªè¿ç»­çš„å·ç§¯å±‚å’Œè¾ƒå°çš„å·ç§¯çª—å£ã€‚
    # é™¤äº†æœ€åçš„å·ç§¯å±‚ï¼Œè¾“å‡ºé€šé“çš„æ•°é‡è¿›ä¸€æ­¥å¢åŠ ã€‚
    # åœ¨å‰ä¸¤ä¸ªå·ç§¯å±‚ä¹‹åï¼Œæ±‡èšå±‚ä¸ç”¨äºå‡å°‘è¾“å…¥çš„é«˜åº¦å’Œå®½åº¦
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),
    # è¿™é‡Œï¼Œå…¨è¿æ¥å±‚çš„è¾“å‡ºæ•°é‡æ˜¯LeNetä¸­çš„å¥½å‡ å€ã€‚ä½¿ç”¨dropoutå±‚æ¥å‡è½»è¿‡æ‹Ÿåˆ
    nn.Linear(6400, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 4096), nn.ReLU(),
    nn.Dropout(p=0.5),
    # æœ€åæ˜¯è¾“å‡ºå±‚ã€‚ç”±äºè¿™é‡Œä½¿ç”¨Fashion-MNISTï¼Œæ‰€ä»¥ç”¨ç±»åˆ«æ•°ä¸º10ï¼Œè€Œéè®ºæ–‡ä¸­çš„1000
    nn.Linear(4096, 10))
```

- æŸ¥çœ‹æ¨¡å‹è¾“å‡ºå½¢çŠ¶

```Python
def get_net_outshape(net, X):
    for layer in net:
        X=layer(X)
        print(layer.__class__.__name__,'output shape:\t',X.shape)
        
        
X = torch.randn(1, 1, 224, 224)
get_net_outshape(net, X)
```

```Python
X = torch.randn(1, 1, 224, 224)
for layer in net:
    X=layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)

Conv2d output shape:	 torch.Size([1, 96, 54, 54])
ReLU output shape:	 torch.Size([1, 96, 54, 54])
MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])
Conv2d output shape:	 torch.Size([1, 256, 26, 26])
ReLU output shape:	 torch.Size([1, 256, 26, 26])
MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])
Conv2d output shape:	 torch.Size([1, 384, 12, 12])
ReLU output shape:	 torch.Size([1, 384, 12, 12])
Conv2d output shape:	 torch.Size([1, 384, 12, 12])
ReLU output shape:	 torch.Size([1, 384, 12, 12])
Conv2d output shape:	 torch.Size([1, 256, 12, 12])
ReLU output shape:	 torch.Size([1, 256, 12, 12])
MaxPool2d output shape:	 torch.Size([1, 256, 5, 5])
Flatten output shape:	 torch.Size([1, 6400])
Linear output shape:	 torch.Size([1, 4096])
ReLU output shape:	 torch.Size([1, 4096])
Dropout output shape:	 torch.Size([1, 4096])
Linear output shape:	 torch.Size([1, 4096])
ReLU output shape:	 torch.Size([1, 4096])
Dropout output shape:	 torch.Size([1, 4096])
Linear output shape:	 torch.Size([1, 10])
```

- è¯»å–æ•°æ®é›†

```Python
batch_size = 128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)

lr, num_epochs = 0.01, 10
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 25.png)

### æ¨¡å‹è¾“å‡ºå½¢çŠ¶è®¡ç®—

                                           $\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.$

## VGG-11

3Ã—3å·ç§¯æ ¸ã€å¡«å……ä¸º1ï¼ˆä¿æŒé«˜åº¦å’Œå®½åº¦ï¼‰çš„å·ç§¯å±‚ï¼Œå’Œå¸¦æœ‰2Ã—2æ±‡èšçª—å£ã€æ­¥å¹…ä¸º2ï¼ˆæ¯ä¸ªå—åçš„åˆ†è¾¨ç‡å‡åŠï¼‰çš„æœ€å¤§æ±‡èšå±‚ã€‚åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º`vgg_block`çš„å‡½æ•°æ¥å®ç°ä¸€ä¸ªVGGå—ã€‚

è¯¥å‡½æ•°æœ‰ä¸‰ä¸ªå‚æ•°ï¼Œåˆ†åˆ«å¯¹åº”äºå·ç§¯å±‚çš„æ•°é‡`num_convs`ã€è¾“å…¥é€šé“çš„æ•°é‡`in_channels` å’Œè¾“å‡ºé€šé“çš„æ•°é‡`out_channels`.

```Python
import torch
from torch import nn
from d2l import torch as d2l


def vgg_block(num_convs, in_channels, out_channels):
    layers = []
    for _ in range(num_convs):
        layers.append(nn.Conv2d(in_channels, out_channels,
                                kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        in_channels = out_channels  # æ­¤æ—¶ï¼Œè¾“å…¥é€šé“å’Œè¾“å‡ºé€šé“å²‚ä¸æ˜¯ä¸€æ ·äº†
    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
    return nn.Sequential(*layers)
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 26.png)

- æ¨¡å‹

```Python
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

def vgg(conv_arch):
    conv_blks = []
    in_channels = 1
    # å·ç§¯å±‚éƒ¨åˆ†
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        in_channels = out_channels

    return nn.Sequential(
        *conv_blks, nn.Flatten(),
        # å…¨è¿æ¥å±‚éƒ¨åˆ†
        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
        nn.Linear(4096, 10))

net = vgg(conv_arch)

X = torch.randn(size=(1, 1, 224, 224))
for blk in net:
    X = blk(X)
    print(blk.__class__.__name__,'output shape:\t',X.shape)

Sequential output shape:	 torch.Size([1, 64, 112, 112])
Sequential output shape:	 torch.Size([1, 128, 56, 56])
Sequential output shape:	 torch.Size([1, 256, 28, 28])
Sequential output shape:	 torch.Size([1, 512, 14, 14])
Sequential output shape:	 torch.Size([1, 512, 7, 7])
Flatten output shape:	 torch.Size([1, 25088])
Linear output shape:	 torch.Size([1, 4096])
ReLU output shape:	 torch.Size([1, 4096])
Dropout output shape:	 torch.Size([1, 4096])
Linear output shape:	 torch.Size([1, 4096])
ReLU output shape:	 torch.Size([1, 4096])
Dropout output shape:	 torch.Size([1, 4096])
Linear output shape:	 torch.Size([1, 10])
```

## NiN

- æ¨¡å‹

NiNå—ä»¥ä¸€ä¸ªæ™®é€šå·ç§¯å±‚å¼€å§‹ï¼Œåé¢æ˜¯ä¸¤ä¸ª1Ã—1çš„å·ç§¯å±‚ã€‚è¿™ä¸¤ä¸ª1Ã—1å·ç§¯å±‚å……å½“å¸¦æœ‰ReLUæ¿€æ´»å‡½æ•°çš„é€åƒç´ å…¨è¿æ¥å±‚ã€‚ ç¬¬ä¸€å±‚çš„å·ç§¯çª—å£å½¢çŠ¶é€šå¸¸ç”±ç”¨æˆ·è®¾ç½®ã€‚ éšåçš„å·ç§¯çª—å£å½¢çŠ¶å›ºå®šä¸º1Ã—1ã€‚

```Python
import torch
from torch import nn
from d2l import torch as d2l


def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())
    
net = nn.Sequential(
    nin_block(1, 96, kernel_size=11, strides=4, padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96, 256, kernel_size=5, strides=1, padding=2),
    nn.MaxPool2d(3, stride=2),
    nin_block(256, 384, kernel_size=3, strides=1, padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    # æ ‡ç­¾ç±»åˆ«æ•°æ˜¯10
    nin_block(384, 10, kernel_size=3, strides=1, padding=1),
    nn.AdaptiveAvgPool2d((1, 1)),
    # å°†å››ç»´çš„è¾“å‡ºè½¬æˆäºŒç»´çš„è¾“å‡ºï¼Œå…¶å½¢çŠ¶ä¸º(æ‰¹é‡å¤§å°,10)
    nn.Flatten())

X = torch.rand(size=(1, 1, 224, 224))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)

Sequential output shape:	 torch.Size([1, 96, 54, 54])
MaxPool2d output shape:	 torch.Size([1, 96, 26, 26])
Sequential output shape:	 torch.Size([1, 256, 26, 26])
MaxPool2d output shape:	 torch.Size([1, 256, 12, 12])
Sequential output shape:	 torch.Size([1, 384, 12, 12])
MaxPool2d output shape:	 torch.Size([1, 384, 5, 5])
Dropout output shape:	 torch.Size([1, 384, 5, 5])
Sequential output shape:	 torch.Size([1, 10, 5, 5])
AdaptiveAvgPool2d output shape:	 torch.Size([1, 10, 1, 1])
Flatten output shape:	 torch.Size([1, 10])
```

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 27.png)

## GoogleLeNet(å¹¶è¡Œ)

- Inceptionï¼šåœ¨GoogLeNetä¸­ï¼ŒåŸºæœ¬çš„å·ç§¯å—è¢«ç§°ä¸º*Inceptionå—*ï¼ˆInception blockï¼‰ã€‚è¿™å¾ˆå¯èƒ½å¾—åäºç”µå½±ã€Šç›—æ¢¦ç©ºé—´ã€‹ï¼ˆInceptionï¼‰ï¼Œå› ä¸ºç”µå½±ä¸­çš„ä¸€å¥è¯â€œæˆ‘ä»¬éœ€è¦èµ°å¾—æ›´æ·±â€ï¼ˆâ€œWe need to go deeperâ€ï¼‰ã€‚

![image.png](ææ²-åŠ¨æ‰‹å­¦+5236e5ec-360b-4d43-8d10-20aed84bc3c6/image 28.png)

```Python
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


class Inception(nn.Module):
    # c1--c4æ˜¯æ¯æ¡è·¯å¾„çš„è¾“å‡ºé€šé“æ•°
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        # çº¿è·¯1ï¼Œå•1x1å·ç§¯å±‚
        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
        # çº¿è·¯2ï¼Œ1x1å·ç§¯å±‚åæ¥3x3å·ç§¯å±‚
        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # çº¿è·¯3ï¼Œ1x1å·ç§¯å±‚åæ¥5x5å·ç§¯å±‚
        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # çº¿è·¯4ï¼Œ3x3æœ€å¤§æ±‡èšå±‚åæ¥1x1å·ç§¯å±‚
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        # åœ¨é€šé“ç»´åº¦ä¸Šè¿ç»“è¾“å‡º
        return torch.cat((p1, p2, p3, p4), dim=1)
```

- ***GoogLeNetä¸­ä¸åŒInceptionå—out_channelsçš„ä¸ªæ•°å’Œé…æ¯”æŠŠæˆ‘çœ‹éœ‡æƒŠäº†ï¼Œä»–ä»¬å½“åˆæ˜¯è¿›è¡Œäº†å¤šå°‘å®éªŒæ‰æ‰¾å‡ºæ¥çš„æ•°é‡é…æ¯”å•Šï¼Œç¦»è°±ï¼Œç‰›ï¼å•æ˜¯è¿™ä¸ªè¾“å‡ºå½¢çŠ¶æˆ‘å°±å¾—ç®—ä¸€å¤©***

```Python
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64, 192, kernel_size=3, padding=1),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                   Inception(256, 128, (128, 192), (32, 96), 64),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                   Inception(832, 384, (192, 384), (48, 128), 128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())

net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))

X = torch.rand(size=(1, 1, 96, 96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t', X.shape)
    
Sequential output shape:	 torch.Size([1, 64, 24, 24])
Sequential output shape:	 torch.Size([1, 192, 12, 12])
Sequential output shape:	 torch.Size([1, 480, 6, 6])
Sequential output shape:	 torch.Size([1, 832, 3, 3])
Sequential output shape:	 torch.Size([1, 1024])
Linear output shape:	 torch.Size([1, 10])
```

- *ç¬¬ä¸€ä¸ªæ¨¡å—ä½¿ç”¨64ä¸ªé€šé“ã€7Ã—7å·ç§¯å±‚ã€‚*

- *ç¬¬äºŒä¸ªæ¨¡å—ä½¿ç”¨ä¸¤ä¸ªå·ç§¯å±‚ï¼šç¬¬ä¸€ä¸ªå·ç§¯å±‚æ˜¯64ä¸ªé€šé“ã€1Ã—1å·ç§¯å±‚ï¼›ç¬¬äºŒä¸ªå·ç§¯å±‚ä½¿ç”¨å°†é€šé“æ•°é‡å¢åŠ ä¸‰å€çš„3Ã—3å·ç§¯å±‚ã€‚ è¿™å¯¹åº”äº`Inception`å—ä¸­çš„ç¬¬äºŒæ¡è·¯å¾„ã€‚*

- *ç¬¬ä¸‰ä¸ªæ¨¡å—ä¸²è”ä¸¤ä¸ªå®Œæ•´çš„`Inception`å—ã€‚ ç¬¬ä¸€ä¸ª`Inception`å—çš„è¾“å‡ºé€šé“æ•°ä¸º`64+128+32+32=256`ï¼Œå››ä¸ªè·¯å¾„ä¹‹é—´çš„è¾“å‡ºé€šé“æ•°é‡æ¯”ä¸º`64:128:32:32=2:4:1:1`ã€‚ ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªè·¯å¾„é¦–å…ˆå°†è¾“å…¥é€šé“çš„æ•°é‡åˆ†åˆ«å‡å°‘åˆ°`96/192=1/2`å’Œ`16/192=1/12`ï¼Œç„¶åè¿æ¥ç¬¬äºŒä¸ªå·ç§¯å±‚ã€‚ç¬¬äºŒä¸ª`Inception`å—çš„è¾“å‡ºé€šé“æ•°å¢åŠ åˆ°`128+192+96+64=480`ï¼Œå››ä¸ªè·¯å¾„ä¹‹é—´çš„è¾“å‡ºé€šé“æ•°é‡æ¯”ä¸º`128:192:96:64=4:6:3:2`ã€‚ ç¬¬äºŒæ¡å’Œç¬¬ä¸‰æ¡è·¯å¾„é¦–å…ˆå°†è¾“å…¥é€šé“çš„æ•°é‡åˆ†åˆ«å‡å°‘åˆ°`128/256=1/2å’Œ32/256=1/8`ã€‚*

- *ç¬¬å››æ¨¡å—æ›´åŠ å¤æ‚ï¼Œ å®ƒä¸²è”äº†5ä¸ª`Inception`å—ï¼Œå…¶è¾“å‡ºé€šé“æ•°åˆ†åˆ«æ˜¯`192+208+48+64=512`ã€`160+224+64+64=512`ã€`128+256+64+64=512`ã€`112+288+64+64=528`å’Œ`256+320+128+128=832`ã€‚ è¿™äº›è·¯å¾„çš„é€šé“æ•°åˆ†é…å’Œç¬¬ä¸‰æ¨¡å—ä¸­çš„ç±»ä¼¼ï¼Œé¦–å…ˆæ˜¯å«3Ã—3å·ç§¯å±‚çš„ç¬¬äºŒæ¡è·¯å¾„è¾“å‡ºæœ€å¤šé€šé“ï¼Œå…¶æ¬¡æ˜¯ä»…å«1Ã—1å·ç§¯å±‚çš„ç¬¬ä¸€æ¡è·¯å¾„ï¼Œä¹‹åæ˜¯å«5Ã—5å·ç§¯å±‚çš„ç¬¬ä¸‰æ¡è·¯å¾„å’Œå«3Ã—3æœ€å¤§æ±‡èšå±‚çš„ç¬¬å››æ¡è·¯å¾„ã€‚ å…¶ä¸­ç¬¬äºŒã€ç¬¬ä¸‰æ¡è·¯å¾„éƒ½ä¼šå…ˆæŒ‰æ¯”ä¾‹å‡å°é€šé“æ•°ã€‚ è¿™äº›æ¯”ä¾‹åœ¨å„ä¸ª`Inception`å—ä¸­éƒ½ç•¥æœ‰ä¸åŒã€‚*

- *ç¬¬äº”æ¨¡å—åŒ…å«è¾“å‡ºé€šé“æ•°ä¸º`256+320+128+128=832`å’Œ`384+384+128+128=1024`çš„ä¸¤ä¸ª`Inception`å—ã€‚ å…¶ä¸­æ¯æ¡è·¯å¾„é€šé“æ•°çš„åˆ†é…æ€è·¯å’Œç¬¬ä¸‰ã€ç¬¬å››æ¨¡å—ä¸­çš„ä¸€è‡´ï¼Œåªæ˜¯åœ¨å…·ä½“æ•°å€¼ä¸Šæœ‰æ‰€ä¸åŒã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç¬¬äº”æ¨¡å—çš„åé¢ç´§è·Ÿè¾“å‡ºå±‚ï¼Œè¯¥æ¨¡å—åŒNiNä¸€æ ·ä½¿ç”¨å…¨å±€å¹³å‡æ±‡èšå±‚ï¼Œå°†æ¯ä¸ªé€šé“çš„é«˜å’Œå®½å˜æˆ1ã€‚ æœ€åæˆ‘ä»¬å°†è¾“å‡ºå˜æˆäºŒç»´æ•°ç»„ï¼Œå†æ¥ä¸Šä¸€ä¸ªè¾“å‡ºä¸ªæ•°ä¸ºæ ‡ç­¾ç±»åˆ«æ•°çš„å…¨è¿æ¥å±‚ã€‚*

## 7.5 æ‰¹é‡è§„èŒƒåŒ–

- ä»é›¶å®ç°

```Python
import torch
from torch import nn
from d2l import torch as d2l


def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # é€šè¿‡is_grad_enabledæ¥åˆ¤æ–­å½“å‰æ¨¡å¼æ˜¯è®­ç»ƒæ¨¡å¼è¿˜æ˜¯é¢„æµ‹æ¨¡å¼
    if not torch.is_grad_enabled():
        # å¦‚æœæ˜¯åœ¨é¢„æµ‹æ¨¡å¼ä¸‹ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ç§»åŠ¨å¹³å‡æ‰€å¾—çš„å‡å€¼å’Œæ–¹å·®
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # ä½¿ç”¨å…¨è¿æ¥å±‚çš„æƒ…å†µï¼Œè®¡ç®—ç‰¹å¾ç»´ä¸Šçš„å‡å€¼å’Œæ–¹å·®
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # ä½¿ç”¨äºŒç»´å·ç§¯å±‚çš„æƒ…å†µï¼Œè®¡ç®—é€šé“ç»´ä¸Šï¼ˆaxis=1ï¼‰çš„å‡å€¼å’Œæ–¹å·®ã€‚
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ä¿æŒXçš„å½¢çŠ¶ä»¥ä¾¿åé¢å¯ä»¥åšå¹¿æ’­è¿ç®—
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œç”¨å½“å‰çš„å‡å€¼å’Œæ–¹å·®åšæ ‡å‡†åŒ–
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # æ›´æ–°ç§»åŠ¨å¹³å‡çš„å‡å€¼å’Œæ–¹å·®
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # ç¼©æ”¾å’Œç§»ä½
    return Y, moving_mean.data, moving_var.data

class BatchNorm(nn.Module):
    # num_featuresï¼šå®Œå…¨è¿æ¥å±‚çš„è¾“å‡ºæ•°é‡æˆ–å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚
    # num_dimsï¼š2è¡¨ç¤ºå®Œå…¨è¿æ¥å±‚ï¼Œ4è¡¨ç¤ºå·ç§¯å±‚
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # å‚ä¸æ±‚æ¢¯åº¦å’Œè¿­ä»£çš„æ‹‰ä¼¸å’Œåç§»å‚æ•°ï¼Œåˆ†åˆ«åˆå§‹åŒ–æˆ1å’Œ0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # éæ¨¡å‹å‚æ•°çš„å˜é‡åˆå§‹åŒ–ä¸º0å’Œ1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)

    def forward(self, X):
        # å¦‚æœXä¸åœ¨å†…å­˜ä¸Šï¼Œå°†moving_meanå’Œmoving_var
        # å¤åˆ¶åˆ°Xæ‰€åœ¨æ˜¾å­˜ä¸Š
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # ä¿å­˜æ›´æ–°è¿‡çš„moving_meanå’Œmoving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```

- ç®€å•å®ç°

```Python
# num_featuresï¼šå®Œå…¨è¿æ¥å±‚çš„è¾“å‡ºæ•°é‡æˆ–å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ã€‚
# num_dimsï¼š2è¡¨ç¤ºå®Œå…¨è¿æ¥å±‚ï¼Œ4è¡¨ç¤ºå·ç§¯å±‚
```

```Python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),
    nn.Linear(256, 120), nn.BatchNorm1d(120), nn.Sigmoid(),
    nn.Linear(120, 84), nn.BatchNorm1d(84), nn.Sigmoid(),
    nn.Linear(84, 10))
```

# ch-8

## 8.1 åºåˆ—æ¨¡å‹

- è¿™æ®µä»£ç æˆ‘æ„¿ç§°ä¹‹ä¸ºæœ€å¼ºï¼è¦æ˜¯æˆ‘çš„è¯ å¯èƒ½å°±è¦ä¸€æ¡ä¸€æ¡æ’å…¥æ•°æ®äº†ï¼Œå¤§ç¥ç›´æ¥ä¸€æ’æ’ä¸€åˆ—ï¼Œä¸æ„§æ˜¯å¤§æ ¼å±€

```Python
%matplotlib inline
import torch
from torch import nn
from d2l import torch as d2l

T = 1000  # æ€»å…±äº§ç”Ÿ1000ä¸ªç‚¹
time = torch.arange(1, T + 1, dtype=torch.float32)
x = torch.sin(0.01 * time) + torch.normal(0, 0.2, (T,))
d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))

# è¿™å‡ è¡Œä»£ç å¤ªç§€äº†ï¼Œè¦æ˜¯æˆ‘çš„è¯ å¯èƒ½å°±è¦ä¸€æ¡ä¸€æ¡æ’å…¥æ•°æ®äº†ï¼Œå¤§ç¥ç›´æ¥ä¸€æ’æ’ä¸€åˆ—ï¼Œä¸æ„§æ˜¯å¤§æ ¼å±€
tau = 4
features = torch.zeros((T - tau, tau))
for i in range(tau):
    features[:, i] = x[i: T - tau + i]
labels = x[tau:].reshape((-1, 1))
```

## 8.2 æ–‡æœ¬é¢„å¤„ç†

- è¯å…ƒ

```Python
class Vocab:  #@save
    """æ–‡æœ¬è¯è¡¨"""
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                   reverse=True)
        # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx
                             for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1

    def __len__(self):
        return len(self.idx_to_token)

    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]

    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]

    @property
    def unk(self):  # æœªçŸ¥è¯å…ƒçš„ç´¢å¼•ä¸º0
        return 0

    @property
    def token_freqs(self):
        return self._token_freqs

def count_corpus(tokens):  #@save
    """ç»Ÿè®¡è¯å…ƒçš„é¢‘ç‡"""
    # è¿™é‡Œçš„tokensæ˜¯1Dåˆ—è¡¨æˆ–2Dåˆ—è¡¨
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # å°†è¯å…ƒåˆ—è¡¨å±•å¹³æˆä¸€ä¸ªåˆ—è¡¨
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)
```

å¤ªæºœäº† æˆ‘ä¹‹å‰éƒ½ä¸ä¼šè¿™æ ·è§£åŒ…

```Python
li = [[1, 2, 3], [3, 4, 5], [3,9,10]]
[j for l in li for j in l]

[1, 2, 3, 3, 4, 5, 3, 9, 10]
```

## 8.3 å­¦ä¹ è¯­è¨€æ¨¡å‹

$P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}).$

ä¾‹å¦‚ï¼ŒåŒ…å«äº†å››ä¸ªå•è¯çš„ä¸€ä¸ªæ–‡æœ¬åºåˆ—çš„æ¦‚ç‡æ˜¯ï¼š

$P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).$

ä¸ºäº†è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—å•è¯çš„æ¦‚ç‡ï¼Œä»¥åŠç»™å®šå‰é¢å‡ ä¸ªå•è¯åå‡ºç°æŸä¸ªå•è¯çš„æ¡ä»¶æ¦‚ç‡ã€‚è¿™äº›æ¦‚ç‡æœ¬è´¨ä¸Šå°±æ˜¯è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚

## 8.5 å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆå›°æƒ‘åº¦ï¼‰

å¦‚æœæƒ³è¦å‹ç¼©æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å½“å‰è¯å…ƒé›†é¢„æµ‹çš„ä¸‹ä¸€ä¸ªè¯å…ƒã€‚ä¸€ä¸ªæ›´å¥½çš„è¯­è¨€æ¨¡å‹åº”è¯¥èƒ½è®©æˆ‘ä»¬æ›´å‡†ç¡®åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¯å…ƒã€‚å› æ­¤ï¼Œå®ƒåº”è¯¥å…è®¸æˆ‘ä»¬åœ¨å‹ç¼©åºåˆ—æ—¶èŠ±è´¹æ›´å°‘çš„æ¯”ç‰¹ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªåºåˆ—ä¸­æ‰€æœ‰çš„$n$ä¸ªè¯å…ƒçš„äº¤å‰ç†µæŸå¤±çš„å¹³å‡å€¼æ¥è¡¡é‡ï¼š

$\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),$

å…¶ä¸­$P$ç”±è¯­è¨€æ¨¡å‹ç»™å‡º$x_t$æ˜¯åœ¨æ—¶é—´æ­¥$t$ä»è¯¥åºåˆ—ä¸­è§‚å¯Ÿåˆ°çš„å®é™…è¯å…ƒã€‚è¿™ä½¿å¾—ä¸åŒé•¿åº¦çš„æ–‡æ¡£çš„æ€§èƒ½å…·æœ‰äº†å¯æ¯”æ€§ã€‚
ç”±äºå†å²åŸå› ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†çš„ç§‘å­¦å®¶æ›´å–œæ¬¢ä½¿ç”¨ä¸€ä¸ªå«åš*å›°æƒ‘åº¦*ï¼ˆperplexityï¼‰çš„é‡ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒæ˜¯

$\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).$

å›°æƒ‘åº¦çš„æœ€å¥½çš„ç†è§£æ˜¯â€œä¸‹ä¸€ä¸ªè¯å…ƒçš„å®é™…é€‰æ‹©æ•°çš„è°ƒå’Œå¹³å‡æ•°â€ã€‚æˆ‘ä»¬çœ‹çœ‹ä¸€äº›æ¡ˆä¾‹ã€‚

- åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯å®Œç¾åœ°ä¼°è®¡æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º1ã€‚
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„å›°æƒ‘åº¦ä¸º1ã€‚

- åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯é¢„æµ‹æ ‡ç­¾è¯å…ƒçš„æ¦‚ç‡ä¸º0ã€‚
åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå›°æƒ‘åº¦æ˜¯æ­£æ— ç©·å¤§ã€‚

# ch-9

## 9.6 ç¼–ç å™¨å’Œè§£ç å™¨

- ç¼–ç å™¨

```Python
from torch import nn


#@save
class Encoder(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„åŸºæœ¬ç¼–ç å™¨æ¥å£"""
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
        raise NotImplementedError
```

- è§£ç å™¨

```Python
#@save
class Decoder(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„åŸºæœ¬è§£ç å™¨æ¥å£"""
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)

    def init_state(self, enc_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError
```

- åˆå¹¶ç¼–ç å™¨-è§£ç å™¨

```Python
#@save
class EncoderDecoder(nn.Module):
    """ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„åŸºç±»"""
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)
        dec_state = self.decoder.init_state(enc_outputs, *args)
        return self.decoder(dec_X, dec_state)
```



